{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Sentinel-2 images and extracting time-series statistics\n",
    "\n",
    "Workflow for the practical:\n",
    "1. Search the ESA Copernicus Sentinel Data Hub for all available images over a region within a defined time period and low cloud cover\n",
    "2. Download all selected images into a data directory\n",
    "3. Convert all images into Geotiff files (retain only the 10 m resolution bands)\n",
    "4. Save quicklooks of all images\n",
    "5. Extract polygon statistics of the selected bands as a time series from all acquisition dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required Python packages:\n",
    "# Note: GDAL needs to be version 2.1.3\n",
    "# from Anaconda terminal type:\n",
    "#    conda install -c conda-forge gdal\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gdal\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ogr\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import osr\n",
    "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "import shutil\n",
    "from skimage import io, exposure\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "gdal.UseExceptions()\n",
    "io.use_plugin('matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some directory names. \n",
    "\n",
    "Modify these to match your data directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up your directories for the satellite data\n",
    "rootdir = join(os.sep, 'gy7709', 'practicals', 'p08')\n",
    "# directory for downloading the Sentinel-2 granules\n",
    "downloaddir = join(rootdir, 'download') \n",
    "# directory for the tiff files we will make\n",
    "tiffdir = join(rootdir, 'tiff') \n",
    "# quicklook directory\n",
    "quickdir = join(rootdir, 'quicklooks') \n",
    "# output directory for statistics file and search results\n",
    "outdir = join(rootdir, 'outputs') \n",
    "\n",
    "# create the new directories, unless they already exist\n",
    "os.makedirs(downloaddir, exist_ok=True)\n",
    "os.makedirs(tiffdir, exist_ok=True)\n",
    "os.makedirs(quickdir, exist_ok=True)\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, you have to make a text file called \"sencredentials.txt\" with your login details for the ESA Copernicus Sentinel Hub. The file has two lines of text.\n",
    "Line 1: Your username\n",
    "Line 2: Your password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download options and Data Hub search parameters\n",
    "ndown = 10 # number of scenes to be downloaded (in order of least cloud cover)\n",
    "shapefile = join(rootdir, 'p8area.shp') # ESRI Shapefile of the study area\n",
    "datefrom = '20190401' # start date for imagery search\n",
    "dateto   = '20190630' # end date for imagery search\n",
    "clouds = '[0 TO 10]' # range of acceptable cloud cover % for imagery search\n",
    "    # Note that later versions of the Sentinelsat package require this in the format: clouds = (0, 10) \n",
    "credentials = join(rootdir, 'sencredentials.txt')  # contains two lines of text with username and password\n",
    "\n",
    "# Filename options\n",
    "# VRT virtual raster stack of all image files\n",
    "vrtfile = \"mosaic.vrt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a function for converting all Sentinel-2 images in a directory into tiff files. Retain only the 10 m resolution bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next block of code is a function that reads in the Sentinel-2 L2A (Level 2A) image obtained \n",
    "#     from the Copernicus Sentinel Data Hub.\n",
    "# It also changes the file to a new projection if defined\n",
    "# Sentinel images can be obtained for free from this web site: https://scihub.copernicus.eu/dhus/#/home\n",
    "\n",
    "def s2tiff(indir, outdir, ID, proj):\n",
    "    '''\n",
    "    indir = directory where the input files are located\n",
    "    outdir = directory where the tiff file will be written if it does not already exist\n",
    "    ID = a string giving the filename without file extension\n",
    "    proj = output projection\n",
    "    '''\n",
    "    # list all files in input directory    \n",
    "    print('Files in directory ' + indir)\n",
    "    allfiles = [f for f in listdir(indir) if isfile(join(indir, f))]\n",
    "    for f in allfiles:\n",
    "        print(f)\n",
    "\n",
    "    # make a band selection\n",
    "    print('Band files to be included in tiff file:')    \n",
    "    bands = allfiles[1:5]\n",
    "    for b in bands:\n",
    "        print(b)\n",
    "\n",
    "    # build a command line command for GDAL to convert the files into 10 m resolution VRT format\n",
    "    vrtfile = join(outdir, ID + '_16Bit.vrt')\n",
    "    cmd = ['gdalbuildvrt', '-resolution', 'user', '-tr' ,'10', '10', '-separate', vrtfile]\n",
    "    for band in bands:\n",
    "        cmd.append(join(indir, band))\n",
    "\n",
    "    if not os.path.exists(vrtfile): # skip if the output file already exists\n",
    "        print('\\n')\n",
    "        print(' '.join(cmd))\n",
    "        print('\\n')\n",
    "        subprocess.run(cmd) # execute the command in the command line\n",
    "    else:\n",
    "        print(vrtfile,' already exists.\\n')\n",
    "    \n",
    "    # check whether the output directory already exists and create it if not\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # define output file names\n",
    "    tiffile = join(outdir, ID + '_16Bit.tif')\n",
    "    projtiffile = join(outdir, ID + '_16Bit_proj.tif')\n",
    "\n",
    "    if not os.path.exists(projtiffile): # skip if the output file already exists\n",
    "        # build a command to translate the four band raster files into one geotiff file with 4 bands\n",
    "        cmd = ['gdal_translate', '-of' ,'GTiff', vrtfile, tiffile]\n",
    "        print(' '.join(cmd))\n",
    "        print('\\n')\n",
    "        # execute the GDAL command\n",
    "        subprocess.run(cmd) \n",
    "\n",
    "        # build a command to reproject the single Geotiff image to a new projection\n",
    "        cmd = ['gdalwarp', tiffile, projtiffile, '-t_srs', proj]\n",
    "        print(' '.join(cmd))\n",
    "        print('\\n')\n",
    "        # execute the GDAL command\n",
    "        subprocess.run(cmd) \n",
    "\n",
    "        os.remove(vrtfile) # delete the vrt file\n",
    "        os.remove(tiffile) # delete the tiffile in the old projection\n",
    "\n",
    "    else:\n",
    "        print(projtiffile,' already exists.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a function for making one quicklook image in PNG format out of one or many TIFF files. The output file is smaller than the originals, i.e. not full resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function makes one PNG quicklook file by mosaicking all tiff files in a list of filenames\n",
    "# Note: the files must all the in the same projection\n",
    "\n",
    "def quicklooks(tiffdir, infilelist, outdir, mosaicID, outsize):\n",
    "    '''\n",
    "    tiffdir = directory in which the files in infilelist are located\n",
    "    infilelist = list of filenames (full path not included) to be processed\n",
    "    outdir = directory where the PNG quicklook files will be saved\n",
    "    mosaicID = beginning of the file name of the output files to be created (full path)\n",
    "    outsize = percentage downscaling factor, e.g. 10 means 10% of xsize and 10% of ysize\n",
    "    '''\n",
    "\n",
    "    # make output filenames\n",
    "    vrtfile = join(outdir, mosaicID + \".vrt\")\n",
    "    quicklookfile = join(outdir, mosaicID + \".png\")\n",
    "\n",
    "    print(vrtfile)\n",
    "    print(quicklookfile)\n",
    "    \n",
    "    # save the list of file names to a text file for use in the GDAL command\n",
    "    filelist = join(outdir, \"filelist.txt\") # make a file name\n",
    "    \n",
    "    print(filelist)\n",
    "\n",
    "    file1 = open(filelist,\"w\") # open file in write mode \n",
    "\n",
    "    for f in infilelist:\n",
    "        file1.write(join(tiffdir, f)) \n",
    "        file1.write(\"\\n\")\n",
    "        print(join(tiffdir, f))\n",
    "        print(os.path.exists(join(tiffdir, f)))\n",
    "    file1.close() \n",
    "\n",
    "    # make a quicklook classification mosaic of all granules in PNG format using GDAL\n",
    "    com = \"gdalbuildvrt -overwrite -input_file_list \" + filelist + \" \"+ vrtfile\n",
    "    print(com)\n",
    "    flag = os.system(com)\n",
    "    if flag == 0:\n",
    "        print('Created VRT file: ' + vrtfile)\n",
    "    else:\n",
    "        print('Error creating VRT file')\n",
    "\n",
    "    # Create a PNG quicklook, scaled from 0 - 255\n",
    "    com = \"gdal_translate -of PNG -ot Byte -scale -outsize \" + str(outsize) + \"% \" + \\\n",
    "        str(outsize) + \"% \" + vrtfile + \" \" + quicklookfile\n",
    "    print(com)\n",
    "    flag = os.system(com)\n",
    "    if flag == 0:\n",
    "        print('Created quicklook mosaic file: ' + quicklookfile)\n",
    "    else:\n",
    "        print('Error creating quicklook mosaic file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################\n",
    "# MAIN SCRIPT\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 1. Search the ESA Copernicus Sentinel Data Hub for all available images \n",
    "#    over a region within a defined time period and low cloud cover\n",
    "#######################################################################\n",
    "\n",
    "os.chdir(rootdir) # go to working directory\n",
    "\n",
    "# load user credentials for Sentinel Data Hub at ESA, i.e. read two lines of text with username and password\n",
    "with open(credentials) as f:\n",
    "    lines = f.readlines()\n",
    "username = lines[0].strip()\n",
    "password = lines[1].strip()\n",
    "f.close()\n",
    "\n",
    "# Define the API\n",
    "api = SentinelAPI(username, password, 'https://scihub.copernicus.eu/dhus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the shapefile into GeoJSON if not already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the shapefile to geojson as required by the Sentinel Hub\n",
    "gjfile = shapefile.split(\".\")[0]+\".geojson\"\n",
    "\n",
    "# check whether it exists already, e.g. from a previous run, in that case skip this step\n",
    "if not os.path.exists(gjfile):\n",
    "    com = \"ogr2ogr -f GeoJSON -t_srs crs:84 \" + gjfile + \" \" + shapefile\n",
    "    flag = os.system(com)\n",
    "    if flag == 0:\n",
    "        print('Shapefile converted to Geojson format: ' + gjfile)\n",
    "    else:\n",
    "        print('Error converting shapefile to Geojson')\n",
    "else:\n",
    "    print('Geojson file already exists: ' + gjfile)\n",
    "\n",
    "# convert the geojson to wkt for the API search on the Sentinel Hub\n",
    "footprint = geojson_to_wkt(read_geojson(gjfile))\n",
    "\n",
    "# get projection information from the shapefile to serve as output projection for the mosaic\n",
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "shp = driver.Open(shapefile)\n",
    "layer = shp.GetLayer()\n",
    "outSpatialRef = layer.GetSpatialRef().ExportToWkt()\n",
    "shp = None # close file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the ESA Sentinel data hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set query parameters and search the Sentinel data hub\n",
    "query_kwargs = {\n",
    "        'area': footprint,\n",
    "        'date': (datefrom, dateto),\n",
    "        'platformname': 'Sentinel-2',\n",
    "        'processinglevel': 'Level-2A',\n",
    "        'cloudcoverpercentage': clouds\n",
    "        }\n",
    "\n",
    "# search the Sentinel data hub API\n",
    "products = api.query(**query_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of products to Pandas DataFrame\n",
    "products_df = api.to_dataframe(products)\n",
    "print('Search resulted in '+str(products_df.shape[0])+' satellite images with '+\n",
    "      str(products_df.shape[1])+' attributes.')\n",
    "\n",
    "os.chdir(outdir) # set working direcory to directory for our text file outputs\n",
    "\n",
    "# sort the search results\n",
    "products_df_sorted = products_df.sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True])\n",
    "print(products_df_sorted)\n",
    "outfile = 'searchresults_full.csv'\n",
    "products_df_sorted.to_csv(outfile)\n",
    "print(\"Search results saved: \" + outfile)\n",
    "\n",
    "# limit to first Ndown products sorted by lowest cloud cover and earliest acquisition date\n",
    "products_df_n = products_df_sorted.head(ndown)\n",
    "outfile = 'searchresults4download.csv'\n",
    "products_df_n.to_csv(outfile)\n",
    "print(\"Download list saved: \" + outfile)\n",
    "\n",
    "# get the footprints of the selected scenes\n",
    "s2footprints = products_df_n.footprint\n",
    "outfile = 'searchresultsfootprints.csv'\n",
    "s2footprints.to_csv(outfile, header = False)\n",
    "print(\"Granule footprints saved: \" + outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data. This takes a long time if many images are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 2. Download all selected images into a data directory\n",
    "#######################################################################\n",
    "\n",
    "os.chdir(downloaddir) # set working direcory to download directory\n",
    "\n",
    "# download sorted and reduced products in order\n",
    "api.download_all(products_df_n['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the footprints of the scenes marked for download together with their metadata in a Geojson file\n",
    "# first, run a new query to get the metadata for the selected scenes\n",
    "products_n = OrderedDict()\n",
    "for uuid in products_df_n['uuid']:\n",
    "    kw = query_kwargs.copy()\n",
    "    kw['uuid'] = uuid\n",
    "    pp = api.query(**kw)\n",
    "    products_n.update(pp)\n",
    "\n",
    "# then, write the footprints and metadata to a geojson file\n",
    "outfile = join(outdir, 'footprints.geojson')\n",
    "with open(outfile, 'w') as f:\n",
    "    json.dump(api.to_geojson(products_n), f)\n",
    "print(\"Granule footprints saved as GeoJson: \" + outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 3. Convert all images into reprojected Geotiff files (retain only the 10 m resolution bands)\n",
    "#######################################################################\n",
    "    \n",
    "# set working direcory to download directory\n",
    "os.chdir(downloaddir)\n",
    "\n",
    "# get list of all zip files in the data directory\n",
    "allfiles = [f for f in listdir(downloaddir) if isfile(join(downloaddir, f))]\n",
    "\n",
    "# unzip all these files and convert the 10 m bands to Geotiff\n",
    "\n",
    "for x in range(len(allfiles)):\n",
    "\n",
    "    if allfiles[x].split(\".\")[1] == \"zip\":\n",
    "        \n",
    "        print(\"Unzipping file \", x+1, \": \", allfiles[x])\n",
    "        with zipfile.ZipFile(allfiles[x], \"r\") as zipf:\n",
    "            zipf.extractall(downloaddir)\n",
    "\n",
    "        # in this directory are the Sentinel-2, 10 m resolution band files\n",
    "        \n",
    "        # extract *.SAFE subdirectory path, then GRANULE, then L2A_*, then IMG_DATA, then R10m\n",
    "        sceneID = allfiles[x].split(\".\")[0] # the first part of the directory name is the granule ID\n",
    "\n",
    "        # find *.SAFE subdirectory\n",
    "        dirlist = [d for d in listdir(downloaddir) if isdir(join(downloaddir, d))]\n",
    "        for y in range(len(dirlist)):\n",
    "            if dirlist[y].split(\".\")[1] == \"SAFE\":\n",
    "                thisdir = join(downloaddir, dirlist[y])\n",
    "\n",
    "        # find GRANULE subdirectory\n",
    "        thisdir = join(thisdir, \"GRANULE\")\n",
    "\n",
    "        # find L2A_* subdirectory\n",
    "        dirlist = [d for d in listdir(thisdir) if isdir(join(thisdir, d))]\n",
    "        for y in range(len(dirlist)):\n",
    "            if dirlist[y].split(\"_\")[0] == \"L2A\":\n",
    "                thisdir = join(thisdir, dirlist[y])\n",
    "\n",
    "        # find IMG_DATA subdirectory\n",
    "        thisdir = join(thisdir, \"IMG_DATA\")\n",
    "\n",
    "        # find R10m subdirectory\n",
    "        thisdir = join(thisdir, \"R10m\")\n",
    "\n",
    "        # call our function to convert the granule into a reprojected Geotiff file\n",
    "        s2tiff(thisdir,tiffdir, sceneID, outSpatialRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 4. Save quicklooks of all image mosaics for each image acquisition date\n",
    "#######################################################################\n",
    "\n",
    "# declare a function to eliminate duplicate entries in a list by converting to a dictionary and back\n",
    "def remove_duplicates(x):\n",
    "    return list(dict.fromkeys(x))\n",
    "\n",
    "# get list of all tiff files in the data directory\n",
    "allfiles = [f for f in listdir(tiffdir) if (isfile(join(tiffdir, f)) and f.endswith('.tif'))]\n",
    "\n",
    "print(\"\\nAll files:\")\n",
    "print(allfiles)\n",
    "\n",
    "# pull out all acquisition dates from the file names\n",
    "# for the Sentinel-2 naming convention, see \n",
    "#     https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/naming-convention\n",
    "acqdates = remove_duplicates([f.split(\"_\")[2].split(\"T\")[0] for f in allfiles])\n",
    "\n",
    "print(\"\\nAll acquisition dates (sorted):\")\n",
    "print([f for f in sorted(acqdates)])\n",
    "\n",
    "# go through all dates\n",
    "for d in acqdates:\n",
    "    \n",
    "    # pull out all file names of the same acquisition date\n",
    "    files2mosaic = [f for f in allfiles if (f.split(\"_\")[2].split(\"T\")[0] == d)]\n",
    "\n",
    "    print(\"\\nImages taken on date \" + d)\n",
    "    print(files2mosaic)\n",
    "    \n",
    "    # define a filename for the output file\n",
    "    mosaicID = \"S2_\" + d\n",
    "\n",
    "    # call our function\n",
    "    quicklooks(tiffdir, files2mosaic, quickdir, mosaicID, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the main functions, including one to extract zonal statistics from the polygons in the shapefile for all 10 m resolution bands of an image, and another one to loop over the entire time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 5. Extract polygon statistics of NDVI as a time series from all data\n",
    "#######################################################################\n",
    "\n",
    "def tif2raster(tiffile):\n",
    "    # read in a geotiff file (unsigned 16-bit integer format) and turn into a Python raster object\n",
    "    img_ds = io.imread(tiffile)\n",
    "    # convert to 16bit numpy array \n",
    "    img = np.array(img_ds, dtype='int16')\n",
    "    return(img)\n",
    "\n",
    "def zonal_stats(feat, input_zone_polygon, input_tiff, verbose = False, graphics = True, bands=[1,2,3]):\n",
    "    '''\n",
    "        extracts statistics from a feature (polygon) of a shapefile from a tiff file with several bands\n",
    "        heavily adapted from:\n",
    "        https://gis.stackexchange.com/questions/208441/zonal-statistics-of-a-polygon-and-assigning-mean-value-to-the-polygon\n",
    "    '''\n",
    "\n",
    "    if verbose:\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        print(\"Processing raster file: \" + input_tiff)\n",
    "        \n",
    "    # Open vector data\n",
    "    shp = ogr.Open(input_zone_polygon)\n",
    "    lyr = shp.GetLayer()\n",
    "\n",
    "    # Get raster georeference info\n",
    "    dataset = gdal.Open(input_tiff, gdal.GA_ReadOnly)\n",
    "    if not dataset:\n",
    "        print(\"Error. Tiff file not found: \" + input_tiff)\n",
    "\n",
    "    ncols = dataset.RasterXSize\n",
    "    nrows = dataset.RasterYSize\n",
    "    nbands = dataset.RasterCount\n",
    "    wkt_projection = dataset.GetProjection()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Raster size: {} x {} x {}\".format(ncols, nrows, nbands))\n",
    "        print(\"Raster projection: {}\".format(wkt_projection))\n",
    "\n",
    "    geotransform = dataset.GetGeoTransform()\n",
    "    if geotransform:\n",
    "        xOrigin = geotransform[0]\n",
    "        yOrigin = geotransform[3]\n",
    "        pixelWidth = geotransform[1]\n",
    "        pixelHeight = geotransform[5]\n",
    "        if verbose:\n",
    "            print(\"Tiff file image origin xmin,yax = ({}, {})\".format(geotransform[0], geotransform[3]))\n",
    "            print(\"Pixel Size x,y = ({}, {})\".format(geotransform[1], geotransform[5]))\n",
    "    else:\n",
    "        print(\"Error. Geotransform not found in tiff file: \" + input_tiff) \n",
    "\n",
    "    # Reproject vector geometry to same projection as raster\n",
    "    sourceSR = lyr.GetSpatialRef()\n",
    "    targetSR = osr.SpatialReference()\n",
    "    targetSR.ImportFromWkt(wkt_projection)\n",
    "    coordTrans = osr.CoordinateTransformation(sourceSR,targetSR)\n",
    "    feat = lyr.GetNextFeature()\n",
    "    geom = feat.GetGeometryRef()\n",
    "    geom.Transform(coordTrans)\n",
    "\n",
    "    # Get extent of the feature for which to extract the raster statistics\n",
    "    geom = feat.GetGeometryRef()\n",
    "    if (geom.GetGeometryName() == 'MULTIPOLYGON'):\n",
    "        count = 0\n",
    "        pointsX = []; pointsY = []\n",
    "        for polygon in geom:\n",
    "            geomInner = geom.GetGeometryRef(count)\n",
    "            ring = geomInner.GetGeometryRef(0)\n",
    "            numpoints = ring.GetPointCount()\n",
    "            for p in range(numpoints):\n",
    "                    lon, lat, z = ring.GetPoint(p)\n",
    "                    pointsX.append(lon)\n",
    "                    pointsY.append(lat)\n",
    "            count += 1\n",
    "    elif (geom.GetGeometryName() == 'POLYGON'):\n",
    "        ring = geom.GetGeometryRef(0)\n",
    "        numpoints = ring.GetPointCount()\n",
    "        pointsX = []; pointsY = []\n",
    "        for p in range(numpoints):\n",
    "                lon, lat, z = ring.GetPoint(p)\n",
    "                pointsX.append(lon)\n",
    "                pointsY.append(lat)\n",
    "\n",
    "    else:\n",
    "        sys.exit(\"ERROR: Geometry needs to be either Polygon or Multipolygon\")\n",
    "\n",
    "    # extent of this feature in map coordinates\n",
    "    xmin = min(pointsX)\n",
    "    xmax = max(pointsX)\n",
    "    ymin = min(pointsY)\n",
    "    ymax = max(pointsY)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nFeature extent in map coordinates: xmin,xmax, ymin, ymax\", xmin, xmax, ymin, ymax)\n",
    "       \n",
    "    # raster layer extent in map coordinates\n",
    "    xmin1 = xOrigin\n",
    "    xmax1 = xOrigin + ncols * pixelWidth\n",
    "    ymin1 = yOrigin + nrows * pixelHeight\n",
    "    ymax1 = yOrigin\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nGeotiff raster extent in map coordinates: xmin1, xmax1, ymin1, ymax1\")\n",
    "        print(xmin1,xmax1,ymin1,ymax1)\n",
    "    \n",
    "    # extent of the intersect between the featurs / polygon / zone and the raster image in map coordinates\n",
    "    xmin2 = max(xmin,xmin1)\n",
    "    xmax2 = min(xmax,xmax1)\n",
    "    ymin2 = max(ymin,ymin1)\n",
    "    ymax2 = min(ymax,ymax1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nIntersect extent in map coordinates: xmin2, xmax2, ymin2, ymax2\")\n",
    "        print(xmin2,xmax2,ymin2,ymax2)\n",
    "    \n",
    "    # clip the Geotiff raster layer to the extent of the intersect with the feature\n",
    "    xoff = int((xmin2 - xOrigin)/pixelWidth)\n",
    "    yoff = int((yOrigin - ymax2)/pixelWidth)\n",
    "    xcount = int((xmax2 - xmin2)/pixelWidth)\n",
    "    ycount = int((ymax2 - ymin2)/pixelWidth)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nclipped raster layer extent in pixels: xoff, yoff, xcount, ycount\")\n",
    "        print(\"xoff = \", xoff)\n",
    "        print(\"yoff = \", yoff)\n",
    "        print(\"xcount = \", xcount)\n",
    "        print(\"ycount = \", ycount)\n",
    "\n",
    "    # clip the rasterised feature layer to the extent of the intersect with the feature\n",
    "    xoff1 = int((xmin2 - xmin)/pixelWidth)\n",
    "    yoff1 = int((ymax - ymax2)/pixelWidth)\n",
    "    xcount1 = int((xmax2 - xmin2)/pixelWidth)\n",
    "    ycount1 = int((ymax2 - ymin2)/pixelWidth)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nclipped feature layer extent in pixels: xoff1,yoff1,xcount1,ycount1\")\n",
    "        print(\"xoff1 = \", xoff1)\n",
    "        print(\"yoff1 = \", yoff1)\n",
    "        print(\"xcount1 = \", xcount1)\n",
    "        print(\"ycount1 = \", ycount1)\n",
    "\n",
    "    # Create memory target raster for the mask with all pixels within the feature (zone)\n",
    "    target_ds = gdal.GetDriverByName('MEM').Create('', xcount, ycount, 1, gdal.GDT_Byte)\n",
    "    target_ds.SetGeoTransform((xmin2, pixelWidth, 0, ymax2, 0, pixelHeight,))\n",
    "\n",
    "    # make projection the same as for the Geotiff Sentinel-2 raster file\n",
    "    target_ds.SetProjection(wkt_projection)\n",
    "\n",
    "    # create an empty array to hold the statistics results for all bands\n",
    "    stats = np.zeros((nbands, 7))\n",
    "\n",
    "    # plot a colour composite of three bands\n",
    "    if graphics:\n",
    "        # create an empty array to hold the 3 stretched colour bands\n",
    "        img = np.zeros((ycount, xcount, 3), dtype = np.int16)\n",
    "        for band in range(0, 3):\n",
    "            # Read intersecting Geotiff raster layer values as an array\n",
    "            \n",
    "            ################## TODO ######################\n",
    "            # memory error here\n",
    "            # downsample the image if too big\n",
    "            ################## TODO ######################\n",
    "            \n",
    "            thisband = np.array(dataset.GetRasterBand(bands[band]).ReadAsArray(xoff, yoff, xcount, ycount)).astype(np.float)\n",
    "            # Histogram equalisation stretching\n",
    "            img[:, :, band] = np.int16(exposure.equalize_hist(thisband, nbins=256, mask=None))\n",
    "            # Contrast stretching and byte scaling\n",
    "            # p2, p98 = np.percentile(thisband, (2, 98))\n",
    "            # img[:, :, band] = np.int16(exposure.rescale_intensity(thisband, in_range=(p2, p98)) * 255)\n",
    "        plt.figure().suptitle(\"Colour composite\")\n",
    "        io.imshow(img)\n",
    "        io.show()\n",
    "        img = None # free up memory\n",
    "        thisband = None\n",
    "\n",
    "    # Rasterize the feature (zone polygon) to the target raster layer\n",
    "    #   burn_value of 1 indicates that a pixels is within the feature (zone)\n",
    "    gdal.RasterizeLayer(target_ds, [1], lyr, burn_values=[1]) #, ['ALL_TOUCHED=TRUE']\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Zone mask raster size: \", target_ds.GetRasterBand(1).ReadAsArray().shape)\n",
    "        print(\"Zone mask raster min,max: \", np.nanmin(target_ds.GetRasterBand(1).ReadAsArray()), np.nanmax(target_ds.GetRasterBand(1).ReadAsArray()))\n",
    "\n",
    "    if graphics:\n",
    "        # Display the whole rasterised zone mask\n",
    "        plt.figure().suptitle(\"Zone mask\")\n",
    "        io.imshow(np.array(target_ds.GetRasterBand(1).ReadAsArray()), cmap='gray_r')\n",
    "\n",
    "    # loop over all bands\n",
    "    for band in range(1, nbands + 1):\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nBand \", band)\n",
    "        \n",
    "        # Read intersecting Geotiff raster layer values as an array\n",
    "        dataraster = np.array(dataset.GetRasterBand(band).ReadAsArray(xoff, yoff, xcount, ycount)).astype(np.float)\n",
    "\n",
    "        # clip the rasterised feature layer to the extent of the intersect with the geotiff raster layer\n",
    "        datamask = target_ds.GetRasterBand(1).ReadAsArray().astype(np.byte)\n",
    "\n",
    "        # do something here to catch situations where dataraster is all NaNs\n",
    "        #    replace 1 in datamask with 0 where NaNs are found in dataraster\n",
    "        datamask[np.isnan(dataraster)] = 0\n",
    "\n",
    "        warnme = 0 # no warnings so far\n",
    "\n",
    "        if (np.nanmin(dataraster) == np.nan and np.nanmax(dataraster) == np.nan):\n",
    "            print(\"WARNING: All dataraster values are NaN.\")\n",
    "            warnme = warnme + 1\n",
    "        else:\n",
    "            if (dataraster.shape == ()):\n",
    "                print(\"WARNING: Failed to create dataraster.\")\n",
    "                warnme = warnme + 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Data raster min, max: \", np.nanmin(dataraster), np.nanmax(dataraster))\n",
    "                            \n",
    "                if (np.min(datamask) == 0 and np.max(datamask) == 0):\n",
    "                    print(\"WARNING: All datamask values are zero.\")\n",
    "                    warnme = warnme + 1\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"Data mask min, max: \", np.min(datamask), np.max(datamask))\n",
    "                    \n",
    "                    # Apply the mask from the rasterised feature layer (zone polygon) to the Geotiff raster\n",
    "                    zoneraster = np.ma.MaskedArray(dataraster, datamask)\n",
    "                    dataraster = None # free up memory\n",
    "                    datamask = None\n",
    "                    \n",
    "                    if (np.min(zoneraster) == 0 and np.max(zoneraster) == 0):\n",
    "                        print(\"WARNING: All zoneraster values are zero.\")\n",
    "                        warnme = warnme + 1\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(\"Zone raster min, max: \", np.min(zoneraster.data), np.max(zoneraster.data))\n",
    "                \n",
    "                        if graphics:\n",
    "                            # histogram equalisation\n",
    "                            img = exposure.equalize_hist(zoneraster, nbins=256, mask=None)\n",
    "                            # Contrast stretching\n",
    "                            #p2, p98 = np.percentile(zoneraster, (2, 98))\n",
    "                            #img = exposure.rescale_intensity(zoneraster, in_range=(p2, p98))\n",
    "                            plt.figure().suptitle(\"Zone raster\")\n",
    "                            io.imshow(img, cmap='gray_r')\n",
    "\n",
    "        if graphics:\n",
    "            io.show()\n",
    "\n",
    "        # calculate band statistics under the zone mask\n",
    "        # N.B. The -1 is there to get Python indices from 0...3 from the band variable 1...4 (see for loop)\n",
    "        stats[band - 1, 0] = band\n",
    "        stats[band - 1, 1] = np.ma.average(zoneraster)\n",
    "        stats[band - 1, 2] = np.ma.mean(zoneraster)\n",
    "        stats[band - 1, 3] = np.ma.median(zoneraster)\n",
    "        stats[band - 1, 4] = np.ma.std(zoneraster)\n",
    "        stats[band - 1, 5] = np.ma.var(zoneraster)\n",
    "        stats[band - 1, 6] = np.ma.sum(zoneraster)       \n",
    "        \n",
    "    # close files\n",
    "    dataset = None\n",
    "    shp = None\n",
    "    \n",
    "    # free up memory\n",
    "    target_ds = None\n",
    "\n",
    "    if (warnme > 0):\n",
    "        print(warnme, \" WARNINGS issued. Returning zero values from the zone statistics function.\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def loop_zonal_stats(input_zone_polygon, input_tiff, verbose = False, graphics = False, bands = [1,2,3]):\n",
    "\n",
    "    shp = ogr.Open(input_zone_polygon)\n",
    "    lyr = shp.GetLayer()\n",
    "    featList = range(lyr.GetFeatureCount())\n",
    "    statDict = {}\n",
    "\n",
    "    for FID in featList:\n",
    "        if verbose:\n",
    "            print(\"Feature ID: \", FID)\n",
    "        feat = lyr.GetFeature(FID)\n",
    "        meanValue = zonal_stats(feat, input_zone_polygon, input_tiff, verbose, graphics)\n",
    "        statDict[FID] = meanValue\n",
    "    return statDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pull out all tile IDs from the file names\n",
    "# the tiling grid can be downloaded from here as a .kml file:\n",
    "#     https://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products\n",
    "tiles = sorted(remove_duplicates([f.split(\"_\")[5] for f in allfiles]))\n",
    "\n",
    "print(\"\\nAll tile IDs (sorted):\")\n",
    "for t in tiles:\n",
    "    print(t)\n",
    "print(\"\\n\") # new line\n",
    "\n",
    "# check whether shapefile exists\n",
    "if not os.path.exists(shapefile):\n",
    "    print('Shapefile not found: ' + shapefile)\n",
    "else:\n",
    "    # go through all tiles and extract statistics for all available acquisition dates for that tile\n",
    "    \n",
    "    for t in tiles:\n",
    "\n",
    "        # get all file names with the same tile ID from the list of all tiff files\n",
    "        filestack = [f for f in allfiles if (f.split(\"_\")[5] == t)]\n",
    "        print(\"\\nImage files acquired for tile \" + t)\n",
    "        for xfile in filestack:\n",
    "            print(xfile)\n",
    "\n",
    "        # pull out all acquisition dates from the file names\n",
    "        acqdates = remove_duplicates([f.split(\"_\")[2].split(\"T\")[0] for f in filestack])\n",
    "        print(\"\\nAcquisition dates for this tile (sorted):\")\n",
    "        print([f for f in sorted(acqdates)])\n",
    "        \n",
    "        # define a filename for the output text file with the statistics\n",
    "        timeseries = join(outdir, \"S2_\" + t + \".txt\")\n",
    "        print(\"\\nSaving tile statistics to file: \", timeseries)\n",
    "        outfile = open(timeseries, \"w\")\n",
    "\n",
    "        # write header line\n",
    "        outfile.write(\"AcqDate, Zone, Band, Average, Mean, Median, Std_Deviation, Variance, N_Pixels\\n\")\n",
    "\n",
    "        # for each acquisition date, get the polygon statistics of that tile\n",
    "        for xfile in filestack:\n",
    "\n",
    "            # get acquisition date for that file\n",
    "            acqdate = xfile.split(\"_\")[2].split(\"T\")[0]\n",
    "\n",
    "            # run the loop over all polygons (zones) and all image files\n",
    "            # the bands here will be shown as RGB if graphics is True\n",
    "            result_dict = loop_zonal_stats(shapefile, join(tiffdir, xfile), verbose = True, graphics = False, bands=[1,2,3])\n",
    "\n",
    "            # save acquisition dates and statistics of that tile to the statistics file\n",
    "            # for each zone (polygon ID) in the results dictionary, there is an array with the statistics for each band\n",
    "            for zone, stat in result_dict.items():\n",
    "                for band in range(stat.shape[0]):\n",
    "                    outfile.write(acqdate + ', ' + str(zone) + ', ')\n",
    "                    this_line = list(stat[band, ]) # line of statistics for output file\n",
    "                    for item in this_line:\n",
    "                        if item == this_line[0]: # first item in the line\n",
    "                            outfile.write(\"%s, \" % np.int32(item))\n",
    "                        else:\n",
    "                            if item == this_line[len(this_line) - 1]:  # last item in the line\n",
    "                                outfile.write(\"%s\\n\" % item)\n",
    "                            else:\n",
    "                                outfile.write(\"%s, \" % item) # all other items in the line\n",
    "\n",
    "        outfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your portfolio assignment\n",
    "1. Choose a different shapefile of your choice for a study area of interest.\n",
    "2. Define a date range for your analysis - bear in mind computing power.\n",
    "3. Run the analysis on your data.\n",
    "4. Present the results in your practical portfolio and discuss what they show.\n",
    "\n",
    "It is recommended to choose a study area with low cloud cover.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
