{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"practical_sync_week6_Sentinel2_NDVI_timeseries.ipynb","provenance":[{"file_id":"1lGfEvGesdlT5XG5PHPQ9gCG4zDDm1VJJ","timestamp":1629984130338}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"XUexW0CmhlF3"},"source":["# Week 6: Sentinel-2 time-series analysis\n","\n","Individual learning outcomes: At the end of this week, all students should be able to access Sentinel-2 time series data of a region of interest from Google Earth Engine via the Python API and visualise the results."]},{"cell_type":"markdown","metadata":{"id":"Fcro81WxTf4_"},"source":["# Sentinel-2 time-series\n","\n","Workflow for this practical:\n","* Define an area of interest based on an ESRI shapefile\n","* Define a time window for our data search\n","* Set a maximum acceptable cloud cover for our search\n","* Use Google Earth Engine to download a time-series of average NDVI values averaged over the area of interest\n","* Visualise the time series and fit a simple model\n"]},{"cell_type":"markdown","metadata":{"id":"___N_2vyp_f2"},"source":["Connect to our Google Drive from Colab."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"RvpK17wXp_f7"},"source":["# Load the Drive helper and mount your Google Drive as a drive in the virtual machine\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zMCt1egJyZpv"},"source":["Import required libraries"]},{"cell_type":"code","metadata":{"id":"cmglHMgnXhGw"},"source":["# install some libraries that are not on Colab by default\n","!pip install rasterio\n","!pip install geopandas\n","!pip install rasterstats\n","!pip install earthengine-api\n","!pip install requests\n","!pip install sentinelsat\n","\n","\n","# import libraries\n","import json\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","from osgeo import gdal, ogr\n","import os\n","from os import listdir\n","from os.path import isfile, isdir, join\n","import pandas as pd\n","from pprint import pprint\n","import rasterio\n","from rasterio import plot\n","from rasterio.plot import show_hist\n","from scipy import optimize\n","import shutil\n","import sys\n","import zipfile\n","import requests\n","import io\n","import webbrowser\n","import ee\n","\n","# make sure that this path points to the location of the pygge module on your Google Drive\n","libdir = '/content/drive/MyDrive/practicals21-22' # this is where pygge.py needs to be saved\n","if libdir not in sys.path:\n","    sys.path.append(libdir)\n","\n","# import the pygge module\n","import pygge\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DVsHiXZFeUnC"},"source":["# Authenticate to the Google Earth Engine API.\n","\n","API stands for 'application programming interface'. An API defines interactions between multiple software intermediaries, in this case between our Jupyter Notebook and the ESA Copernicus Data Hub. It defines the kinds of calls or requests that can be made, how to make them, the data formats that should be used, the conventions to follow etc. (text modified after Wikipedia)"]},{"cell_type":"code","metadata":{"id":"uVhdO4CYNXf4"},"source":["# Connect to Google Earth Engine API\n","# This will open a web page where you have to enter your account information and a code is provided. Paste it in the terminal.\n","!earthengine authenticate\n","\n","ee.Initialize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0-BMbjnTf5K"},"source":["# Set up the directory paths on Google Drive\n","Modify these string variables to match your data directory structure.\n","\n","BEFORE YOU RUN THIS CELL, EDIT THE VARIABLE wd BELOW TO POINT TO YOUR DIRECTORY ON GOOGLE DRIVE\n","\n","IMPORTANT: You must upload a shapefile of your area of interest to your Google Drive before running the next cell. Set the variable 'shapefile' below to point to this file. You can draw a polygon and save it as a shapefile on http://www.geojson.io."]},{"cell_type":"code","metadata":{"id":"wzmlF4GQD1ji"},"source":["# path to your permanent Google Drive \n","# (not so much space but will be kept after the session)\n","# EDIT THIS LINE (/content/drive/MyDrive is the top directory on Google Drive):\n","wd = \"/content/drive/MyDrive/practicals21-22\"\n","print(\"Connected to data directory: \" + wd)\n","\n","# path to your temporary drive on the Colab Virtual Machine \n","# (more disk space but will be deleted when Colab is closed)\n","cd = \"/content/work\"\n","\n","# directory for downloading data\n","downloaddir = join(cd, 'download') # where we save the downloaded images\n","\n","# CAREFUL: This code removes the named directories and everything inside them to free up space\n","# Note: shutil provides a lot of useful functions for file and directory management\n","try:\n","  shutil.rmtree(downloaddir)\n","except:\n","  print(downloaddir + \" not found.\")\n","\n","# create the new directories, unless they already exist\n","os.makedirs(cd, exist_ok=True)\n","os.makedirs(downloaddir, exist_ok=True)\n","\n","print(\"Connected to Colab temporary data directory: \" + cd)\n","\n","print(\"\\nList of contents of \" + wd)\n","for f in sorted(os.listdir(wd)):\n","  print(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYU_0NnEyiSP"},"source":["# Define our search parameters\n","\n","You can modify some of the parameters and upload your own shapefile."]},{"cell_type":"code","metadata":{"id":"jy8PKeYGTf5Q"},"source":["# EDIT THE SEARCH OPTIONS BELOW\n","\n","# YOU CAN PLACE A DIFFERENT SHAPEFILE ONTO YOUR GOOGLE DRIVE BUT MAKE SURE THAT\n","#    THE VARIABLE shapefile POINTS TO THE CORRECT FILE:\n","shapefile = join(wd, 'oakham', 'Polygons_small.shp') # ESRI Shapefile of the study area\n","\n","# Define a date range for our search\n","datefrom = '2021-01-01' # start date for imagery search\n","dateto   = '2021-08-31' # end date for imagery search\n","time_range = [datefrom, dateto] # format as a list\n","\n","# Define which cloud cover we accept in the images\n","clouds = 10 # maximum acceptable cloud cover in %"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFWXZq5TzLhr"},"source":["Get some information about our shapefile."]},{"cell_type":"code","metadata":{"id":"BDXQnbChE0b3"},"source":["# Get the shapefile layer's extent, CRS and EPSG code\n","extent, outSpatialRef, epsg = pygge.get_shp_extent(shapefile)\n","print(\"Extent of the area of interest (shapefile):\\n\", extent)\n","print(type(extent))\n","print(\"\\nCoordinate referencing system (CRS) of the shapefile:\\n\", outSpatialRef)\n","print('EPSG code: ', epsg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WAnLtG3HzPOj"},"source":["Get the extent of the shapefile into a format that Google Earth Engine understands.\n","\n","Look at the printed outputs of the type conversions. The code will make more sense then."]},{"cell_type":"code","metadata":{"id":"-tOoBk83HgsG"},"source":["# GEE needs a special format for defining an area of interest. \n","# It has to be a GeoJSON Polygon and the coordinates should be first defined in a list and then converted using ee.Geometry. \n","extent_list = list(extent)\n","print(extent_list)\n","print(type(extent_list))\n","# close the list of polygon coordinates by adding the starting node at the end again\n","# and make list elements in the form of coordinate pairs (y,x)\n","area_list = list([(extent[0], extent[2]),(extent[1], extent[2]),(extent[1], extent[3]),(extent[0], extent[3]),(extent[0], extent[2])])\n","print(area_list)\n","print(type(area_list))\n","\n","search_area = ee.Geometry.Polygon(area_list)\n","print(search_area)\n","print(type(search_area))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zx4KzrZSzcyR"},"source":["Now we can access the Sentinel-2 collection on Google Earth Engine and run our search. This will return a URL (web link) from which we can download the data."]},{"cell_type":"code","metadata":{"id":"tsv6J_uMYBCI"},"source":["# Obtain download links for image composites from an image collection on Google Earth Engine\n","# All products available are detailed on this page https://developers.google.com/earth-engine/datasets/.\n","\n","# Name of the Sentinel 2 image collection\n","s2collection = ('COPERNICUS/S2')\n","print(\"Image collection: \", s2collection)\n","\n","# select bands\n","bands = ['B4', 'B8']\n","print(bands)\n","\n","# spatial resolution of the downloaded data\n","resolution = 320 # in units of metres\n","print(\"resolution: \", resolution)\n","# get the Sentinel-2 image collection within the time range\n","s2collect = pygge.obtain_image_collection_sentinel(s2collection, time_range, search_area, clouds).select(bands)\n","\n","# ‘region’ is obtained from the area, but the format has to be adjusted using get_region(geom) method\n","search_region = pygge.get_region(search_area)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuMH8InRsMta"},"source":["# Get the time series data over the area of interest\n","\n","Now we calculate the average reflectances of the selected bands for each Sentinel-2 image in the image collection."]},{"cell_type":"code","metadata":{"id":"PP9n9Ckgsg88"},"source":["# Get the data for the pixel intersecting the point in urban area\n","# .getRegion outputs an array of values for each [pixel, band, image] tuple in an ImageCollection\n","# The output contains rows of id, lon, lat, time, and all bands for each image that \n","# intersects each pixel in the given region.\n","# Note that the getRegion function is limited to about 1 million values.\n","s2aoi = s2collect.getRegion(search_area, resolution).getInfo()\n","\n","# Preview the result\n","s2aoi[:4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndnHX_gqo3O_"},"source":["# We apply a function to get the two time series for each of the downloaded bands into a pandas dataframe\n","df = pygge.ee_array_to_df(s2aoi,['longitude', 'latitude', 'B4', 'B8'])\n","\n","# Calculate the vegetation index from B4 and B8 in the pandas data frame\n","df['ndvi'] =  (df['B8'] - df['B4']) / (df['B8'] + df['B4'])\n","\n","# drop all rows with NaN values in the NDVI column\n","df = df.dropna(0)\n","\n","print(df)\n","print(df.iloc[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-_Hdsig-6k6"},"source":["Let's find out some properties of the data frame and the values within it for the date and ndvi columns.\n"]},{"cell_type":"code","metadata":{"id":"TUivYMHb-wcd"},"source":["print(type(df['date']))\n","print(type(df['date'].iloc[0]))\n","print(df['date'])\n","\n","print(type(df['ndvi']))\n","print(type(df['ndvi'].iloc[0]))\n","print(df['ndvi'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_PD7HArt--KR"},"source":["We can see from the output that the 'date' series in the dataframe is a pandas.core.series.Series type object. Its individual values are of type str (string).\n","\n","The 'ndvi' series in the dataframe is of the same type but its individual values are of type float64 (floating point values encoded with 64 bits per value)."]},{"cell_type":"markdown","metadata":{"id":"mWyXXs23ADrg"},"source":["Work out the Julian day from the 'date' series. See https://quasar.as.utexas.edu/BillInfo/JulianDatesG.html for how is is done."]},{"cell_type":"code","metadata":{"id":"jrV4O-zoAIqx"},"source":["# express the date as Y M D, where Y is the year, M is the month number \n","#   (Jan = 1, Feb = 2, etc.), and D is the day of the month\n","\n","jd = [] # empty list to store the results in\n","\n","# iterate over all time index entries in the 'date' series\n","for t in range(len(df['date'].iloc[:])): # time index\n","\n","  # get year, month and date as integer values from the 'date' series\n","  y,m,d = pygge.split_YYYYMMDD(df['date'].iloc[t])\n","\n","  # add result to the new list\n","  jd.append(pygge.julian_date(y,m,d))\n","\n","# after the time loop has finished, add the Julian date list to the dataframe as a new column\n","df.insert(0, \"JD\", jd, True)\n","\n","print(df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEJdL7LZdroo"},"source":["# Because we are likely to have time series of NDVI over many pixels,\n","#   we want to plot a time series for each pixel location. These are\n","#   defined by the latitude and longitude values in the dataframe.\n","print(df.columns)\n","print(df['longitude'])\n","print(df['latitude'])\n","print(type(df['latitude'].iloc[0]))\n","\n","# get unique combinations of latitude and longitude for each pixel and store them in the dataframe\n","pixel_ids = []\n","\n","# iterate over all time index entries in the 'date' series\n","for t in range(len(df['date'].iloc[:])): # time index\n","  lon = df['longitude'].iloc[t]\n","  lat = df['latitude'].iloc[t]\n","  pixel_ids.append(str(lon)+'_'+str(lat))\n","\n","df.insert(1, \"lon_lat\", pixel_ids, True)\n","\n","print(df.columns)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4nUgJTD0f6T"},"source":["# Now that we have our data in a good shape, we can make plots\n","\n","# get a stratified split dataset by column lat_lon\n","df_grouped = df.groupby(df.lon_lat)\n","\n","print(\"Number of pixels: \", df_grouped.ngroups)\n","\n","# select maximum number of pixels to plot\n","npixels = 100\n","if npixels > df_grouped.ngroups:\n","  npixels = df_grouped.ngroups\n","\n","# define the model fitting function with parameters\n","def fit_model(t, a, b):\n","    return a + b * t\n","\n","# Set starting parameters for curve fitting\n","a = 0.\n","b = 0.1\n","\n","# Subplots.\n","fig, ax = plt.subplots(1, figsize=(6, 6))\n","\n","print(\"Making a plot with the following pixel locations:\")\n","pixels_for_plot = pd.unique(df['lon_lat'])[0:npixels]\n","print(pixels_for_plot)\n","\n","# iterate over each pixel based on its lon_lat location index\n","for p in pixels_for_plot:\n","\n","  # get group of NDVI values in the time series for pixel with index p\n","  this_pixel_series = df_grouped.get_group(p)\n","\n","  # extract the Julian dates available for that pixel from the dataframe into an array\n","  x = np.array(this_pixel_series['JD']).astype(float)\n","\n","  # extract the NDVI values from the dataframe as well\n","  y = np.array(this_pixel_series['ndvi']).astype(float)\n","\n","  # Add scatter plots\n","  #ax.scatter(this_pixel_series['JD'], this_pixel_series['ndvi'],\n","  #          c='green', alpha=0.2, label='NDVI')\n","  ax.scatter(x, y, c='green', alpha=0.2, label='NDVI')\n","\n","  # check that we have enough values in the time series for that pixel\n","  if len(x) < 5:\n","    print(\"pixel \" + p + \" has only \" + str(len(x)) + \" time points. Omitted from analysis.\")\n","  else:\n","    # fit the model to the NDVI time series\n","    params_u, params_covariance_u = optimize.curve_fit(fit_model, x, y, p0=[a,b])\n","\n","    # Add fitted curves\n","    ax.plot(this_pixel_series['JD'],\n","            fit_model(x, params_u[0], params_u[1]),\n","            label='fitted model', color='black', lw=1)\n","\n","# Add some parameters.\n","ax.set_title('NDVI near Rutland Water', fontsize=16)\n","ax.set_xlabel('Date', fontsize=14)\n","ax.set_ylabel('NDVI', fontsize=14)\n","ax.grid(lw=0.2)\n","#ax.legend(fontsize=14, loc='lower right')\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_q3ivxi4qBmj"},"source":["# Formative Assignment for this week\n","Write a new code cell that annotates the x axis with the date in the notation of \"Day Month Year\", e.g. \"4/12/2021\" or \"04122021\" whilst retaining the plotting position of the points on the x axis as the Julian date. You can do this by extracting a string from the 'date' column of the dataframe, splitting it into individual characters and putting them together again in the right order. Then you can define x axis labels using the following help page: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xlabel.html\n","\n","If you can do it, make the time series a bit longer and change the model from a linear model to a second order polynomial."]},{"cell_type":"code","metadata":{"id":"qmanzkHb7ta_"},"source":[""],"execution_count":null,"outputs":[]}]}