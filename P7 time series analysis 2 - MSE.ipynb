{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis in Python 2\n",
    "\n",
    "Multi-scale sample entropy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# set plotting option for notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before the next step we have to install the package 'netCDF4' by opening the Anaconda terminal with administrator rights and typing in:\n",
    "\n",
    "    conda install netCDF4\n",
    "    \n",
    "We also have to install the gdal (geospatial data language), geopandas (for geographic PANDAS data objects), geoplot (for map making) & proj4 (geographic projection database) packages for Python.\n",
    "    \n",
    "    conda install geopandas\n",
    "\n",
    "    conda install geoplot -c conda-forge\n",
    "    \n",
    "    conda install proj4\n",
    "    \n",
    "Geopandas also contains the GDAL package.\n",
    "Now we can import the netCDF file with the data and plot geographic maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from copy import copy, deepcopy\n",
    "import geopandas\n",
    "import geoplot as gplt\n",
    "import imageio #to make a movie loop\n",
    "import netCDF4 # import the libary to read netCDF format files\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from pymse_with_ci95 import PyMSE # the file pymse_with_ci95.py needs to be available locally\n",
    "from scipy import stats # import the linear regression model and plot functions\n",
    "from scipy import misc # import image visualisation\n",
    "from scipy.io import netcdf\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit # import the nonlinear regression model and plot functions\n",
    "from skimage import data, io, filters\n",
    "import sys\n",
    "io.use_plugin('matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space-time data analysis\n",
    "\n",
    "Now we will learn to run the multi-scale sample entropy on a stack of multitemporal spatial data. We have downloaded the Climate Research Unit's data on variance-adjusted global gridded temperature anomalies from the University of East Anglia.\n",
    "\n",
    "Let's set some options first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set options for plots to be produced\n",
    "monthlymaps = False # create global maps for each month?\n",
    "yearlymaps = True  # create global maps for each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CRUTEM4 temperature data from the web site\n",
    "\n",
    "# set working direcory\n",
    "wd = '/home/heiko/sf_GY7709_Satellite_Data_Analysis_in_Python/practicals/'\n",
    "# The lat/lon coordinate data and time stamp file is here:\n",
    "datafile = 'CRUTEM.4.6.0.0.variance_adjusted.nc'\n",
    "# The data file was downloaded from https://crudata.uea.ac.uk/cru/data/temperature/CRUTEM.4.6.0.0.variance_adjusted.nc\n",
    "\n",
    "# create a dataset object\n",
    "foo = netCDF4.Dataset(wd + datafile)\n",
    "\n",
    "print('What file format is the file in?')\n",
    "print(foo.file_format)\n",
    "\n",
    "print('Which dimensions does the netCDF file have?')\n",
    "print(foo.dimensions.keys())\n",
    "\n",
    "print('Which variables does the netCDF file have?')\n",
    "print(foo.variables.keys())\n",
    "\n",
    "print('How can we get the time into a new variable?')\n",
    "t = foo.variables['time']\n",
    "print(t)\n",
    "print(\"Number of time steps:\")\n",
    "nt = len(t)\n",
    "print(nt)\n",
    "print('First 10 values:')\n",
    "print(t[0:10,])\n",
    "\n",
    "# extract netcdf variables\n",
    "lat = foo.variables['latitude']\n",
    "lon = foo.variables['longitude']\n",
    "crutem4 = foo.variables['temperature_anomaly']\n",
    "print(\"Type of CRUTEM4 object: \" + str(type(crutem4)))\n",
    "\n",
    "print(\"Latitudes:\")\n",
    "print(lat.units)\n",
    "print(lat.shape)\n",
    "print(\"Longitudes:\")\n",
    "print(lon.units)\n",
    "print(lon.shape)\n",
    "print(\"Temperature anomaly:\")\n",
    "print(crutem4.units)\n",
    "print(crutem4.shape)\n",
    "\n",
    "# convert netcdf variables into geopandas object\n",
    "lat = foo.variables['latitude'][:]\n",
    "lon = foo.variables['longitude'][:]\n",
    "crutem4 = foo.variables['temperature_anomaly'][:]\n",
    "print(\"Type of CRUTEM4 object: \" + str(type(crutem4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have imported a whole lot of good libraries, we are much better prepared to do some serious plotting.\n",
    "First, we want to plot a histogram of our temperature data.\n",
    "We need to first of all remove the missing values from the data, which are coded as -1e+30 in the CRUTEM4v dataset.\n",
    "We also want to remove all zero values, which indicate water areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask out zeros and missing values in CRUTEM4v data\n",
    "# missing values in the temperature anomalies are coded as -1e+30\n",
    "crutem4.data[crutem4.data < -999.] = np.nan\n",
    "crutem4.data[crutem4.data == 0.] = np.nan\n",
    "tmin = crutem4[~np.isnan(crutem4)].min()\n",
    "tmax = crutem4[~np.isnan(crutem4)].max()\n",
    "print(\"tmin=\"+str(tmin)+\"  tmax=\"+str(tmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a generic plotting function that makes a global map of the temperature anomaly data and plots the coastlines of continents over it. We will use geopandas to make it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define world map\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "world = world[(world.pop_est>0) & (world.name!=\"Antarctica\")]\n",
    "\n",
    "# define a plotting function to make a global map\n",
    "def do_worldmap(image, lat, lon, ymin, ymax, file, title, label):\n",
    "    # image = 2D array\n",
    "    # lat, lon = arrays of latitude / longitude coordinates (same dimensions as image)\n",
    "    # ymin, ymax = values for boundaries of colour legend\n",
    "    # file = filename including path\n",
    "    # title = title string for map\n",
    "    # label = text for legend label\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 8)) # create the empty figure (size in inches)\n",
    "    ax1 = plt.subplot() # create an axis object for one plot\n",
    "    gplt.polyplot(world, ax=ax1, edgecolor='black') # add worldmap and temperature anomalies\n",
    "    plt.suptitle(title, fontsize=16) # add main plot title\n",
    "    im = ax1.imshow(image, vmax=ymax, vmin=ymin, cmap='jet', origin='lower', \n",
    "                    extent=(min(lon),max(lon),min(lat),max(lat))) # show image in subplot 1\n",
    "    # Note that with 'imshow', the direction of the vertical axis and thus the default extent\n",
    "    #    values (left, right, bottom, top) for top and bottom depend on the origin:\n",
    "    #    For origin == 'upper' the default is (-0.5, numcols-0.5, numrows-0.5, -0.5).\n",
    "    #    For origin == 'lower' the default is (-0.5, numcols-0.5, -0.5, numrows-0.5).\n",
    "    fig.colorbar(im, ax=ax1, label=label) # add colour bar legend\n",
    "    ax1.set_aspect('auto')\n",
    "    plt.savefig(file, bbox_inches='tight', pad_inches=0.1, figsize=(8.0, 4.0), dpi=150)\n",
    "    plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function called do_worldmap to standardise our map making, we can process all the months or years and make one map per time period automatically. We save all the files as tif files at the end of the function, so we can see the results in our file explorer.\n",
    "Once we have made monthly or annual images, how about we make a movie loop over all images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set limits for the colorbar display range\n",
    "tmin = -3\n",
    "tmax = 3\n",
    "# set label for legend of worldmaps\n",
    "label='temperature anomaly (oC)'\n",
    "\n",
    "nadisp = 0 # display value for NaN\n",
    "\n",
    "#now make maps for all months (optional)\n",
    "if monthlymaps:\n",
    "    mapdir = wd+'crutem4_maps_monthly'\n",
    "    if not os.path.isdir(mapdir):\n",
    "        os.mkdir(mapdir)\n",
    "        print('New directory has been created: ' + mapdir)\n",
    "    for year in range(1850, int(1850 + nt/12)):\n",
    "        for month in range(1, 13):\n",
    "            file = mapdir+'crutem4_map_'+str(year)+'_'+str(month)+'.tif'\n",
    "            if os.path.isfile(file): # check if the tif file already exists, if so then skip it\n",
    "                print(file+' already exists. Skipping to the next.')\n",
    "            else:\n",
    "                print(year, month)\n",
    "                title = 'CRUTEM4 '+str(month)+' '+str(year)\n",
    "                ti = (year-1850)*12 + month-1 #time index for array\n",
    "                image = crutem4[ti,:,:]\n",
    "                mask = image == np.nan\n",
    "                image[mask] = nadisp\n",
    "                do_worldmap(image, lat, lon, tmin, tmax, file, title, label)\n",
    "                print('Created file: '+file)\n",
    "    # make a movie of all monthly maps\n",
    "    imagestack = []\n",
    "    filenames = [f for f in listdir(mapdir) if isfile(join(mapdir, f))]\n",
    "    kargs = { 'duration': 0.5 }\n",
    "    for filename in filenames:\n",
    "        imagestack.append(imageio.imread(mapdir+filename))\n",
    "    imageio.mimsave(mapdir + 'crutem4movie_monthly.gif', imagestack, loop=0, **kargs)\n",
    "    print('Created file: '+mapdir + 'crutem4movie_monthly.gif')\n",
    "\n",
    "# now make maps for all years (optional)\n",
    "if yearlymaps:\n",
    "    mapdir = wd+'crutem4_maps_annual/'\n",
    "    if not os.path.isdir(mapdir):\n",
    "        os.mkdir(mapdir)\n",
    "        print('New directory has been created: ' + mapdir)\n",
    "    for year in range(1850, int(1850 + nt/12)):\n",
    "        file = mapdir+'crutem4_map_'+str(year)+'.tif'\n",
    "        if os.path.isfile(file): # check if the tif file already exists, if so then skip it\n",
    "            print(file+' already exists. Skipping to the next.')\n",
    "        else:\n",
    "            print(year)\n",
    "            image = []\n",
    "            n = [] # map of the number of zeros per grid cell over one year\n",
    "            for month in range(1,13):\n",
    "                ti = (year-1850)*12 + month - 1 # time index for array\n",
    "                imagem = crutem4[ti,:,:]\n",
    "                if month == 1: \n",
    "                    image = deepcopy(imagem)\n",
    "                    n = 1*(image == 0.) # returns 1 for cells with missing values and 0 otherwise\n",
    "                else:\n",
    "                    image = image+imagem\n",
    "                    n = n + 1*(image == 0.) \n",
    "                mask = imagem == np.nan\n",
    "                image[mask] = nadisp\n",
    "            image=image/(12-n)\n",
    "            title = 'CRUTEM4 '+str(year)\n",
    "            do_worldmap(image, lat, lon, tmin, tmax, file, title, label)\n",
    "            print('Created file: '+file)\n",
    "\n",
    "    # make a movie of all annual maps\n",
    "    imagestack = []\n",
    "    filenames = [f for f in listdir(mapdir) if isfile(join(mapdir, f))]\n",
    "    for filename in filenames:\n",
    "        imagestack.append(imageio.imread(mapdir+filename))\n",
    "    kargs = { 'duration': 0.5 }\n",
    "    imageio.mimsave(mapdir + '_crutem4movie.gif', imagestack, loop=0, **kargs)\n",
    "    print('Created file: '+mapdir + '_crutem4movie.gif')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, open powerpoint and insert the gif file with the movie into a slide. Now play the slide show. Voila!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your portfolio task\n",
    "\n",
    "Inspect the output tiff files and write a one-page summary of the data for your portfolio, including 2-3 small figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-scale entropy analysis\n",
    "\n",
    "We now treat the grid boxes of the temperature anomaly data as separate time series. For each time series, we want to calculate the multi-scale sample entropy (MSE) in that grid box for two separate chunks of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONS\n",
    "\n",
    "#Folder name for plots; used in build_dir_structure\n",
    "outfolder = 'crutem4_mse'\n",
    "\n",
    "##############################################################################\n",
    "# Set the parameters for MSE calculation\n",
    "##############################################################################\n",
    "\n",
    "maxscale = 48  # maximum scale factor\n",
    "r = 0.5        # tolerance r\n",
    "m = 3          # pattern length m\n",
    "q = 0.95       # probability for the confidence interval\n",
    "bp = 111*12    # define a breakpoint in the timeseries to analyse it in two chunks (here, after 111 years)\n",
    "minlen = 500   # minimum length of the time series chunks to be considered for analysis\n",
    "plottrends = True # create graphics files showing whether trends are present in the chunks of data\n",
    "cols = list(['black','red','blue','green', 'magenta','purple','brown']) # define color palette for maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "# Sets the directory tree up for the rest of the processing\n",
    "def build_dir_structure():\n",
    "    dirs = ('crutem4_maps_monthly', 'crutem4_maps_annual', outfolder, outfolder + '/data', outfolder + '/mse', outfolder + '/plots')\n",
    "    for dirName in dirs:\n",
    "        if not os.path.exists(wd + dirName):\n",
    "            os.mkdir(wd + dirName)\n",
    "            print(\"Directory \" , wd + dirName ,  \" created\")\n",
    "        else:\n",
    "            print(\"Directory \" , wd + dirName ,  \" already exists\")\n",
    "\n",
    "\n",
    "# define the nonlinear model\n",
    "def func(t, A, K, C):\n",
    "    return A * np.exp(-K*t) + C\n",
    "\n",
    "\n",
    "def fit_exp_nonlinear(t, y):\n",
    "    opt_parms, parm_cov = optimize.curve_fit(func, t, y, maxfev=15000)\n",
    "    A, K, C = opt_parms\n",
    "    return A, K, C\n",
    "\n",
    "\n",
    "def conf_int(sd, n, confidence=0.95):\n",
    "    # conf_int = standard error*t_coef\n",
    "    #\n",
    "    tcoef = stats.t.ppf(confidence, n)\n",
    "    se = sd/(np.sqrt(n))\n",
    "    ci = se*tcoef\n",
    "    return ci\n",
    "\n",
    "\n",
    "# define functions to read/write floating point numbers from/to a text file\n",
    "def read_floats(filename):\n",
    "    with open(filename) as f:\n",
    "        return [float(x) for x in f]\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def write_floats(data, filename):\n",
    "    file = open(filename, 'w')\n",
    "    for item in data:\n",
    "        file.write(\"%f\\n\" % item)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "# define a plotting function for the maps\n",
    "def do_plot(image, file, title, tmin, tmax):\n",
    "    \"\"\"\"\"\"\n",
    "    plt.figure(figsize=(12.0, 9.0),dpi=150)\n",
    "    plt.title(title)\n",
    "    plt.subplot(1, 1, 1)\n",
    "    io.imshow(image, cmap=plt.cm.jet, vmin=tmin, vmax=tmax), plt.savefig(file, figsize=(12.0, 9.0),dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# make the directory structure if it does not exist\n",
    "##############################################################################\n",
    "\n",
    "build_dir_structure()\n",
    "\n",
    "########################################################################\n",
    "from scipy.stats import t\n",
    "import collections\n",
    "########################################################################\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def read_data(dataset):\n",
    "    \"\"\"\"\"\"\n",
    "    if isinstance(dataset, (str, bytes)):\n",
    "        assert os.path.isfile(dataset), \"Missing \\\"{}\\\" file.\".format(dataset)\n",
    "\n",
    "        with open(dataset, \"r\") as file:\n",
    "            dataset = np.array(list(map(float, file.readlines())))\n",
    "            file.close()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "class PyMSE:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    def __init__(self, dataset, scale=20, m=2, r=0.15, q=0.975):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        if isinstance(dataset, collections.Iterable):\n",
    "            self.data = np.array(dataset)\n",
    "        elif os.path.exists(dataset):\n",
    "            self.data = read_data(dataset)\n",
    "        self.data_cg = self.data.copy()\n",
    "        self.get(scale, m, r, q)  # Everything in get() should be in init()\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    def get(self, scale=20, m=2, r=0.15, q=0.975):\n",
    "\n",
    "        assert isinstance(scale, (int, list, tuple, map, range, np.ndarray)), \"scale must be int, list or tuple.\"\n",
    "        assert isinstance(m, (int, list, tuple, map, range, np.ndarray)), \"m must be int, list or tuple.\"\n",
    "        assert isinstance(r, (int, float, list, tuple, map, range, np.ndarray)), \"r must be int, float, list or tuple.\"\n",
    "        assert isinstance(q, (float)), \"q must be float.\"\n",
    "\n",
    "        #Scales\n",
    "        if isinstance(scale, int):\n",
    "            self.SCALE = [scale] # convert integer to a list of length 1\n",
    "        elif isinstance(scale, (list, tuple, range, np.ndarray)):\n",
    "            self.SCALE = scale # leave it unchanged\n",
    "        else:\n",
    "            self.SCALE = range(1, 21, 1) # set a default\n",
    "        self.scale_max = self.SCALE[-1]\n",
    "        if len(self.SCALE) > 1:\n",
    "            self.scale_step = self.SCALE[1] - self.SCALE[0]\n",
    "        else:\n",
    "            self.scale_step = 1\n",
    "\n",
    "        # q\n",
    "        self.q = [q]\n",
    "\n",
    "        # So, I _think_ that M is the range of resolutions for MSE calcs.\n",
    "        if isinstance(m, int):\n",
    "            self.M = [m]\n",
    "        elif isinstance(m, (list, tuple, range, np.ndarray)):\n",
    "            self.M = m\n",
    "        else:\n",
    "            self.M = range(2, 3, 1)  # This is a very roundabout way of saying '2'.\n",
    "        self.m_min = self.M[0]\n",
    "        self.m_max = self.M[-1]\n",
    "        if len(self.M) > 1:\n",
    "            self.m_step = self.M[1] - self.M[0]\n",
    "        else:\n",
    "            self.m_step = 1\n",
    "\n",
    "        if isinstance(r, (int, float)):\n",
    "            self.R = np.arange(r, r*1.0000000001, 0.05)\n",
    "        elif isinstance(r, list, tuple, range, np.ndarray):\n",
    "            self.R = r\n",
    "        else:\n",
    "            self.R = np.arange(r, r*1.0000000001, 0.05)\n",
    "        self.r_min = self.R[0]\n",
    "        self.r_max = self.R[-1]\n",
    "        if len(self.R) > 1:\n",
    "            self.r_step = self.R[1] - self.R[0]\n",
    "        else:\n",
    "            self.r_step = 0.05\n",
    "        \n",
    "        standard_deviation = self.data.std()\n",
    "        \n",
    "        SE = {}\n",
    "        for sc in self.SCALE:\n",
    "            self.__coarse_graining__(sc)\n",
    "            for r in self.R:\n",
    "                se = self.sample_entropy(r, standard_deviation, sc);\n",
    "                if not r in SE:\n",
    "                    SE[r] = []\n",
    "                SE[r].append(se)\n",
    "\n",
    "        self.DATA = []\n",
    "\n",
    "        for se_r in SE.keys():\n",
    "            ent = list(map(lambda *arg:arg, *SE[se_r]))\n",
    "            for m_ in range(len(self.M)):\n",
    "                entr = dict(map(lambda *arg:arg, self.SCALE, ent[m_]))\n",
    "                self.DATA.append({\"m\": self.M[m_], \"mse\": entr, \"r\": se_r, \"q\": q})\n",
    "\n",
    "        if len(self.DATA) == 1:\n",
    "            return self.DATA[0]\n",
    "        else:\n",
    "            return self.DATA\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    def __coarse_graining__(self, resolution):\n",
    "        out_len = int(len(self.data) / resolution)\n",
    "        out = np.empty([out_len, 1])\n",
    "        for i, subarray in enumerate(np.array_split(self.data, out_len)):\n",
    "            # numpy.array_split is identical to numpy.split, but won't raise an exception if the groups aren't equal length.\n",
    "            # If number of chunks > len(subarray) you get blank arrays nested inside.\n",
    "            # To address that you can remove empty arrays by:\n",
    "            subarray = [x for x in subarray if x.size > 0]\n",
    "            out[i] = np.mean(subarray)\n",
    "        self.data_cg = out\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    def sample_entropy(self, r, standard_deviation, scale=1, q=0.975):\n",
    "        \"\"\"\"\"\"\n",
    "        se = []\n",
    "        ci = []\n",
    "\n",
    "        nlin = float(len(self.data))\n",
    "        nlin_j = int((nlin/scale) - self.m_max)\n",
    "        r_new = r*standard_deviation\n",
    "\n",
    "        cont = [0] * (self.m_max+2)\n",
    "\n",
    "        for i in range(0, nlin_j):\n",
    "            for l in range(i+1, nlin_j):\n",
    "                k = 0\n",
    "                while k < self.m_max and (np.abs(self.data_cg[i+k] - self.data_cg[l+k]) <= r_new):\n",
    "                    k += 1\n",
    "                    cont[k] += 1\n",
    "                if k == self.m_max and (np.abs(self.data_cg[i+self.m_max] - self.data_cg[l+self.m_max]) <= r_new):\n",
    "                    cont[self.m_max+1] += 1\n",
    "\n",
    "        for i in self.M:\n",
    "            if cont[i+1] == 0 or cont[i] == 0:\n",
    "                if (nlin_j > 0) and ((nlin_j-1) > 0):\n",
    "                    se.append(-1 * np.log(1.0/(nlin_j*(nlin_j-1))))\n",
    "                    ci.append(standard_deviation * t.ppf(q, cont[i] - 1) / np.sqrt(cont[i]))\n",
    "                else: \n",
    "                    se.append(0.0)\n",
    "                    ci.append(0.0)\n",
    "            else:\n",
    "                se.append(-1 * np.log(float(cont[i+1])/cont[i]))\n",
    "                ci.append(standard_deviation * t.ppf(q, cont[i] - 1) / np.sqrt(cont[i]))\n",
    "        return se, ci\n",
    "\n",
    "\n",
    "    def conf_int(self, r, standard_deviation, scale=1):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        r\n",
    "        standard_deviation\n",
    "        scale\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        ci95 = []\n",
    "        \n",
    "        nlin = float(len(self.data))\n",
    "        nlin_j = int((nlin/scale) - self.m_max)\n",
    "        r_new = r * standard_deviation\n",
    "\n",
    "        cont = [0] * (self.m_max+2)\n",
    "\n",
    "        for i in range(0, nlin_j):\n",
    "            for l in range(i+1, nlin_j):\n",
    "                k = 0\n",
    "                while k < self.m_max and (np.abs(self.data_cg[i+k] - self.data_cg[l+k]) <= r_new):\n",
    "                    k += 1\n",
    "                    cont[k] += 1\n",
    "                if k == self.m_max and (np.abs(self.data_cg[i+self.m_max] - self.data_cg[l+self.m_max]) <= r_new):\n",
    "                    cont[self.m_max+1] += 1\n",
    "\n",
    "        for i in self.M:\n",
    "            if cont[i+1] == 0 or cont[i] == 0:\n",
    "                if (nlin_j > 0) and ((nlin_j-1) > 0):\n",
    "                    # calculate the confidence interval of the sample entropy at scale j after Richman and Moorman (2000)\n",
    "                    # We have B template matches of which A actually occur\n",
    "                    # Assign 1 to the A forward matches and 0 to the B-A potential forward matches that do not occur\n",
    "                    # The 95% confidence interval is then: SD * t(B-1, 0.975) / sqrt(B)\n",
    "                    # where SD is the standard deviation of the time-series\n",
    "                    # Here, A = cont[i]+1 and B = cont[i]\n",
    "                    ci95.append(np.std(self[1:nlin_j]) * t.ppf(0.975, cont[i]-1) / np.sqrt(cont[i]))\n",
    "                else: \n",
    "                    ci95.append(0.0)\n",
    "            else:\n",
    "                ci95.append(0.0)\n",
    "                \n",
    "        return ci95\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    def __standard_deviation__(self):\n",
    "        \"\"\"\"\"\"\n",
    "        nlin = float(len(self.data))\n",
    "        sum_ = sum(self.data)\n",
    "        sum2_ = sum(self.data*self.data)\n",
    "\n",
    "        return np.sqrt((sum2_ - sum_*(sum_/nlin))/(nlin - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Multi-scale entropy analysis of temperature data for each grid box\n",
    "##############################################################################\n",
    "\n",
    "# create arrays for the results\n",
    "msestack = np.zeros([36,72,2,maxscale]) # store the results of MSE for each grid box and chunk\n",
    "predict_msestack = np.zeros([36,72,2,maxscale]) # store the model predictions\n",
    "cistack = np.zeros([36,72,2,maxscale]) # store the confidence intervals\n",
    "\n",
    "# loop over all grid boxes\n",
    "for row in range(0,36):\n",
    "    for col in range(0,72):\n",
    "        # Pull out the time series data for this grid box\n",
    "        ts = np.array(crutem4[:,row,col]) # temperature time series data for that grid box\n",
    "\n",
    "        # Missing value removal is needed by PyMSE:\n",
    "        # The assumption is that we want to find a time-series starting at time 0+ti0\n",
    "        # and ending at time nt-ti1, where the missing values at the beginning and the end of the series\n",
    "        # are dropped. In case any missing values occur in the middle of the time series,\n",
    "        # these are interpolated with linear interpolation.\n",
    "\n",
    "        # find the first time index from which on no more missing values (NaN) are found for chunk 0\n",
    "        ti0 = -999 # remember the time index position\n",
    "        for ti in range(bp,0,-1):\n",
    "            subset = ts[ti:bp]\n",
    "            if np.all(~np.isnan(subset)): # (sum(1*(subset < -999)) == 0):\n",
    "                ti0 = ti # remember the time index position\n",
    "                ts0 = ts[ti0:bp]\n",
    "\n",
    "        # find the last time index before which no missing values are found for chunk 1\n",
    "        ti1 = -999 # remember the time index position\n",
    "        for ti in range(bp,nt,1):\n",
    "            subset = ts[bp:ti]\n",
    "            if np.all(~np.isnan(subset)): # (sum(1*(subset < -999)) == 0):\n",
    "                ti1 = ti # remember the time index position\n",
    "                ts1 = ts[bp:ti1]\n",
    "\n",
    "        # count the number of subsequent values that are not NaN in chunks 1 and 2\n",
    "        n1 = len(ts0)\n",
    "        n2 = len(ts1)\n",
    "        \n",
    "        # only proceed if both chunks of the time series are long enough\n",
    "        if (n1 >= minlen) & (n2 >= minlen):\n",
    "            print('Row '+str(row+1)+' Col '+str(col+1)+' suitable for analysis.  n='+str(nt)+' n1='+str(n1)+' n2='+str(n2))\n",
    "            \n",
    "        # plot the original time series data and the trend line if significant\n",
    "        # note that we will continue to use the original data in the MSE analysis\n",
    "        # and not the detrended data, because we do not want to change the variances\n",
    "            if plottrends:\n",
    "                file = wd + 'crutem4_mse/plots/crutem4_trends_R'+str(row+1)+'_C'+str(col+1)\n",
    "                title = 'CRUTEM4, R='+str(row+1)+' C='+str(col+1)\n",
    "                plt.figure(figsize=(12.0, 9.0),dpi=150)\n",
    "                f, axarr = plt.subplots(2, 2)\n",
    "                plt.title(title)\n",
    "                # trend analysis\n",
    "                x0 = range(ti0, bp)\n",
    "                x1 = range(bp, ti1)\n",
    "                slope0, intercept0, r_value0, p_value0, std_err0 = stats.linregress(x0,ts0)\n",
    "                slope1, intercept1, r_value1, p_value1, std_err1 = stats.linregress(x1,ts1)\n",
    "                axarr[0, 0].set_title('Time series TS0')\n",
    "                axarr[0, 0].plot(x0, ts0, '.k', ms=2)\n",
    "                if p_value0 < 0.05:\n",
    "                    pred0 = intercept0 + slope0 * x0\n",
    "                    axarr[0, 0].plot(x0, pred0, '-r')\n",
    "                    dts0 = ts0 - pred0\n",
    "                else:\n",
    "                    dts0 = ts0\n",
    "                axarr[1, 0].set_title('Detrended TS0')\n",
    "                axarr[1, 0].plot(x0, dts0, '.k', ms=2)\n",
    "                axarr[0, 1].set_title('Time series TS1')\n",
    "                axarr[0, 1].plot(x1, ts1, '.k', ms=2)\n",
    "                if p_value1 < 0.05:\n",
    "                    pred1 = intercept1 + slope1 * x1\n",
    "                    axarr[0, 1].plot(x1, pred1, '-r')\n",
    "                    dts1 = ts1 - pred1\n",
    "                else:\n",
    "                    dts1 = ts1\n",
    "                axarr[1, 1].set_title('Detrended TS1')\n",
    "                axarr[1, 1].plot(x1, dts1, '.k', ms=2)\n",
    "                # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "                plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "                plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False), plt.savefig(file, figsize=(12.0, 9.0),dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "            # write the chunks to a separate text file for further analysis\n",
    "            write_floats(ts0, outfolder+'/data/crutem4_ts_'+str(row+1)+'_'+str(col+1)+'_1.txt')\n",
    "            print('created text file: '+outfolder+'/data/crutem4_ts_'+str(row+1)+'_'+str(col+1)+'_1.txt')\n",
    "            write_floats(ts1, outfolder+'/data/crutem4_ts_'+str(row+1)+'_'+str(col+1)+'_2.txt')\n",
    "            print('created text file: '+outfolder+'/data/crutem4_ts_'+str(row+1)+'_'+str(col+1)+'_2.txt')\n",
    "\n",
    "            # convert time series into class 'PyMSE'\n",
    "            # this can be done by giving a filename or the name of an array or similar data object\n",
    "            ts0 = PyMSE(ts0)\n",
    "            ts1 = PyMSE(ts1)\n",
    "            '''\n",
    "            # Or:\n",
    "            ts0 = PyMSE(outfolder+'/data/crutem4_ts_'+str(row+1)+'_'+str(col+1)+'_1.txt')\n",
    "            ts1 = PyMSE(outfolder+'/data/crutem4_ts_'+str(row+1)+'_'+str(col+1)+'_2.txt')\n",
    "            '''\n",
    "            \n",
    "            # coarsegraining\n",
    "            ts0.data_cg\n",
    "            ts1.data_cg\n",
    "\n",
    "            # calculate the sample entropy, for r,std,scale_factor\n",
    "            mse0 = np.zeros([maxscale,])\n",
    "            ci0 = np.zeros([maxscale,])\n",
    "            cin0 = np.zeros([maxscale,])\n",
    "            mse1 = np.zeros([maxscale,])\n",
    "            ci1 = np.zeros([maxscale,])\n",
    "            cin1 = np.zeros([maxscale,])\n",
    "\n",
    "            # chunk 0\n",
    "            for i in range(0, maxscale):\n",
    "                sd = np.std(ts0.data)\n",
    "                ts0.get(scale=i+1,m=m,r=r,q=q) # get the sample entropy parameters into the object\n",
    "                out = ts0.sample_entropy(r, sd, scale=i+1, q=q) # calculate MSE for a given r, standard deviation, scale factor\n",
    "                mse0[i] = out[0][:][0]\n",
    "                ci0[i] = out[1][:][0]\n",
    "                \n",
    "            # chunk 1\n",
    "            for i in range(0, maxscale):\n",
    "                sd = np.std(ts1.data)\n",
    "                ts1.get(scale=i+1,m=m,r=r,q=q)\n",
    "                out = ts1.sample_entropy(r, sd, scale=i+1, q=q)\n",
    "                mse1[i] = out[0][:][0]\n",
    "                ci1[i] = out[1][:][0]\n",
    "                  \n",
    "            # Fit the exponential model, masking out SE >= 2\n",
    "            # these are values that could not be estimated from the data and have been assigned a theoretical upper bound \n",
    "            x = np.arange(1,maxscale+1,dtype=float)\n",
    "            A0, K0, C0 = fit_exp_nonlinear(x[mse0<2],mse0[mse0<2])\n",
    "            A1, K1, C1 = fit_exp_nonlinear(x[mse1<2],mse1[mse1<2])\n",
    "            predict_mse0 = func(x, A0, K0, C0)\n",
    "            predict_mse1 = func(x, A1, K1, C1)\n",
    "            \n",
    "            #Calculate prediction error\n",
    "            residuals0 = mse0 - predict_mse0\n",
    "            residuals1 = mse1 - predict_mse1\n",
    "            fres0 = sum(residuals0**2)\n",
    "            fres1 = sum(residuals1**2)\n",
    "    \n",
    "            #save results to text file\n",
    "            np.savetxt(outfolder+'/data/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_1.txt', \n",
    "                       np.column_stack((x, mse0, predict_mse0, ci0, residuals0)), delimiter='\\t')\n",
    "            print('Created text file: '+outfolder+'/data/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_1.txt')\n",
    "            np.savetxt(outfolder+'/data/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_2.txt', \n",
    "                       np.column_stack((x, mse1, predict_mse1, ci1, residuals1)), delimiter='\\t')\n",
    "            print('Created text file: '+outfolder+'/data/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_2.txt')\n",
    "    \n",
    "            # save MSE plot to graphics file, leaving out SE>=2\n",
    "            # these are values that could not be estimated from the data and have been assigned a theoretical upper bound \n",
    "            file = wd+'crutem4_mse/mse/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_1.tif'\n",
    "            fig1 = plt.figure(figsize=(12.0, 9.0),dpi=150) # The size of the figure is specified as (width, height) in inches\n",
    "            fig1 = plt.plot(x[mse0<2], mse0[mse0<2], 'ok')\n",
    "            fig1 = plt.plot(x[mse0<2], mse0[mse0<2], '--g')\n",
    "            fig1 = plt.plot(x[mse0<2], mse0[mse0<2] + ci0[mse0<2], ':g')\n",
    "            fig1 = plt.plot(x[mse0<2], mse0[mse0<2] - ci0[mse0<2], ':g')\n",
    "            #plt.ylim(0,2)\n",
    "            plt.xlabel('scale factor')\n",
    "            plt.ylabel('SE')\n",
    "            plt.title(('CRUTEM4, R%d C%d, part=%d, r=%4.2f, m=%d, sd=%4.2f') % (row+1,col+1,1,r,m,sd))\n",
    "            fig1 = plt.plot(x, predict_mse0, 'k-')\n",
    "            plt.savefig(file,figsize=(12.0, 9.0),dpi=150)\n",
    "            print('Created file: '+file)\n",
    "\n",
    "            # the same for chunk 2\n",
    "            file = wd+'crutem4_mse/mse/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_2.tif'\n",
    "            fig1 = plt.figure(figsize=(12.0, 9.0),dpi=150) # The size of the figure is specified as (width, height) in inches\n",
    "            fig1 = plt.plot(x[mse1<2], mse1[mse1<2], 'ok')\n",
    "            fig1 = plt.plot(x[mse1<2], mse1[mse1<2], '--g')\n",
    "            fig1 = plt.plot(x[mse1<2], mse1[mse1<2] + ci1[mse1<2], ':g')\n",
    "            fig1 = plt.plot(x[mse1<2], mse1[mse1<2] - ci1[mse1<2], ':g')\n",
    "            #plt.ylim(0,2)\n",
    "            plt.xlabel('scale factor')\n",
    "            plt.ylabel('SE')\n",
    "            plt.title(('CRUTEM4, R%d C%d, part=%d, r=%4.2f, m=%d, sd=%4.2f') % (row+1,col+1,2,r,m,sd))\n",
    "            fig1 = plt.plot(x, predict_mse1, 'k-'), \n",
    "            plt.savefig(file,figsize=(12.0, 9.0),dpi=150)\n",
    "            print('Created file: '+file)\n",
    "\n",
    "            #keep MSE estimates for later\n",
    "            msestack[row,col,0,:] = mse0\n",
    "            cistack[row,col,0,:] = ci0\n",
    "            predict_msestack[row,col,0,:] = predict_mse0\n",
    "            msestack[row,col,1,:] = mse1\n",
    "            cistack[row,col,1,:] = ci1\n",
    "            predict_msestack[row,col,1,:] = predict_mse1\n",
    "            \n",
    "            ymax = 1.05 * msestack.max() # set a common y axis range\n",
    "            ypredmax = 1.05 * predict_msestack.max() # set a common y axis range\n",
    "            x = np.arange(1,maxscale+1,dtype=float) # all scale factors on x axis\n",
    "        \n",
    "            # save MSE plots with predicted models for all chunks in one figure\n",
    "            file = wd+'crutem4_mse/plots/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_all.tif'\n",
    "            fig1 = plt.figure(figsize=(12.0, 9.0),dpi=150) # The size of the figure is specified as (width, height) in inches\n",
    "            for ch in range(0,2):\n",
    "                fig1 = plt.plot(x, msestack[row,col,ch,:], 'o', color=cols[ch], label = 'TS'+str(ch))\n",
    "                fig1 = plt.plot(x, msestack[row,col,ch,:], '--', color=cols[ch])\n",
    "                fig1 = plt.plot(x, msestack[row,col,ch,:] + cistack[row,col,ch,:], ':', color=cols[ch])\n",
    "                fig1 = plt.plot(x, msestack[row,col,ch,:] - cistack[row,col,ch,:], ':', color=cols[ch])\n",
    "                fig1 = plt.plot(x, predict_msestack[row,col,ch,:], '-', color=cols[ch])\n",
    "            plt.legend()\n",
    "            #plt.ylim(0,2)\n",
    "            plt.xlabel('scale factor')\n",
    "            plt.ylabel('SE')\n",
    "            plt.title(('CRUTEM4, R%d C%d, r=%4.2f, m=%d, sd=%4.2f') % (row+1,col+1,r,m,sd))\n",
    "            plt.savefig(file,figsize=(12.0, 9.0),dpi=150)\n",
    "            print('Created file: '+file)\n",
    "\n",
    "            # save model results to graphics file\n",
    "            file = wd+'crutem4_mse/plots/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_all_models.tif'\n",
    "            fig1 = plt.figure(figsize=(12.0, 9.0),dpi=150) # The size of the figure is specified as (width, height) in inches\n",
    "            for ch in range(0,2):\n",
    "                fig1 = plt.plot(x, predict_msestack[row,col,ch,:], '-', color=cols[ch], label = 'TS'+str(ch))\n",
    "            plt.legend()\n",
    "            plt.xlabel('scale factor')\n",
    "            plt.ylabel('SE')\n",
    "            plt.title(('CRUTEM4, R%d C%d, r=%4.2f, m=%d, sd=%4.2f') % (row+1,col+1,r,m,sd))\n",
    "            plt.savefig(file,figsize=(12.0, 9.0),dpi=150)\n",
    "            print('Created file: '+file)\n",
    "\n",
    "            # save MSE anomaly plot to graphics file, with chunk 0 being the baseline\n",
    "            file = wd+'crutem4_mse/plots/crutem4_mse_'+str(row+1)+'_'+str(col+1)+'_anomalies.tif'\n",
    "            fig1 = plt.figure(figsize=(12.0, 9.0),dpi=150) # The size of the figure is specified as (width, height) in inches\n",
    "            y = msestack[row,col,1,:] - msestack[row,col,0,:]\n",
    "            fig1 = plt.plot(x, y, 'o', color=cols[ch], label = 'TS1 anomaly')\n",
    "            fig1 = plt.plot(x, y, '--', color=cols[ch])\n",
    "            plt.legend()\n",
    "            plt.xlabel('scale factor')\n",
    "            plt.ylabel('SE anomaly')\n",
    "            plt.title(('CRUTEM4, R%d C%d, r=%4.2f, m=%d, sd=%4.2f') % (row+1,col+1,r,m,sd))\n",
    "            fig1 = plt.plot([0,maxscale], [0,0], 'k', linestyle='dotted')\n",
    "            plt.savefig(file,figsize=(12.0, 9.0),dpi=150)\n",
    "            print('Created figure file: '+file)\n",
    "\n",
    "            plt.close('all')\n",
    " \n",
    "            # now do the next column, then the next row\n",
    "\n",
    "        else:\n",
    "            # If time series chunks are not good for analysis, print a warning message\n",
    "            if (n1 < minlen) or (n2 < minlen):\n",
    "                print('WARNING, Row '+str(row+1)+' Col '+str(col+1)+' excluded from analysis: Not enough data points. nmin='+str(minlen)+' n='+str(nt)+ ' n1='+str(n1)+' n2='+str(n2))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took a while to process. Now let us save the results to some numpy data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your portfolio task\n",
    "\n",
    "Inspect the output files and write a one-page summary of the MSE data analysis results for 2-3 different types of grid boxes. Describe what you see in the data and how you interpret the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the array with the MSE results to file\n",
    "file = wd+'crutem4_mse/data/msestack.npy'\n",
    "msestack.dump(file)\n",
    "print('Created Numpy data file: '+file)\n",
    "\n",
    "# save the array with the ci for the MSE results to file\n",
    "file = wd+'crutem4_mse/data/cistack.npy'\n",
    "cistack.dump(file)\n",
    "print('Created Numpy data file: '+file)\n",
    "\n",
    "# save the array with the crutem4v data to file\n",
    "file = wd+'crutem4_mse/data/crutem4.npy'\n",
    "crutem4.dump(file)\n",
    "print('Created Numpy data file: '+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step, we want to visualise two maps that show globally where the sample entropy in the data has changed between the first and second chunk of the time series. We are interested in two quantities: (i) the greatest difference between the sample entropy of chunk 1 and 2 across all scales, and (ii) the scale at which that greatest change in entropy has occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Make global maps of MSE results\n",
    "##############################################################################\n",
    "\n",
    "# 1. Make a map of the greatest entropy change from TS1 to TS2\n",
    "# Map shows the magnitude of the largest change in sample entropy detected across all scale factors \n",
    "# Negative values show a decrease of SE from T0 to T1\n",
    "# Only statistically significant change is shown\n",
    "msechange = ma.asarray(msestack[:,:,1,:] - msestack[:,:,0,:])\n",
    "# mask out all grid cells where at least one of the chunks has zero value\n",
    "msechange.mask = ma.mask_or(msestack[:,:,1,:] == 0, msestack[:,:,0,:] == 0)\n",
    "# mask out spurious values (greater difference in SE than +- 2)\n",
    "msechange.mask = ma.mask_or(msechange.mask, msechange < -2)\n",
    "msechange.mask = ma.mask_or(msechange.mask, msechange > 2)\n",
    "# find maximum absolute change values over all scale factors for each grid cell\n",
    "msechangemax = msechange.max(axis=2)\n",
    "msechangemin = msechange.min(axis=2)\n",
    "mapdata = ma.copy(msechangemax)\n",
    "mapdata[msechangemax > abs(msechangemin)] = msechangemax[msechangemax > abs(msechangemin)]\n",
    "mapdata[msechangemax <= abs(msechangemin)] = msechangemin[msechangemax <= abs(msechangemin)]\n",
    "file = wd+'crutem4_mse/plots/map_magn_of_maxSEchange.tif'\n",
    "title = 'Largest change in SE'\n",
    "z = max(mapdata.min(), mapdata.max(), key=abs)\n",
    "zlim = -z, z\n",
    "# define colour range\n",
    "ymin = np.amin(mapdata)\n",
    "ymax = np.amax(mapdata)\n",
    "title = 'magnitude of greatest entropy change'\n",
    "label='greatest SE change'\n",
    "do_worldmap(mapdata, lat, lon, ymin, ymax, file, title, label)\n",
    "print('Created file: '+file)\n",
    "\n",
    "# 2. Make a map of the scale factor where the greatest change in SE occurs\n",
    "scalefacmax = np.argmax(msechange, axis=2) # find locations of maximum values over all scale factors for each grid cell\n",
    "scalefacmin = np.argmin(msechange, axis=2) # find locations of minimum values over all scale factors for each grid cell\n",
    "mapdata = ma.copy(scalefacmax)\n",
    "mapdata[msechangemax > abs(msechangemin)] = scalefacmax[msechangemax > abs(msechangemin)]\n",
    "mapdata[msechangemax <= abs(msechangemin)] = scalefacmin[msechangemax <= abs(msechangemin)]\n",
    "mapdata = ma.asarray(mapdata)\n",
    "mapdata.mask = msechangemax.mask\n",
    "file = wd+'crutem4_mse/plots/map_scalefactor_of_maxSEchange.tif'\n",
    "title = 'Scale factor with biggest change in SE'\n",
    "zlim = 0, mapdata.max()\n",
    "# define colour range\n",
    "ymin = np.amin(mapdata)\n",
    "ymax = np.amax(mapdata)\n",
    "title = 'scale factor of greatest entropy change'\n",
    "label = 'scale factor'\n",
    "do_worldmap(mapdata, lat, lon, ymin, ymax, file, title, label)\n",
    "print('Created file: '+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat, isn't it? Take a look at the global maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your portfolio task\n",
    "\n",
    "Inspect the output map files and write a half-page summary of the data and your interpretation of what they show for your portfolio, describing the 2 maps. Include the 2 maps on the second half of the page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
