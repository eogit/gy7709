{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "P08_zonalstats_with_cloudmask_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eogit/gy7709/blob/master/P08_zonalstats_with_cloudmask_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcro81WxTf4_",
        "colab_type": "text"
      },
      "source": [
        "# Analysing Sentinel-2 images and extracting time-series statistics\n",
        "\n",
        "Workflow for the practical:\n",
        "1. Search the ESA Copernicus Sentinel Data Hub for all available images over a region within a defined time period and low cloud cover\n",
        "2. Download all selected images into a data directory\n",
        "3. Convert all images into Geotiff files (retain only the 10 m resolution bands)\n",
        "4. Save quicklooks of all images\n",
        "5. Extract polygon statistics of the selected bands as a time series from all acquisition dates\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndMnc4ZaVPiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount your Google Drive as a drive in the virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# path to your Google Drive\n",
        "wd = \"/content/drive/My Drive/practicals/p08\"\n",
        "print(\"Connected to data directory: \" + wd)\n",
        "\n",
        "# set plotting option for notebook\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n6eL_6vTf5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "from collections import OrderedDict\n",
        "from copy import copy\n",
        "import gdal\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import ogr\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "import osr\n",
        "import shutil\n",
        "from skimage import exposure, io\n",
        "from skimage.morphology import binary_erosion, binary_dilation\n",
        "import subprocess\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "# install and import those packages not known to Colab by default\n",
        "!pip install sentinelsat\n",
        "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt\n",
        "\n",
        "#gdal.UseExceptions()\n",
        "#io.use_plugin('matplotlib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0-BMbjnTf5K",
        "colab_type": "text"
      },
      "source": [
        "Set up some directory names. \n",
        "\n",
        "Modify these to match your data directory structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms9KLrhyTf5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up your directories for the satellite data\n",
        "\n",
        "# directory for downloading the Sentinel-2 granules\n",
        "downloaddir = join(wd, 'download') \n",
        "# directory for the image tiff files we will make\n",
        "tiffdir = join(wd, 'tiff') \n",
        "# directory for the SCL tiff files we will make\n",
        "scltiffdir = join(wd, 'tiffscl') \n",
        "# quicklook directory\n",
        "quickdir = join(wd, 'quicklooks') \n",
        "# output directory for statistics file and search results\n",
        "outdir = join(wd, 'outputs') \n",
        "\n",
        "# CAREFUL: This code removes the named directories and everything inside them to free up space\n",
        "print(\"\\nList of contents of \" + wd)\n",
        "for f in sorted(os.listdir(wd)):\n",
        "  print(f)\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(downloaddir)\n",
        "except:\n",
        "  print(downloaddir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(tiffdir)\n",
        "except:\n",
        "  print(tiffdir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(scltiffdir)\n",
        "except:\n",
        "  print(scltiffdir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(quickdir)\n",
        "except:\n",
        "  print(quickdir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(outdir)\n",
        "except:\n",
        "  print(outdir + \" not found.\")\n",
        "\n",
        "print(\"\\nDeleted output directories to free up space.\")\n",
        "for f in sorted(os.listdir(wd)):\n",
        "  print(f)\n",
        "\n",
        "# create the new directories, unless they already exist\n",
        "os.makedirs(downloaddir, exist_ok=True)\n",
        "os.makedirs(tiffdir, exist_ok=True)\n",
        "os.makedirs(quickdir, exist_ok=True)\n",
        "os.makedirs(outdir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MWM3wcwTf5P",
        "colab_type": "text"
      },
      "source": [
        "Before proceeding, you have to make a text file called \"sencredentials.txt\" with your login details for the ESA Copernicus Sentinel Hub. The file has two lines of text.\n",
        "Line 1: Your username\n",
        "Line 2: Your password\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy8PKeYGTf5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download options and Data Hub search parameters\n",
        "ndown = 5 # number of scenes to be downloaded (in order of least cloud cover)\n",
        "shapefile = join(wd, 'field.shp') # ESRI Shapefile of the study area\n",
        "datefrom = '20190301' # start date for imagery search\n",
        "dateto   = '20191231' # end date for imagery search\n",
        "clouds = '[0 TO 20]' # range of acceptable cloud cover % for imagery search\n",
        "    # Note that later versions of the Sentinelsat package require this in the format: clouds = (0, 10) \n",
        "credentials = join(wd, 'sencredentials.txt')  # contains two lines of text with username and password\n",
        "\n",
        "# Filename options\n",
        "# VRT virtual raster stack of all image files\n",
        "vrtfile = \"mosaic.vrt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymlBzn9sTf5V",
        "colab_type": "text"
      },
      "source": [
        "Declare a function for converting all Sentinel-2 images in a directory into tiff files. Retain only the 10 m resolution bands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_kG2R-STf5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The next block of code is a function that reads in the Sentinel-2 L2A (Level 2A) image obtained \n",
        "#     from the Copernicus Sentinel Data Hub.\n",
        "# It also changes the file to a new projection if defined\n",
        "# Sentinel images can be obtained for free from this web site: https://scihub.copernicus.eu/dhus/#/home\n",
        "\n",
        "def s2tiff(indir, scldir, outdir, outscldir, ID, proj):\n",
        "    '''\n",
        "    indir = directory where the input band files are located\n",
        "    scldir = directory where the input SCL cloud classification file is located\n",
        "    outdir = directory where the image tiff file will be written if it does not already exist\n",
        "    outscldir = directory where the SCL tiff file will be written if it does not already exist\n",
        "    ID = a string giving the filename without file extension\n",
        "    proj = output projection\n",
        "    '''\n",
        "    # list all files in input directory    \n",
        "    print('Files in directory ' + indir)\n",
        "    allfiles = [f for f in listdir(indir) if isfile(join(indir, f))]\n",
        "    for f in allfiles:\n",
        "        print(f)\n",
        "\n",
        "    # make a band selection\n",
        "    print('Band files to be included in tiff file:')    \n",
        "    bands = allfiles[1:5]\n",
        "    for b in bands:\n",
        "        print(b)\n",
        "\n",
        "    # build a command line command for GDAL to convert the files into 10 m resolution VRT format\n",
        "    vrtfile = join(outdir, ID + '_16Bit.vrt')\n",
        "    cmd = ['gdalbuildvrt', '-resolution', 'user', '-tr' ,'10', '10', '-separate', vrtfile]\n",
        "    for band in bands:\n",
        "        cmd.append(join(indir, band))\n",
        "\n",
        "    if not os.path.exists(vrtfile): # skip if the output file already exists\n",
        "        print('\\n')\n",
        "        print(' '.join(cmd))\n",
        "        print('\\n')\n",
        "        subprocess.run(cmd) # execute the command in the command line\n",
        "    else:\n",
        "        print(vrtfile,' already exists.\\n')\n",
        "    \n",
        "    # check whether the output directory already exists and create it if not\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    os.makedirs(outscldir, exist_ok=True)\n",
        "\n",
        "    # define output file names\n",
        "    tiffile = join(outdir, ID + '_16Bit.tif')\n",
        "    projtiffile = join(outdir, ID + '_16Bit_proj.tif')\n",
        "\n",
        "    if not os.path.exists(projtiffile): # skip if the output file already exists\n",
        "        # build a command to translate the four band raster files into one geotiff file with 4 bands\n",
        "        cmd = ['gdal_translate', '-of' ,'GTiff', vrtfile, tiffile]\n",
        "        print(' '.join(cmd))\n",
        "        print('\\n')\n",
        "        # execute the GDAL command\n",
        "        subprocess.run(cmd) \n",
        "\n",
        "        # build a command to reproject the single Geotiff image to a new projection\n",
        "        cmd = ['gdalwarp', tiffile, projtiffile, '-t_srs', proj]\n",
        "        print(' '.join(cmd))\n",
        "        print('\\n')\n",
        "        # execute the GDAL command\n",
        "        subprocess.run(cmd) \n",
        "\n",
        "        os.remove(vrtfile) # delete the vrt file\n",
        "        os.remove(tiffile) # delete the tiffile in the old projection\n",
        "\n",
        "    else:\n",
        "        print(projtiffile,' already exists.\\n')\n",
        "\n",
        "    # build a command line command for GDAL to convert the SCL file into 10 m resolution VRT format\n",
        "    vrtfile = join(outscldir, ID + '_SCL16Bit.vrt')\n",
        "    cmd = ['gdalbuildvrt', '-resolution', 'user', '-tr' ,'10', '10', '-separate', vrtfile]\n",
        "    # find SCL file\n",
        "    allfiles = [f for f in listdir(scldir) if isfile(join(scldir, f))]\n",
        "    sclfile = [f for f in allfiles if \"SCL\" in f.split(\"_\")[2]]\n",
        "    print(type(sclfile))\n",
        "    for item in sclfile:\n",
        "        print('SCL file: ' + item)\n",
        "        cmd.append(join(scldir, item))\n",
        "    \n",
        "    if not os.path.exists(vrtfile): # skip if the output file already exists\n",
        "        print('\\n')\n",
        "        print(' '.join(cmd))\n",
        "        print('\\n')\n",
        "        subprocess.run(cmd) # execute the command in the command line\n",
        "    else:\n",
        "        print(vrtfile,' already exists.\\n')\n",
        "    \n",
        "    # define output file names\n",
        "    scltiffile = join(outscldir, ID + '_SCL16Bit.tif')\n",
        "    projscltiffile = join(outscldir, ID + '_SCL16Bit_proj.tif')\n",
        "\n",
        "    if not os.path.exists(projscltiffile): # skip if the output file already exists\n",
        "        # build a command to translate the cloud mask band raster file into a geotiff file\n",
        "        cmd = ['gdal_translate', '-of' ,'GTiff', vrtfile, scltiffile]\n",
        "        print(' '.join(cmd))\n",
        "        print('\\n')\n",
        "        # execute the GDAL command\n",
        "        subprocess.run(cmd) \n",
        "\n",
        "        # build a command to reproject the Geotiff to a new projection\n",
        "        cmd = ['gdalwarp', scltiffile, projscltiffile, '-t_srs', proj]\n",
        "        print(' '.join(cmd))\n",
        "        print('\\n')\n",
        "        # execute the GDAL command\n",
        "        subprocess.run(cmd) \n",
        "\n",
        "        print(\"Created file: \" + projscltiffile)\n",
        "        os.remove(vrtfile) # delete the vrt file\n",
        "        os.remove(scltiffile) # delete the tiffile in the old projection\n",
        "\n",
        "    else:\n",
        "        print(projscltiffile,' already exists.\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q27Ot9UUTf5a",
        "colab_type": "text"
      },
      "source": [
        "Declare a function for making one quicklook image in PNG format out of one or many TIFF files. The output file is smaller than the originals, i.e. not full resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3W-IDyATf5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function makes one PNG quicklook file by mosaicking all tiff files in a list of filenames\n",
        "# Note: the files must all the in the same projection\n",
        "\n",
        "def quicklooks(tiffdir, infilelist, outdir, mosaicID, outsize):\n",
        "    '''\n",
        "    tiffdir = directory in which the files in infilelist are located\n",
        "    infilelist = list of filenames (full path not included) to be processed\n",
        "    outdir = directory where the PNG quicklook files will be saved\n",
        "    mosaicID = beginning of the file name of the output files to be created (full path)\n",
        "    outsize = percentage downscaling factor, e.g. 10 means 10% of xsize and 10% of ysize\n",
        "    '''\n",
        "\n",
        "    # make output filenames\n",
        "    vrtfile = join(outdir, mosaicID + \".vrt\")\n",
        "    quicklookfile = join(outdir, mosaicID + \".png\")\n",
        "\n",
        "    print(vrtfile)\n",
        "    print(quicklookfile)\n",
        "    \n",
        "    # save the list of file names to a text file for use in the GDAL command\n",
        "    filelist = join(outdir, \"filelist.txt\") # make a file name\n",
        "    \n",
        "    print(filelist)\n",
        "\n",
        "    file1 = open(filelist,\"w\") # open file in write mode \n",
        "\n",
        "    for f in infilelist:\n",
        "        file1.write(join(tiffdir, f)) \n",
        "        file1.write(\"\\n\")\n",
        "        print(join(tiffdir, f))\n",
        "        print(os.path.exists(join(tiffdir, f)))\n",
        "    file1.close() \n",
        "\n",
        "    # make a quicklook classification mosaic of all granules in PNG format using GDAL\n",
        "    com = \"gdalbuildvrt -overwrite -input_file_list \" + filelist + \" \"+ vrtfile\n",
        "    print(com)\n",
        "    flag = os.system(com)\n",
        "    if flag == 0:\n",
        "        print('Created VRT file: ' + vrtfile)\n",
        "    else:\n",
        "        print('Error creating VRT file')\n",
        "\n",
        "    # Create a PNG quicklook, scaled from 0 - 255\n",
        "    com = \"gdal_translate -of PNG -ot Byte -scale -outsize \" + str(outsize) + \"% \" + \\\n",
        "        str(outsize) + \"% \" + vrtfile + \" \" + quicklookfile\n",
        "    print(com)\n",
        "    flag = os.system(com)\n",
        "    if flag == 0:\n",
        "        print('Created quicklook mosaic file: ' + quicklookfile)\n",
        "    else:\n",
        "        print('Error creating quicklook mosaic file')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjOj8jHTf5h",
        "colab_type": "text"
      },
      "source": [
        "Declare a function to read in the Sentinel 2 Level 2A product cloud mask and mask out cloudy pixels from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63uHPCDXTf5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentinel2_cloud_removal(imgfile, sclfile, outfile, maskfile, kernel_dil=(3, 3), kernel_ero=(7, 7), classes = [0, 1, 2, 3, 7, 8, 9, 10, 11]):\n",
        "    \"\"\"Motivated by the geospatial_learn package by Ciaran Robb. A further image erosion function\n",
        "    is applied to reduce the area around clouds missed by the sen2cor algorithm.\n",
        "\n",
        "    Removes clouds using the sen2cor scene classification. This is found in the .scl file of the L2A image folder.\n",
        "\n",
        "    Sen2cor Scene Classification Legend:\n",
        "    0 - NO_DATA (black)\n",
        "    1 - SATURATED_OR_DEFECTIVE (red)\n",
        "    2 - DARK_AREA_PIXELS (dark grey)\n",
        "    3 - CLOUD_SHADOWS (brown)\n",
        "    4 - VEGETATION (bright green)\n",
        "    5 - BARE_SOILS (yellow)\n",
        "    6 - WATER (blue)\n",
        "    7 - CLOUD_LOW_PROBILITY (grey)\n",
        "    8 - CLOUD_MEDIUM_PROBABILITY (light grey)\n",
        "    9 - CLOUD_HIGH_PROBABILITY (white)\n",
        "    10 - THIN_CIRRUS (light blue)\n",
        "    11 - SNOW (pink)\n",
        "\n",
        "    Based on this legend a reclassification will be carried out to generate a\n",
        "    cloud & QC mask. The identified mask classes will become 0 and the others will become 1.\n",
        "\n",
        "    imgfile: String indicating the input file name.\n",
        "    sclfile: String pointing to the respective sen2cor scene classification (SCL).\n",
        "    outfile: String containing the path and file name of the output file for the masked image\n",
        "    maskfile: String containing the path and file name of the mask file\n",
        "    kernel_dil: A tuple defining the dilation window applied to get rid of small cloud detections (often false classifications). Must be a pair of uneven numbers.\n",
        "    kernel_ero: A tuple defining the erosion window applied to reduced cloud fringes. Must be a pair of uneven numbers.\n",
        "    Returns nothing.\n",
        "    \"\"\"\n",
        "    \n",
        "    # split scl file paths\n",
        "    path_scl_new, fn_new = os.path.split(sclfile)\n",
        "\n",
        "    # split raster names\n",
        "    name_new, fmt_new = fn_new.split(\".\", 2)\n",
        "\n",
        "    # scl output names\n",
        "    scl_rcl_new_out = join(path_scl_new, name_new + '_rcl.' + fmt_new)\n",
        "\n",
        "    # read scl raster data and reclassify\n",
        "    ds_new = gdal.Open(sclfile, gdal.GA_ReadOnly)\n",
        "    bnd_new = ds_new.GetRasterBand(1)\n",
        "    array_new = bnd_new.ReadAsArray()\n",
        "\n",
        "    # recode the cloud mask to 0 (cloud and other masked areas) and 1 (good values)\n",
        "    array_new[np.isin(array_new, classes)] = 0\n",
        "    array_new[array_new != 0] = 1\n",
        "\n",
        "    # erode cloudmask using skimage.morphology module\n",
        "    #sieved_new = remove_small_objects(array_new, 10)\n",
        "    dilated_new = binary_dilation(array_new, np.ones(kernel_dil, dtype=np.int))\n",
        "    eroded_new = binary_erosion(dilated_new, np.ones(kernel_ero, dtype=np.int))\n",
        "\n",
        "    # read image raster data and apply the mask to all bands\n",
        "    img = gdal.Open(imgfile, gdal.GA_ReadOnly)\n",
        "    print(\"Raster band count: \", img.RasterCount)\n",
        "    \n",
        "    # save the cloud mask raster layer to file\n",
        "    print(maskfile)\n",
        "    print(img.RasterXSize)\n",
        "    print(img.RasterYSize)\n",
        "    print(ds_new.RasterCount)\n",
        "\n",
        "    output_raster = gdal.GetDriverByName('GTiff').Create(maskfile, img.RasterXSize, img.RasterYSize, ds_new.RasterCount, gdal.GDT_Int16) # Open the output file\n",
        "    output_raster.SetGeoTransform(img.GetGeoTransform()) # use the same Geotransform and projection as for the input file\n",
        "    output_raster.SetProjection(img.GetProjection())\n",
        "    output_raster.GetRasterBand(1).WriteArray(eroded_new) # Write the mask layer to the Geotiff file\n",
        "    output_raster.FlushCache()\n",
        "    # Close raster files\n",
        "    output_raster = None\n",
        "    ds_new = None\n",
        "    \n",
        "    # create an empty numpy array filled with zeros to hold all masked bands\n",
        "    img_masked = np.zeros((img.RasterCount, img.RasterYSize, img.RasterXSize)) # bands, rows, columns as in GDAL\n",
        "\n",
        "    # iterate over all image bands to apply the mask\n",
        "    for band in range(img.RasterCount):\n",
        "        # the range function starts at 0 but GDAL starts numbering at 1\n",
        "        print(\"Getting band \", band + 1)\n",
        "        bnd_new = img.GetRasterBand(band + 1)\n",
        "        bnd_array_new = bnd_new.ReadAsArray()\n",
        "        # mask out clouds from band image and save to file\n",
        "        img_masked[band,:,:] = np.where(eroded_new, bnd_array_new, 0) # apply the mask\n",
        "        \n",
        "    # Write raster dataset with all bands to a Geotiff file\n",
        "    output_raster = gdal.GetDriverByName('GTiff').Create(outfile, img.RasterXSize, img.RasterYSize, img.RasterCount, \\\n",
        "                                                         gdal.GDT_Int16)  # Open the file\n",
        "    output_raster.SetGeoTransform(img.GetGeoTransform()) # use the same Geotransform and projection as for the input file\n",
        "    output_raster.SetProjection(img.GetProjection())\n",
        "    for band in range(img.RasterCount):\n",
        "        output_raster.GetRasterBand(band+1).WriteArray(img_masked[band,:,:]) # Write each masked image band to the Geotiff file\n",
        "    output_raster.FlushCache()\n",
        "\n",
        "    # Close raster files\n",
        "    output_raster = None\n",
        "    img = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRTXwiJQTf5o",
        "colab_type": "text"
      },
      "source": [
        "#######################################################################\n",
        "# MAIN SCRIPT\n",
        "#######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ObLYYUITf5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################\n",
        "# 1. Search the ESA Copernicus Sentinel Data Hub for all available images \n",
        "#    over a region within a defined time period and low cloud cover\n",
        "#######################################################################\n",
        "\n",
        "os.chdir(wd) # go to working directory\n",
        "\n",
        "# load user credentials for Sentinel Data Hub at ESA, i.e. read two lines of text with username and password\n",
        "with open(join(wd, credentials)) as f:\n",
        "    lines = f.readlines()\n",
        "username = lines[0].strip()\n",
        "password = lines[1].strip()\n",
        "f.close()\n",
        "\n",
        "# Define the API\n",
        "api = SentinelAPI(username, password, 'https://scihub.copernicus.eu/dhus')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMwZ5lDQTf5s",
        "colab_type": "text"
      },
      "source": [
        "Convert the shapefile into GeoJSON if not already done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNPhP-VmTf5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the shapefile to geojson as required by the Sentinel Hub\n",
        "gjfile = shapefile.split(\".\")[0]+\".geojson\"\n",
        "\n",
        "# check whether it exists already, e.g. from a previous run, in that case skip this step\n",
        "if not os.path.exists(gjfile):\n",
        "    com = \"ogr2ogr -f GeoJSON -t_srs crs:84 \" + gjfile + \" \" + shapefile\n",
        "    flag = os.system(com)\n",
        "    if flag == 0:\n",
        "        print('Shapefile converted to Geojson format: ' + gjfile)\n",
        "    else:\n",
        "        print('Error converting shapefile to Geojson')\n",
        "else:\n",
        "    print('Geojson file already exists: ' + gjfile)\n",
        "\n",
        "# convert the geojson to wkt for the API search on the Sentinel Hub\n",
        "footprint = geojson_to_wkt(read_geojson(gjfile))\n",
        "\n",
        "# get projection information from the shapefile to serve as output projection for the mosaic\n",
        "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
        "shp = driver.Open(shapefile)\n",
        "layer = shp.GetLayer()\n",
        "outSpatialRef = layer.GetSpatialRef().ExportToWkt()\n",
        "shp = None # close file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYn1WnBTTf5z",
        "colab_type": "text"
      },
      "source": [
        "Search the ESA Sentinel data hub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC78b6POTf50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set query parameters and search the Sentinel data hub\n",
        "query_kwargs = {\n",
        "        'area': footprint,\n",
        "        'date': (datefrom, dateto),\n",
        "        'platformname': 'Sentinel-2',\n",
        "        'processinglevel': 'Level-2A',\n",
        "        'cloudcoverpercentage': clouds\n",
        "        }\n",
        "\n",
        "# search the Sentinel data hub API\n",
        "products = api.query(**query_kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT1Dg1fGTf54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert list of products to Pandas DataFrame\n",
        "products_df = api.to_dataframe(products)\n",
        "print('Search resulted in '+str(products_df.shape[0])+' satellite images with '+\n",
        "      str(products_df.shape[1])+' attributes.')\n",
        "\n",
        "os.chdir(outdir) # set working direcory to directory for our text file outputs\n",
        "\n",
        "# sort the search results\n",
        "products_df_sorted = products_df.sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True])\n",
        "#print(products_df_sorted)\n",
        "outfile = 'searchresults_full.csv'\n",
        "products_df_sorted.to_csv(outfile)\n",
        "print(\"Search results saved: \" + outfile)\n",
        "\n",
        "# limit to first Ndown products sorted by lowest cloud cover and earliest acquisition date\n",
        "products_df_n = products_df_sorted.head(ndown)\n",
        "outfile = 'searchresults4download.csv'\n",
        "products_df_n.to_csv(outfile)\n",
        "print(\"Download list saved: \" + outfile)\n",
        "\n",
        "# get the footprints of the selected scenes\n",
        "s2footprints = products_df_n.footprint\n",
        "outfile = 'searchresultsfootprints.csv'\n",
        "s2footprints.to_csv(outfile, header = False)\n",
        "print(\"Granule footprints saved: \" + outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EEPt8cWTf58",
        "colab_type": "text"
      },
      "source": [
        "Download data. This takes a long time if many images are selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKHKknH8Tf59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################\n",
        "# 2. Download all selected images into a data directory\n",
        "#######################################################################\n",
        "\n",
        "os.chdir(downloaddir) # set working direcory to download directory\n",
        "\n",
        "# download sorted and reduced products in order\n",
        "api.download_all(products_df_n['uuid'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LC6XapHTf6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the footprints of the scenes marked for download together with their metadata in a Geojson file\n",
        "# first, run a new query to get the metadata for the selected scenes\n",
        "products_n = OrderedDict()\n",
        "for uuid in products_df_n['uuid']:\n",
        "    kw = query_kwargs.copy()\n",
        "    kw['uuid'] = uuid\n",
        "    pp = api.query(**kw)\n",
        "    products_n.update(pp)\n",
        "\n",
        "# then, write the footprints and metadata to a geojson file\n",
        "outfile = join(outdir, 'footprints.geojson')\n",
        "with open(outfile, 'w') as f:\n",
        "    json.dump(api.to_geojson(products_n), f)\n",
        "print(\"Granule footprints saved as GeoJson: \" + outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "D_yRA41mTf6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################\n",
        "# 3. Convert all images into reprojected Geotiff files (retain only the 10 m resolution bands and apply cloud mask)\n",
        "#######################################################################\n",
        "    \n",
        "# set working direcory to download directory\n",
        "os.chdir(downloaddir)\n",
        "\n",
        "# get list of all zip files in the data directory\n",
        "allfiles = [f for f in listdir(downloaddir) if isfile(join(downloaddir, f))]\n",
        "\n",
        "# unzip all these files and convert the 10 m bands to Geotiff\n",
        "for x in range(len(allfiles)):\n",
        "\n",
        "    if allfiles[x].split(\".\")[1] == \"zip\":\n",
        "        \n",
        "        print(\"Unzipping file \", x+1, \": \", allfiles[x])\n",
        "        with zipfile.ZipFile(allfiles[x], \"r\") as zipf:\n",
        "            zipf.extractall(downloaddir)\n",
        "        print(\"Removing zip file to save space.\")\n",
        "        try:\n",
        "          shutil.rmtree(allfiles[x])\n",
        "        except:\n",
        "          print(\"File removal error.\")\n",
        "\n",
        "        # in this directory are the Sentinel-2, 10 m resolution band files\n",
        "        # extract *.SAFE subdirectory path, then GRANULE, then L2A_*, then IMG_DATA, then R10m\n",
        "        sceneID = allfiles[x].split(\".\")[0] # the first part of the directory name is the granule ID\n",
        "\n",
        "        # find *.SAFE subdirectory\n",
        "        dirlist = [d for d in listdir(downloaddir) if isdir(join(downloaddir, d))]\n",
        "        for y in range(len(dirlist)):\n",
        "            if dirlist[y].split(\".\")[1] == \"SAFE\":\n",
        "                thisdir = join(downloaddir, dirlist[y])\n",
        "\n",
        "        # find GRANULE subdirectory\n",
        "        thisdir = join(thisdir, \"GRANULE\")\n",
        "\n",
        "        # find L2A_* subdirectory\n",
        "        dirlist = [d for d in listdir(thisdir) if isdir(join(thisdir, d))]\n",
        "        for y in range(len(dirlist)):\n",
        "            if dirlist[y].split(\"_\")[0] == \"L2A\":\n",
        "                thisdir = join(thisdir, dirlist[y])\n",
        "\n",
        "        # find IMG_DATA subdirectory\n",
        "        thisdir = join(thisdir, \"IMG_DATA\")\n",
        "        scldir = copy(thisdir) # also find the path to the 20 m resolution SCL file with cloud mask information\n",
        "\n",
        "        # find R10m subdirectory\n",
        "        thisdir = join(thisdir, \"R10m\")\n",
        "\n",
        "        # find R20m subdirectory for SCL file\n",
        "        scldir = join(scldir, \"R20m\")\n",
        "        print(scldir)\n",
        "        \n",
        "        # call our function to convert the granule into a reprojected Geotiff file\n",
        "        s2tiff(thisdir, scldir, tiffdir, scltiffdir, sceneID, outSpatialRef)\n",
        "        \n",
        "        # apply the cloud masking procedure\n",
        "        imgfile = join(tiffdir, sceneID + '_16Bit_proj.tif')\n",
        "        sclfile = join(scltiffdir, sceneID + '_SCL16Bit_proj.tif')\n",
        "\n",
        "        # define the output file names\n",
        "        outfile = join(outdir, sceneID + '_clmasked.tif') # for the cloud masked image bands\n",
        "        maskfile = join(outdir, sceneID + '_clmask.tif') # for the cloud mask itself\n",
        "        sentinel2_cloud_removal(imgfile, sclfile, outfile, maskfile, kernel_dil=(5, 5), kernel_ero=(15, 15), classes = [0, 1, 2, 3, 6, 8, 9, 10, 11])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "w5RfxqNmTf6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################\n",
        "# 4. Save quicklooks of all image mosaics for each image acquisition date\n",
        "#######################################################################\n",
        "\n",
        "# declare a function to eliminate duplicate entries in a list by converting to a dictionary and back\n",
        "def remove_duplicates(x):\n",
        "    return list(dict.fromkeys(x))\n",
        "\n",
        "# get list of all tiff files in the data directory\n",
        "allfiles = [f for f in listdir(tiffdir) if (isfile(join(tiffdir, f)) and f.endswith('.tif'))]\n",
        "\n",
        "print(\"\\nAll files:\")\n",
        "print(allfiles)\n",
        "\n",
        "# pull out all acquisition dates from the file names\n",
        "# for the Sentinel-2 naming convention, see \n",
        "#     https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/naming-convention\n",
        "acqdates = remove_duplicates([f.split(\"_\")[2].split(\"T\")[0] for f in allfiles])\n",
        "\n",
        "print(\"\\nAll acquisition dates (sorted):\")\n",
        "print([f for f in sorted(acqdates)])\n",
        "\n",
        "# go through all dates\n",
        "for d in acqdates:\n",
        "    \n",
        "    # pull out all file names of the same acquisition date\n",
        "    files2mosaic = [f for f in allfiles if (f.split(\"_\")[2].split(\"T\")[0] == d)]\n",
        "\n",
        "    print(\"\\nImages taken on date \" + d)\n",
        "    print(files2mosaic)\n",
        "    \n",
        "    # define a filename for the output file\n",
        "    mosaicID = \"S2_\" + d\n",
        "\n",
        "    # call our function\n",
        "    quicklooks(tiffdir, files2mosaic, quickdir, mosaicID, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIRsebAMTf6Q",
        "colab_type": "text"
      },
      "source": [
        "Below, we define the main functions, including one to extract zonal statistics from the polygons in the shapefile for all 10 m resolution bands of an image, and another one to loop over the entire time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjIC3JiJTf6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################\n",
        "# 5. Extract polygon statistics as a time series from all data\n",
        "#######################################################################\n",
        "\n",
        "def tif2raster(tiffile):\n",
        "    # read in a geotiff file (unsigned 16-bit integer format) and turn into a Python raster object\n",
        "    img_ds = io.imread(tiffile)\n",
        "    # convert to 16bit numpy array \n",
        "    img = np.array(img_ds, dtype='int16')\n",
        "    return(img)\n",
        "\n",
        "def zonal_stats(feat, input_zone_polygon, input_tiff, verbose = False, graphics = True, bands=[1,2,3]):\n",
        "    '''\n",
        "        extracts statistics from a feature (polygon) of a shapefile from a tiff file with several bands\n",
        "        heavily adapted from:\n",
        "        https://gis.stackexchange.com/questions/208441/zonal-statistics-of-a-polygon-and-assigning-mean-value-to-the-polygon\n",
        "    '''\n",
        "\n",
        "    if verbose:\n",
        "        print(\"--------------------------------------------------------------------\")\n",
        "        print(\"Processing raster file: \" + input_tiff)\n",
        "        \n",
        "    # Open vector data\n",
        "    shp = ogr.Open(input_zone_polygon)\n",
        "    lyr = shp.GetLayer()\n",
        "\n",
        "    # Get raster georeference info\n",
        "    dataset = gdal.Open(input_tiff, gdal.GA_ReadOnly)\n",
        "    if not dataset:\n",
        "        print(\"Error. Tiff file not found: \" + input_tiff)\n",
        "\n",
        "    ncols = dataset.RasterXSize\n",
        "    nrows = dataset.RasterYSize\n",
        "    nbands = dataset.RasterCount\n",
        "    wkt_projection = dataset.GetProjection()\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Raster size: {} x {} x {}\".format(ncols, nrows, nbands))\n",
        "        print(\"Raster projection: {}\".format(wkt_projection))\n",
        "\n",
        "    geotransform = dataset.GetGeoTransform()\n",
        "    if geotransform:\n",
        "        xOrigin = geotransform[0]\n",
        "        yOrigin = geotransform[3]\n",
        "        pixelWidth = geotransform[1]\n",
        "        pixelHeight = geotransform[5]\n",
        "        if verbose:\n",
        "            print(\"Tiff file image origin xmin,yax = ({}, {})\".format(geotransform[0], geotransform[3]))\n",
        "            print(\"Pixel Size x,y = ({}, {})\".format(geotransform[1], geotransform[5]))\n",
        "    else:\n",
        "        print(\"Error. Geotransform not found in tiff file: \" + input_tiff) \n",
        "\n",
        "    # Reproject vector geometry to same projection as raster\n",
        "    sourceSR = lyr.GetSpatialRef()\n",
        "    targetSR = osr.SpatialReference()\n",
        "    targetSR.ImportFromWkt(wkt_projection)\n",
        "    coordTrans = osr.CoordinateTransformation(sourceSR,targetSR)\n",
        "    feat = lyr.GetNextFeature()\n",
        "    geom = feat.GetGeometryRef()\n",
        "    geom.Transform(coordTrans)\n",
        "\n",
        "    # Get extent of the feature for which to extract the raster statistics\n",
        "    geom = feat.GetGeometryRef()\n",
        "    if (geom.GetGeometryName() == 'MULTIPOLYGON'):\n",
        "        count = 0\n",
        "        pointsX = []; pointsY = []\n",
        "        for polygon in geom:\n",
        "            geomInner = geom.GetGeometryRef(count)\n",
        "            ring = geomInner.GetGeometryRef(0)\n",
        "            numpoints = ring.GetPointCount()\n",
        "            for p in range(numpoints):\n",
        "                    lon, lat, z = ring.GetPoint(p)\n",
        "                    pointsX.append(lon)\n",
        "                    pointsY.append(lat)\n",
        "            count += 1\n",
        "    elif (geom.GetGeometryName() == 'POLYGON'):\n",
        "        ring = geom.GetGeometryRef(0)\n",
        "        numpoints = ring.GetPointCount()\n",
        "        pointsX = []; pointsY = []\n",
        "        for p in range(numpoints):\n",
        "                lon, lat, z = ring.GetPoint(p)\n",
        "                pointsX.append(lon)\n",
        "                pointsY.append(lat)\n",
        "\n",
        "    else:\n",
        "        sys.exit(\"ERROR: Geometry needs to be either Polygon or Multipolygon\")\n",
        "\n",
        "    # extent of this feature in map coordinates\n",
        "    xmin = min(pointsX)\n",
        "    xmax = max(pointsX)\n",
        "    ymin = min(pointsY)\n",
        "    ymax = max(pointsY)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nFeature extent in map coordinates: xmin,xmax, ymin, ymax\", xmin, xmax, ymin, ymax)\n",
        "       \n",
        "    # raster layer extent in map coordinates\n",
        "    xmin1 = xOrigin\n",
        "    xmax1 = xOrigin + ncols * pixelWidth\n",
        "    ymin1 = yOrigin + nrows * pixelHeight\n",
        "    ymax1 = yOrigin\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nGeotiff raster extent in map coordinates: xmin1, xmax1, ymin1, ymax1\")\n",
        "        print(xmin1,xmax1,ymin1,ymax1)\n",
        "    \n",
        "    # extent of the intersect between the featurs / polygon / zone and the raster image in map coordinates\n",
        "    xmin2 = max(xmin,xmin1)\n",
        "    xmax2 = min(xmax,xmax1)\n",
        "    ymin2 = max(ymin,ymin1)\n",
        "    ymax2 = min(ymax,ymax1)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nIntersect extent in map coordinates: xmin2, xmax2, ymin2, ymax2\")\n",
        "        print(xmin2,xmax2,ymin2,ymax2)\n",
        "    \n",
        "    # clip the Geotiff raster layer to the extent of the intersect with the feature\n",
        "    xoff = int((xmin2 - xOrigin)/pixelWidth)\n",
        "    yoff = int((yOrigin - ymax2)/pixelWidth)\n",
        "    xcount = int((xmax2 - xmin2)/pixelWidth)\n",
        "    ycount = int((ymax2 - ymin2)/pixelWidth)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nclipped raster layer extent in pixels: xoff, yoff, xcount, ycount\")\n",
        "        print(\"xoff = \", xoff)\n",
        "        print(\"yoff = \", yoff)\n",
        "        print(\"xcount = \", xcount)\n",
        "        print(\"ycount = \", ycount)\n",
        "\n",
        "    # clip the rasterised feature layer to the extent of the intersect with the feature\n",
        "    xoff1 = int((xmin2 - xmin)/pixelWidth)\n",
        "    yoff1 = int((ymax - ymax2)/pixelWidth)\n",
        "    xcount1 = int((xmax2 - xmin2)/pixelWidth)\n",
        "    ycount1 = int((ymax2 - ymin2)/pixelWidth)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nclipped feature layer extent in pixels: xoff1,yoff1,xcount1,ycount1\")\n",
        "        print(\"xoff1 = \", xoff1)\n",
        "        print(\"yoff1 = \", yoff1)\n",
        "        print(\"xcount1 = \", xcount1)\n",
        "        print(\"ycount1 = \", ycount1)\n",
        "\n",
        "    # Create memory target raster for the mask with all pixels within the feature (zone)\n",
        "    target_ds = gdal.GetDriverByName('MEM').Create('', xcount, ycount, 1, gdal.GDT_Byte)\n",
        "    target_ds.SetGeoTransform((xmin2, pixelWidth, 0, ymax2, 0, pixelHeight,))\n",
        "\n",
        "    # make projection the same as for the Geotiff Sentinel-2 raster file\n",
        "    target_ds.SetProjection(wkt_projection)\n",
        "\n",
        "    # create an empty array to hold the statistics results for all bands\n",
        "    stats = np.zeros((nbands, 7))\n",
        "\n",
        "    # plot a colour composite of three bands\n",
        "    if graphics:\n",
        "        # create an empty array to hold the 3 stretched colour bands\n",
        "        img = np.zeros((ycount, xcount, 3), dtype = np.int16)\n",
        "        for band in range(0, 3):\n",
        "            # Read intersecting Geotiff raster layer values as an array\n",
        "            \n",
        "            ################## TODO ######################\n",
        "            # memory error here if image is very large\n",
        "            # downsample the image if too big\n",
        "            \n",
        "            thisband = np.array(dataset.GetRasterBand(bands[band]).ReadAsArray(xoff, yoff, xcount, ycount)).astype(np.int16)\n",
        "            print(\"min, max of band \", np.min(thisband), np.max(thisband))\n",
        "            # Histogram equalisation stretching\n",
        "            img[:, :, band] = np.int16(256 * exposure.equalize_hist(thisband, nbins=256, mask=None))\n",
        "            print(\"min, max of stretched band \", np.min(img[:, :, band]), np.max(img[:, :, band]))\n",
        "            # Contrast stretching and byte scaling\n",
        "            # p2, p98 = np.percentile(thisband, (2, 98))\n",
        "            # img[:, :, band] = np.int16(exposure.rescale_intensity(thisband, in_range=(p2, p98)) * 255)\n",
        "        plt.figure().suptitle(\"Colour composite\")\n",
        "        io.imshow(img)\n",
        "        io.show()\n",
        "        img = None # free up memory\n",
        "        thisband = None\n",
        "\n",
        "    # Rasterize the feature (zone polygon) to the target raster layer\n",
        "    #   burn_value of 1 indicates that a pixels is inside the feature (zone)\n",
        "    gdal.RasterizeLayer(target_ds, [1], lyr, burn_values=[1]) #, options = ['ALL_TOUCHED=True'])\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Zone mask raster size: \", target_ds.GetRasterBand(1).ReadAsArray().shape)\n",
        "        print(\"Zone mask raster min,max: \", np.nanmin(target_ds.GetRasterBand(1).ReadAsArray()), np.nanmax(target_ds.GetRasterBand(1).ReadAsArray()))\n",
        "\n",
        "    if graphics:\n",
        "        # Display the whole rasterised zone mask\n",
        "        plt.figure().suptitle(\"Zone mask\")\n",
        "        io.imshow(np.array(target_ds.GetRasterBand(1).ReadAsArray()), cmap='gray_r')\n",
        "\n",
        "    # loop over all bands\n",
        "    for band in range(1, nbands + 1):\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"\\nBand \", band)\n",
        "\n",
        "        # Read intersecting Geotiff raster layer values as an array\n",
        "        dataraster = np.array(dataset.GetRasterBand(band).ReadAsArray(xoff, yoff, xcount, ycount)).astype(np.float)\n",
        "\n",
        "        # clip the rasterised feature layer to the extent of the intersect with the geotiff raster layer\n",
        "        datamask = target_ds.GetRasterBand(1).ReadAsArray().astype(np.byte)\n",
        "\n",
        "        # do something here to catch situations where dataraster is all NaNs\n",
        "        #    replace 1 in datamask with 0 where NaNs are found in dataraster\n",
        "        datamask[np.isnan(dataraster)] = 0\n",
        "        #    replace 1 in datamask with 0 where 0 values are found in dataraster (i.e. cloud masked)\n",
        "        datamask = np.where(dataraster == 0, 0, datamask)\n",
        "\n",
        "        warnme = 0 # no warnings so far\n",
        "\n",
        "        if (np.nanmin(dataraster) == np.nan and np.nanmax(dataraster) == np.nan):\n",
        "            print(\"WARNING: All dataraster values are NaN.\")\n",
        "            warnme = warnme + 1\n",
        "        else:\n",
        "            if (dataraster.shape == ()):\n",
        "                print(\"WARNING: Failed to create dataraster.\")\n",
        "                warnme = warnme + 1\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Data raster min, max: \", np.nanmin(dataraster), np.nanmax(dataraster))\n",
        "                            \n",
        "                if (np.min(datamask) == 0 and np.max(datamask) == 0):\n",
        "                    print(\"WARNING: All datamask values are zero.\")\n",
        "                    warnme = warnme + 1\n",
        "                else:\n",
        "                    if verbose:\n",
        "                        print(\"Data mask min, max: \", np.min(datamask), np.max(datamask))\n",
        "                    \n",
        "                    # Apply the mask from the rasterised feature layer (zone polygon) to the Geotiff raster\n",
        "                    # at this point, we need to invert the mask (swap 0 and 1) because masked arrays treat values of 1 as being masked out\n",
        "                    # the stats are calculated under the mask where it is 0. Hence the logical_not below.\n",
        "                    zoneraster = np.ma.MaskedArray(dataraster, np.logical_not(datamask))\n",
        "                    dataraster = None # free up memory\n",
        "                    datamask = None\n",
        "                    \n",
        "                    if (np.min(zoneraster.data) == 1 and np.max(zoneraster.data) == 1):\n",
        "                        print(\"WARNING: All zoneraster values are masked out.\")\n",
        "                        warnme = warnme + 1\n",
        "                    else:\n",
        "                        if verbose:\n",
        "                            print(\"Zone raster min, max: \", np.ma.min(zoneraster), np.ma.max(zoneraster))\n",
        "                \n",
        "                        if graphics:\n",
        "                            # histogram equalisation of the masked raster array\n",
        "                            img = np.int16(256 * exposure.equalize_hist(zoneraster.filled(0), nbins=256, mask = np.logical_not(zoneraster.mask)))\n",
        "                            # Contrast stretching\n",
        "                            #p2, p98 = np.percentile(zoneraster, (2, 98))\n",
        "                            #img = exposure.rescale_intensity(zoneraster, in_range=(p2, p98))\n",
        "                            plt.figure().suptitle(\"Zone raster\")\n",
        "                            io.imshow(img, cmap='gray_r')\n",
        "\n",
        "        if graphics:\n",
        "            io.show()\n",
        "\n",
        "        # calculate band statistics under the zone mask\n",
        "        # N.B. The -1 is there to get Python indices from 0...3 from the band variable 1...4 (see for loop)\n",
        "        # Exclude zero values from the stats, as these are missing values or cloud masked pixels\n",
        "        stats[band - 1, 0] = band\n",
        "        stats[band - 1, 1] = np.ma.count(zoneraster)\n",
        "        stats[band - 1, 2] = np.ma.mean(zoneraster)\n",
        "        stats[band - 1, 3] = np.ma.median(zoneraster)\n",
        "        stats[band - 1, 4] = np.ma.std(zoneraster)\n",
        "        stats[band - 1, 5] = np.ma.var(zoneraster)\n",
        "        stats[band - 1, 6] = np.ma.sum(zoneraster)       \n",
        "        \n",
        "    # close files\n",
        "    dataset = None\n",
        "    shp = None\n",
        "    \n",
        "    # free up memory\n",
        "    target_ds = None\n",
        "\n",
        "    if (warnme > 0):\n",
        "        print(warnme, \" WARNINGS issued. Returning zero values from the zone statistics function.\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def loop_zonal_stats(input_zone_polygon, input_tiff, verbose = False, graphics = False, bands = [1,2,3]):\n",
        "\n",
        "    shp = ogr.Open(input_zone_polygon)\n",
        "    lyr = shp.GetLayer()\n",
        "    featList = range(lyr.GetFeatureCount())\n",
        "    statDict = {}\n",
        "\n",
        "    for FID in featList:\n",
        "        if verbose:\n",
        "            print(\"Feature ID: \", FID)\n",
        "        feat = lyr.GetFeature(FID)\n",
        "        meanValue = zonal_stats(feat, input_zone_polygon, input_tiff, verbose, graphics)\n",
        "        statDict[FID] = meanValue\n",
        "    return statDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jqW1Ft46Tf6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pull out all tile IDs from the file names\n",
        "# the tiling grid can be downloaded from here as a .kml file:\n",
        "#     https://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products\n",
        "tiles = sorted(remove_duplicates([f.split(\"_\")[5] for f in allfiles]))\n",
        "\n",
        "print(\"\\nAll tile IDs (sorted):\")\n",
        "for t in tiles:\n",
        "    print(t)\n",
        "print(\"\\n\") # new line\n",
        "\n",
        "# check whether shapefile exists\n",
        "if not os.path.exists(shapefile):\n",
        "    print('Shapefile not found: ' + shapefile)\n",
        "else:\n",
        "    # go through all tiles and extract statistics for all available acquisition dates for that tile\n",
        "    \n",
        "    for t in tiles:\n",
        "\n",
        "        # get all file names with the same tile ID from the list of all tiff files\n",
        "        filestack = [f for f in allfiles if (f.split(\"_\")[5] == t)]\n",
        "        print(\"\\nImage files acquired for tile \" + t)\n",
        "        for xfile in filestack:\n",
        "            print(xfile)\n",
        "\n",
        "        # pull out all acquisition dates from the file names\n",
        "        acqdates = remove_duplicates([f.split(\"_\")[2].split(\"T\")[0] for f in filestack])\n",
        "        print(\"\\nAcquisition dates for this tile (sorted):\")\n",
        "        print([f for f in sorted(acqdates)])\n",
        "        \n",
        "        # define a filename for the output text file with the statistics\n",
        "        timeseries = join(outdir, \"S2_\" + t + \".txt\")\n",
        "        print(\"\\nSaving tile statistics to file: \", timeseries)\n",
        "        outfile = open(timeseries, \"w\")\n",
        "\n",
        "        # write header line\n",
        "        outfile.write(\"AcqDate, Zone, Band, Count, Mean, Median, Std_Deviation, Variance, Sum\\n\")\n",
        "\n",
        "        # for each acquisition date, get the polygon statistics of that tile\n",
        "        for xfile in filestack:\n",
        "\n",
        "            # get acquisition date for that file\n",
        "            acqdate = xfile.split(\"_\")[2].split(\"T\")[0]\n",
        "\n",
        "            # run the loop over all polygons (zones) and all image files\n",
        "            # the bands here will be shown as RGB if graphics is True\n",
        "            result_dict = loop_zonal_stats(shapefile, join(tiffdir, xfile), verbose = True, graphics = True, bands=[1,2,3])\n",
        "\n",
        "            # save acquisition dates and statistics of that tile to the statistics file\n",
        "            # for each zone (polygon ID) in the results dictionary, there is an array with the statistics for each band\n",
        "            for zone, stat in result_dict.items():\n",
        "                for band in range(stat.shape[0]):\n",
        "                    outfile.write(acqdate + ', ' + str(zone) + ', ')\n",
        "                    this_line = list(stat[band, ]) # line of statistics for output file\n",
        "                    for item in this_line:\n",
        "                        if item == this_line[0]: # first item in the line\n",
        "                            outfile.write(\"%s, \" % np.int32(item))\n",
        "                        else:\n",
        "                            if item == this_line[len(this_line) - 1]:  # last item in the line\n",
        "                                outfile.write(\"%s\\n\" % item)\n",
        "                            else:\n",
        "                                outfile.write(\"%s, \" % item) # all other items in the line\n",
        "\n",
        "        outfile.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mfVOeVeTf6Z",
        "colab_type": "text"
      },
      "source": [
        "# Your portfolio assignment\n",
        "1. Choose a different shapefile of your choice for a study area of interest.\n",
        "2. Define a date range for your analysis - bear in mind computing power.\n",
        "3. Run the analysis on your data.\n",
        "4. Present the results in your practical portfolio and discuss what they show.\n",
        "\n",
        "It is recommended to choose a study area with low cloud cover.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1KvssENTf6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls -l \"/content/drive/My Drive/practicals/p08/outputs\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFYv0ZikTf6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat \"/content/drive/My Drive/practicals/p08/outputs/S2_T30UXD.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRccROkeTf6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}