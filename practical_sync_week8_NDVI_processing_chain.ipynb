{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "practical_sync_week8_NDVI_processing_chain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcz9pnNKFXRA"
      },
      "source": [
        "# Week 8: Creating an automated Sentinel-2 processing chain\n",
        "\n",
        "Individual learning outcomes: At the end of this week, all students should be able to calculate the Normalized Difference Vegetation Index (NDVI) from the Sentinel-2 spectral bands and extract statistics of pixel values within polygons of a shapefile from within Python.\n",
        "\n",
        "We will use the images we downloaded with the processing chain we built in last week's practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___N_2vyp_f2"
      },
      "source": [
        "Connect to our Google Drive from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RvpK17wXp_f7"
      },
      "source": [
        "# Load the Drive helper and mount your Google Drive as a drive in the virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fajjFxKt5nwT"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnzg_pcTSEP"
      },
      "source": [
        "#import required libraries, including the sentinelsat library this time\n",
        "!pip install rasterio\n",
        "!pip install sentinelsat\n",
        "!pip install geopandas\n",
        "# install the function to get zonal statistics from the rasterstats library\n",
        "!pip install rasterstats\n",
        "from rasterstats import zonal_stats\n",
        "import csv \n",
        "import pickle\n",
        "\n",
        "\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio import plot\n",
        "from rasterio.plot import show_hist\n",
        "from rasterio.windows import Window\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt, geojson\n",
        "from collections import OrderedDict\n",
        "from osgeo import gdal, ogr\n",
        "import json\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "import math\n",
        "from math import floor, ceil\n",
        "from pyproj import Proj\n",
        "from pprint import pprint\n",
        "import shutil\n",
        "import sys\n",
        "import zipfile\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37tiOSa7wq8"
      },
      "source": [
        "We need a help function at a later stage, so let's define that now. It converts between lat/lon coordinates and pixel locations in a raster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gceBeSJI7pKF"
      },
      "source": [
        "# define a helper function that converts latitude, longitude coordinates into pixel locations\n",
        "def longlat2window(lon, lat, dataset):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        lon (tuple): Tuple of min and max lon\n",
        "        lat (tuple): Tuple of min and max lat\n",
        "        dataset: Rasterio dataset\n",
        "\n",
        "    Returns:\n",
        "        rasterio.windows.Window\n",
        "    \"\"\"\n",
        "    p = Proj(dataset.crs)\n",
        "    t = dataset.transform\n",
        "    xmin, ymin = p(lon[0], lat[0])\n",
        "    xmax, ymax = p(lon[1], lat[1])\n",
        "    col_min, row_min = ~t * (xmin, ymin)\n",
        "    col_max, row_max = ~t * (xmax, ymax)\n",
        "    return Window.from_slices(rows=(floor(row_max), ceil(row_min)),\n",
        "                              cols=(floor(col_min), ceil(col_max)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcro81WxTf4_"
      },
      "source": [
        "# Processing Sentinel-2 images\n",
        "\n",
        "The workflow for this practical is:\n",
        "* Calculate the Normalised Difference Vegetation Index from the 10 m bands of the images we downloaded last time\n",
        "* Save the NDVI files as Geotiff format\n",
        "* Visualise the NDVI images\n",
        "* Extract zonal statistics of NDVI for polygons in our shapefile\n",
        "* Save the statistics as a csv file for use in Excel\n",
        "* Plot the statistics to explore them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzmlF4GQD1ji"
      },
      "source": [
        "# set up your directories for the satellite data\n",
        "# Note that we do all the downloading and data analysis on the temporary drive\n",
        "#    on Colab. We will copy the output directory to our Google Drive at the end.\n",
        "#    Colab has more disk space (about 40 GB free space) than Google Drive (15 GB).\n",
        "#    However, the data on the Colab disk space are NOT kept when you log out.\n",
        "\n",
        "# path to your Google Drive\n",
        "wd = \"/content/drive/MyDrive/practicals20-21\" \n",
        "print(\"Connected to data directory: \" + wd)\n",
        "\n",
        "# path to the directory where we saved the downloaded Sentinel-2 images last time\n",
        "datadir = \"/content/drive/MyDrive/practicals20-21/download\"\n",
        "print(\"Looking for image data in: \" + datadir)\n",
        "\n",
        "# path to your temporary drive on the Colab Virtual Machine\n",
        "cd = \"/content/work\"\n",
        "\n",
        "# Name of the shape file\n",
        "shapefile = join(wd, 'oakham', 'Polygons_small.shp') # ESRI Shapefile of the study area\n",
        "\n",
        "# directory for downloading the Sentinel-2 granules\n",
        "# Note that we are using the 'join' function imported from the os library here for the first time\n",
        "# It is an easy way of merging strings into a directory structure.\n",
        "# It is clever and chooses the / or \\ depending on whether you are on Windows or Linux.\n",
        "downloaddir = join(cd, 'download') # where we save the downloaded images\n",
        "quickdir = join(cd, 'quicklooks')  # where we save the quicklooks\n",
        "outdir = join(cd, 'out')           # where we save any other outputs\n",
        "\n",
        "# CAREFUL: This code removes the named directories and everything inside them to free up space\n",
        "# Note: shutil provides a lot of useful functions for file and directory management\n",
        "try:\n",
        "  shutil.rmtree(downloaddir)\n",
        "except:\n",
        "  print(downloaddir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(quickdir)\n",
        "except:\n",
        "  print(quickdir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(outdir)\n",
        "except:\n",
        "  print(outdir + \" not found.\")\n",
        "\n",
        "# create the new directories, unless they already exist\n",
        "os.makedirs(cd, exist_ok=True)\n",
        "os.makedirs(downloaddir, exist_ok=True)\n",
        "os.makedirs(quickdir, exist_ok=True)\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "print(\"Connected to Colab temporary data directory: \" + cd)\n",
        "\n",
        "print(\"\\nList of contents of \" + wd)\n",
        "for f in sorted(os.listdir(wd)):\n",
        "  print(f)\n",
        "\n",
        "# check whether the file with the login details exists\n",
        "if \"sencredentials.txt\" not in os.listdir(wd):\n",
        "  print(\"\\nERROR: File sencredentials.txt not found. Cannot log in to Data Hub.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl4uKnceXPEV"
      },
      "source": [
        "Get the extent of the shapefile to define our search area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDXQnbChE0b3"
      },
      "source": [
        "# Get the shapefile layer's extent\n",
        "driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
        "ds = driver.Open(shapefile, 0)\n",
        "lyr = ds.GetLayer()\n",
        "extent = lyr.GetExtent()\n",
        "print(\"Extent of the area of interest (shapefile):\\n\", extent)\n",
        "\n",
        "# get projection information from the shapefile to reproject the images to\n",
        "outSpatialRef = lyr.GetSpatialRef()\n",
        "print(\"\\nSpatial referencing information of the shapefile:\\n\", outSpatialRef)\n",
        "\n",
        "# close file\n",
        "ds = None \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvsufePTX1RU"
      },
      "source": [
        "# Explore the data directory structure of our downloaded files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ0b3ZURLOdm"
      },
      "source": [
        "# where we stored the downloaded files\n",
        "os.chdir(datadir)\n",
        "print(\"contents of \", datadir, \":\")\n",
        "!ls -l\n",
        "\n",
        "# copy the data from Google Drive (permanent storage) to Colab (temporary storage but more disk space)\n",
        "# we write a function to delete the destination directory if it exists to avoid errors\n",
        "def copytree_overwrite(from_path, to_path):\n",
        "    if os.path.exists(to_path):\n",
        "        shutil.rmtree(to_path)\n",
        "    shutil.copytree(from_path, to_path)\n",
        "\n",
        "# call our function\n",
        "copytree_overwrite(datadir, downloaddir)\n",
        "\n",
        "# look at copied files on the Colab drive\n",
        "os.chdir(downloaddir)\n",
        "print(\"contents of \", downloaddir, \":\")\n",
        "!ls -l\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj_1f0euHbVx"
      },
      "source": [
        "# Calculate NDVI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuDbMvzGUR74"
      },
      "source": [
        "Before we calculate the Normalised Difference Vegetation Index from our images, we need to find the directories that contain the band files. They are located in the 10m subdirectories of our downloaded Sentinel-2 directories for each image. We modify the code from last week as follows.\n",
        "\n",
        "We will mark up any changes as follows:\n",
        "\n",
        "```\n",
        "'''\n",
        "Changed from last week\n",
        "'''\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fm_Av5gPOCh"
      },
      "source": [
        "'''\n",
        "Changed from last week:\n",
        "* Modified variable names\n",
        "* Changed from 20 m to 10 m resolution\n",
        "* Changed from TCI file to RED and NIR files\n",
        "'''\n",
        "\n",
        "# make a list of all RED and NIR image band files across all downloaded image directories\n",
        "s2IDs = [] # empty list of all Sentinel-2 granule IDs we have downloaded\n",
        "s2dirs = [] # empty list of all directory paths pointing to the 10 m resolution band files\n",
        "files_red = [] # empty list of all Red band file names\n",
        "files_nir = [] # empty list of all NIR band file names\n",
        "\n",
        "# get the list of all directories in the download directory\n",
        "# there is one directory for each Sentinel-2 image (granule)\n",
        "dirlist = [d for d in listdir(downloaddir) if isdir(join(downloaddir, d))]\n",
        "\n",
        "# iterate over all Sentinel-2 image directories\n",
        "for d in range(len(dirlist)):\n",
        "  # the directory names have the following structure, for example:\n",
        "  # S2A_MSIL2A_20190919T110721_N0213_R137_T30UXD_20190919T140654.SAFE\n",
        "  # the first part of the directory name is the granule ID\n",
        "  # so we split off the \".SAFE\" as follows:\n",
        "  sceneID = dirlist[d].split(\".\")[0] \n",
        "  s2IDs.append(sceneID) #append the unique identifier to the list\n",
        "\n",
        "  # find the GRANULE, then L2A_*, then IMG_DATA, then R10m directory\n",
        "  thisdir = join(downloaddir, dirlist[d], \"GRANULE\")\n",
        "\n",
        "  # find the full name of the L2A_* subdirectory (contains the scene ID)\n",
        "  subdirlist = [s for s in listdir(thisdir) if isdir(join(thisdir, s))]\n",
        "  for y in range(len(subdirlist)):\n",
        "    if subdirlist[y].split(\"_\")[0] == \"L2A\":\n",
        "      thisdir = join(thisdir, subdirlist[y])\n",
        "\n",
        "  # add IMG_DATA/R10m to subdirectory, this is where the TCI image is found\n",
        "  s2dir = join(thisdir, \"IMG_DATA\", \"R10m\")\n",
        "  s2dirs.append(s2dir) # add it to our list\n",
        "\n",
        "  # find the TCI image file name\n",
        "  files_10m = [f for f in listdir(s2dir) if isfile(join(s2dir, f))]\n",
        "\n",
        "  '''\n",
        "  Changed from last week:\n",
        "  * Changed from TCI file to RED and NIR files\n",
        "\n",
        "  Be aware of the indentation. It determines that both IF statements are within the same FOR loop.\n",
        "  '''\n",
        "\n",
        "  # We split the filename into components based on the underscore _\n",
        "  # e.g. \"T30UXD_20190919T110721_B04_10m.jp2\"\n",
        "  # becomes [\"T30UXD\", \"20190919T110721\", \"B04\", \"10m.jp2\"]\n",
        "  # so the component indexed 2 contains the band number\n",
        "  for y in range(len(files_10m)):\n",
        "    if files_10m[y].split(\"_\")[2] == \"B04\": # Band 4 is the Red band\n",
        "      files_red.append(files_10m[y]) # remember the path to the Red band file\n",
        "    if files_10m[y].split(\"_\")[2] == \"B08\": # Band 8 is the NIR band\n",
        "      files_nir.append(files_10m[y]) # remember the path to the NIR band file\n",
        "\n",
        "# the output looks neater if we print each element of the list of strings in a new line\n",
        "print(\"List of all Granule IDs:\")\n",
        "for i in s2IDs:\n",
        "  print(i)\n",
        "print(\"List of all Sentinel-2 directories:\")\n",
        "for i in s2dirs:\n",
        "  print(i)\n",
        "print(\"List of all Red band image files:\")\n",
        "for i in files_red:\n",
        "  print(i)\n",
        "print(\"List of all NIR band image files:\")\n",
        "for i in files_nir:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxhvZZe2UmCu"
      },
      "source": [
        "\n",
        "```\n",
        "'''\n",
        "Changed from last week\n",
        "'''\n",
        "```\n",
        "\n",
        "\n",
        "# Warp all our images to the same projection as our shapefile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIpqcW9a8Fnu"
      },
      "source": [
        "Remember we did this last week with GDAL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SziStT4I8L0p"
      },
      "source": [
        "'''\n",
        "Changed from last week:\n",
        "* Includes getting information on the number of files to process\n",
        "* Processes both the Red and the NIR image files instead of just the TCI files\n",
        "'''\n",
        "\n",
        "# how many files are in the file list?\n",
        "nfiles = len(files_red)\n",
        "\n",
        "# get the spatial referencing system of our shapefile into which we want to reproject the TCI images\n",
        "# remember, we did this when we opened the shapefile earlier and saved it in outSpatialRef\n",
        "print(\"Reprojecting all band images to the following projection:\")\n",
        "print(outSpatialRef)\n",
        "\n",
        "warpfiles_red = [] # make an empty list where we can remember all the warped Red file names\n",
        "warpfiles_nir = [] # same for NIR files\n",
        "\n",
        "# iterate over all Sentinel-2 image directories and warp the image\n",
        "for x in range(nfiles):\n",
        "  # join the directory path with the Red file name\n",
        "  file_red = join(s2dirs[x], files_red[x])\n",
        "  # do the same for NIR\n",
        "  file_nir = join(s2dirs[x], files_nir[x])\n",
        "\n",
        "  # make a directory path and file name for the warped output file\n",
        "  warpfile_red = join(quickdir, s2IDs[x] + \"_Red_warped.jp2\")\n",
        "  warpfiles_red.append(warpfile_red) # add it to our list\n",
        "  # same for NIR\n",
        "  warpfile_nir = join(quickdir, s2IDs[x] + \"_NIR_warped.jp2\")\n",
        "  warpfiles_nir.append(warpfile_nir)\n",
        "\n",
        "  # call the GDAL Warp command for the Red and NIR bands\n",
        "  ds = gdal.Warp(warpfile_red, file_red, dstSRS=outSpatialRef)\n",
        "  ds = None #remember to close and save the output file\n",
        "  ds = gdal.Warp(warpfile_nir, file_nir, dstSRS=outSpatialRef)\n",
        "  ds = None\n",
        "\n",
        "pprint(warpfiles_red)\n",
        "pprint(warpfiles_nir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvFhStJFUCpS"
      },
      "source": [
        "# Now calculate the NDVI from the warped Red and NIR bands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttKA37WAT5G_"
      },
      "source": [
        "The Normalized Difference Vegetation Index (NDVI) is an indicator of the proportion and condition of green vegetation. Generally for surfaces with some vegetation the value of NDVI is positive, for surfaces without vegetation the value is near zero, while for water and clouds the value is usually negative. The closer to the positive end, the higher the density of the vegetation cover, that is, it is consistent with its dense and developed stage. This value gradually decreases for less dense vegetation cover, which has positive but not very high values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2j2an_WPkJ"
      },
      "source": [
        "'''\n",
        "Changed from last week:\n",
        "* This cell is new and calculates NDVI.\n",
        "'''\n",
        "\n",
        "# make an empty list to store the file names of our new NDVI files\n",
        "ndvifiles = []\n",
        "\n",
        "# iterate over all warped Sentinel-2 images and calculate NDVI\n",
        "for x in range(nfiles):\n",
        "  # to get the input bands, join the directory path with the Red file name\n",
        "  file_red = warpfiles_red[x]\n",
        "  # do the same for NIR\n",
        "  file_nir = warpfiles_nir[x]\n",
        "\n",
        "  # open the red band file\n",
        "  redfile = rasterio.open(file_red, 'r') \n",
        "  # load the data from the red band file\n",
        "  band_red = redfile.read(1)\n",
        "\n",
        "  # open the NIR band file\n",
        "  nirfile = rasterio.open(file_nir, 'r') \n",
        "  # load the data from the NIR band file\n",
        "  band_nir = nirfile.read(1)\n",
        "\n",
        "  # The Sentinel-2 bands are delivered as uint16 data type (unsigned integer 16 bits per pixel).\n",
        "  # This means that we cannot do floating point calculations on them without first converting them to float.\n",
        "  # When I first tried this practical, I got strange NDVI images without this conversion!\n",
        "  # Convert the band arrays to float:\n",
        "  print(\"Data type when band is read from file: \", band_red.dtype)\n",
        "  band_red = np.float32(band_red)\n",
        "  print(\"Data type after we have converted the band: \", band_red.dtype)\n",
        "  band_nir = np.float32(band_nir)\n",
        "\n",
        "  '''\n",
        "  Calculate the vegetation index. This is done pixel by pixel using the NumPy masked array arithmetic.\n",
        "  '''\n",
        "\n",
        "  # We need to handle exceptions to the calculation. Where the sum of the two bands\n",
        "  # in the denominator is zero (NIR+Red), the NDVI formula would give an error otherwise.\n",
        "  # We do this by setting the NumPy error state to 'ignore' for this calculation only:\n",
        "  # https://numpy.org/doc/stable/reference/generated/numpy.errstate.html\n",
        "\n",
        "  with np.errstate(divide='ignore'): # this only applies to the following indented lines of code\n",
        "    # NDVI formula:\n",
        "    ndvi = np.divide((band_nir - band_red), (band_nir + band_red)) # ignore division by zero errors here\n",
        "    ndvi[(band_nir + band_red) == 0] = 0 # where NIR + Red is zero, set the NDVI to zero\n",
        "\n",
        "  # make a directory path and file name for the NDVI output file in Geotiff format\n",
        "  ndvifile = join(quickdir, s2IDs[x] + \"_NDVI_warped.tif\")\n",
        "  ndvifiles.append(ndvifile)\n",
        "  print(\"\\nCreating NVDI file with CRS and Transform:\")\n",
        "  print(ndvifile)\n",
        "  print(redfile.crs)\n",
        "  print(redfile.transform)\n",
        "  \n",
        "  # save the NDVI image band as a Geotiff file\n",
        "  outfile = rasterio.open(ndvifile, 'w', driver='Gtiff', width=redfile.width, \n",
        "                          height=redfile.height, count=1, crs=redfile.crs, \n",
        "                          transform=redfile.transform, dtype=np.float32)\n",
        "  outfile.write(ndvi, 1)\n",
        "  outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW_KYo7ZbE2f"
      },
      "source": [
        "# Clip the NDVI files to the shapefile extent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU7a0a_ONUJQ"
      },
      "source": [
        "'''\n",
        "Changed from last week:\n",
        "* This cell is modified from something we did before.\n",
        "* It has been changed to work on a single data band in the image file.\n",
        "* The colour map is set to 'Greens'.\n",
        "* It clips the NDVI raster files in the list 'ndvifiles' to the shapefile extent.\n",
        "'''\n",
        "\n",
        "# We need our old helper function to convert an image to uint8 data type for plotting\n",
        "def show_ndvi(afile, ax=None, xlim=None, ylim=None): \n",
        "  '''\n",
        "  Changed from last week:\n",
        "  * This is an adaptation of our old function 'tci'\n",
        "  * It rescales the image data, changes it to uint8 data type and plots it\n",
        "  * amin is now set to zero, so we ignore all negative NDVI values in the plotting\n",
        "  '''\n",
        "  # afile is a handle to an image file opened with RasterIO.Open()\n",
        "  # ax is the axes handle to plot the map on\n",
        "  # xlim =[xmin, xmax] is the map extent to be shown in x direction\n",
        "  # ylim =[ymin, ymax] is the map extent to be shown in y direction\n",
        "  # bands is the order of image bands in the source file to become RGB channels\n",
        "\n",
        "  # read band data\n",
        "  a = afile.read(1)\n",
        "\n",
        "  # exclude negative values\n",
        "  a[a<0] = 0\n",
        "\n",
        "  amin = a.min()\n",
        "  amax = a.max()\n",
        "  \n",
        "  # catch errors if all values are the same\n",
        "  if (amax-amin) == 0:\n",
        "    print(\"WARNING: max and min are the same values\")\n",
        "\n",
        "  anewmin = 0.0\n",
        "  anewmax = 255.0\n",
        "  ascaled = (a - amin) * ((anewmax - anewmin) / (amax - amin)) + anewmin\n",
        "  a_uint8 = ascaled.astype(np.uint8)\n",
        "\n",
        "  # save the uint8 image as a temporary Geotiff file\n",
        "  tmpfile = rasterio.open('tmp_ndvi_imagefile_ cjdlsbYFEOGFHEWBVUW.tiff',\n",
        "                            'w',driver='Gtiff', width=afile.width, height=afile.height,\n",
        "                            count=1, crs=afile.crs, transform=afile.transform, \n",
        "                            dtype=np.uint8)\n",
        "  tmpfile.write(a_uint8, 1)\n",
        "  tmpfile.close()\n",
        "\n",
        "  # try plotting the image again\n",
        "  imgfile = rasterio.open(r'tmp_ndvi_imagefile_ cjdlsbYFEOGFHEWBVUW.tiff', count=1)\n",
        "\n",
        "  if (xlim==None):\n",
        "    xlim=[afile.bounds.left, afile.bounds.right]\n",
        "    # afile.bounds returns a BoundingBox(left, bottom, right, top) object\n",
        "\n",
        "  if (ylim==None):\n",
        "    ylim=[afile.bounds.bottom, afile.bounds.top]\n",
        "  \n",
        "  # zoom in to an area of interest\n",
        "  ax.set_xlim(xlim)\n",
        "  ax.set_ylim(ylim)\n",
        "  plot.show(imgfile, ax=ax, cmap='Greens')\n",
        "\n",
        "  imgfile.close()\n",
        "\n",
        "  # remove the temporary file\n",
        "  os.remove('tmp_ndvi_imagefile_ cjdlsbYFEOGFHEWBVUW.tiff')\n",
        "\n",
        "  return()\n",
        "\n",
        "\n",
        "# clip the files\n",
        "zoomfiles = [] # remember the file names\n",
        "\n",
        "# arrange our subplots, assuming a 16:9 screen ratio\n",
        "cols = min(nfiles, 4) # maximum of 4 plots in one row\n",
        "rows = math.ceil(nfiles / cols) # round up to nearest integer\n",
        "\n",
        "# create a figure with subplots\n",
        "fig, ax = plt.subplots(rows, cols, figsize=(21,7))\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "# iterate over all NDVI files and plot them\n",
        "for x in range(nfiles):\n",
        "  ndvifile = ndvifiles[x]\n",
        "\n",
        "  # make the filename of the new zoom image file\n",
        "  zoomfile = ndvifile.split(\".\")[0] + \"_zoom.tif\"\n",
        "  zoomfiles.append(zoomfile) # remember the zoom file name in our list\n",
        "  print(zoomfile)\n",
        "\n",
        "  # clip it with rasterio to the shapefile extent\n",
        "  # rasterio offers an option called 'window' to load a subset of a raster file\n",
        "\n",
        "  # open the source file\n",
        "  with rasterio.open(ndvifile, 'r') as src:\n",
        "    \n",
        "    # convert the shapefile extent to a rasterio window object\n",
        "    window = longlat2window((extent[0], extent[1]), (extent[2], extent[3]), src)\n",
        "    print(\"Window coordinates: \", window)\n",
        "    \n",
        "    # read all bands but only for the window extent\n",
        "    arr = src.read(window=window, out_shape=(src.count, window.height, window.width))\n",
        "    print(\"Window array size: \", arr.shape)\n",
        "\n",
        "    # get the data type\n",
        "    dt = arr.dtype\n",
        "\n",
        "    # open the destination file\n",
        "    # copy metadata from source file\n",
        "    # BUT we must change the geotransform to the window with the update below\n",
        "    # https://rasterio.readthedocs.io/en/latest/topics/windowed-rw.html\n",
        "    kwargs = src.meta.copy()\n",
        "    kwargs.update({'height': window.height,\n",
        "                   'width': window.width,\n",
        "                   'transform': rasterio.windows.transform(window, src.transform),\n",
        "                   'driver': 'Gtiff', \n",
        "                   'count': src.count,\n",
        "                   'crs': src.crs,\n",
        "                   'dtype': dt\n",
        "                   })\n",
        "\n",
        "    with rasterio.open(zoomfile, 'w', **kwargs) as dst:\n",
        "      dst.write(arr)\n",
        "\n",
        "      # close the destination file\n",
        "      dst.close()\n",
        "\n",
        "    # close the sourcefile\n",
        "    src.close()\n",
        "\n",
        "  # plot it\n",
        "  with rasterio.open(zoomfile, \"r\") as img:\n",
        "    show_ndvi(img, ax=ax[x])\n",
        "    # set a title for the subplot\n",
        "    mytitle = s2IDs[x]\n",
        "    ax[x].set_title(mytitle, fontsize=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCo-c2l6_75f"
      },
      "source": [
        "# Now let's extract some statistics on NDVI from a small polygon\n",
        "\n",
        "We will use the zonal statistics function from the Rasterstats library for this purpose, as it is easy to use.\n",
        "\n",
        "https://pythonhosted.org/rasterstats/manual.html\n",
        "\n",
        "We will save the files in .csv format, so they can be read into Excel. We generate one file for each image.\n",
        "\n",
        "In Python we can also save files with entire objects in their original form. The pickle library allows us to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VErsFVio4QJ3"
      },
      "source": [
        "# make an empty list to store our output statistics csv file names, one per image\n",
        "statsfiles = []\n",
        "\n",
        "# make an empty list to store our output statistics pickle file names, one per image\n",
        "pklfiles = []\n",
        "\n",
        "# iterate over all NDVI files and extract zonal statistics\n",
        "for x in range(nfiles):\n",
        "  zoomfile = zoomfiles[x]\n",
        "\n",
        "  # make the filename of the new statistics output file\n",
        "  statsfile = zoomfile.split(\".\")[0] + \"_stats.csv\"\n",
        "  statsfiles.append(statsfile) # remember the file name in our list\n",
        "\n",
        "  # get zonal statistics for all polygons in the shapefile\n",
        "  # the result is a list of dictionaries\n",
        "  '''\n",
        "  exclude zero values from calculation by setting a nodata value\n",
        "  '''\n",
        "  stats = zonal_stats(shapefile, zoomfile, nodata=0, stats=\"count min mean max median\")\n",
        "\n",
        "  # get the number of polygons of the shapefile.\n",
        "  # there should be one row with statistics for each of them.\n",
        "  n = len(stats)\n",
        "  print(\"The shapefile has \", n, \" polygons.\")\n",
        "\n",
        "  # Write the statistics results to a text file (overwrite if exists)\n",
        "  # opening the csv file in 'w' mode \n",
        "  f = open(statsfile, \"w\", newline=\"\\n\")\n",
        "    \n",
        "  with f: \n",
        "    # write the header line\n",
        "    header = [\"count\", \"min\", \"mean\", \"max\", \"median\"] \n",
        "    writer = csv.DictWriter(f, fieldnames = header) \n",
        "    writer.writeheader() \n",
        "    # iterate over each polygon\n",
        "    for i in range(n): \n",
        "      # writing data row-wise into the csv file \n",
        "      writer.writerow(stats[i]) \n",
        "\n",
        "  # save and close the file\n",
        "  f.close()\n",
        "\n",
        "  # open the file\n",
        "  f = open(statsfile,\"r\") \n",
        "\n",
        "  # read and print its contents (all lines)\n",
        "  pprint(f.read().splitlines())\n",
        "\n",
        "  # close the file\n",
        "  f.close()\n",
        "  print(\"\\nSaved statistics file: \" + statsfile)\n",
        "\n",
        "  # make the filename of the new pickle file for the stats object\n",
        "  pklfile = zoomfile.split(\".\")[0] + \"_stats.pkl\"\n",
        "  pklfiles.append(pklfile) # remember the file name in our list\n",
        "\n",
        "  # write object to file\n",
        "  pickle.dump(stats, open(pklfile, 'wb'))\n",
        "  print(\"\\nSaved statistics file: \" + pklfile + \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJYaduV2aC6C"
      },
      "source": [
        "# Plotting data as graphs\n",
        "Before we finish, let's just explore how we can make graphs to explore our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVycoGxmZ5Bu"
      },
      "source": [
        "# iterate over all statistics files\n",
        "for x in range(nfiles):\n",
        "  pklfile = pklfiles[x]\n",
        "\n",
        "  # read object from file\n",
        "  s = pickle.load(open(pklfile, 'rb'))\n",
        "  print(\"\\nThe pickled file looks like this:\")\n",
        "  print(s)\n",
        "  print(\"The stored object is of type: \", type(s))\n",
        "  print(\"The list has this many elements: \", len(s))\n",
        "  \n",
        "  print(\"We can iterate over the elements in the list.\")\n",
        "  for poly in range(len(s)):\n",
        "    print(s[poly])\n",
        "\n",
        "  print(\"The elements are of type:\", type(s[0]))\n",
        "  print(\"We can get the elements in the dictionary like this:\")\n",
        "  for poly in range(len(s)):\n",
        "    a = s[poly] # get the list element\n",
        "    b = a[\"min\"] # get the dictionary entry for \"min\"\n",
        "    print(\"Minimum = \", b)\n",
        "\n",
        "  print(\"To get all dictionary entries for the minimum NDVI per polygon as a list object:\")\n",
        "  a_min = [b[\"min\"] for b in s]\n",
        "  pprint(a_min)\n",
        "\n",
        "  # and get the other statistics too\n",
        "  a_max = [b[\"max\"] for b in s]\n",
        "  a_mn = [b[\"mean\"] for b in s]\n",
        "  a_n = [b[\"count\"] for b in s]\n",
        "  a_md = [b[\"median\"] for b in s]\n",
        "\n",
        "  # make a bar chart of the data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3J_hTcuafw3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}