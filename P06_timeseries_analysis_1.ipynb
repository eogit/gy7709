{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "P06_timeseries_analysis_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eogit/gy7709/blob/master/P06_timeseries_analysis_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___N_2vyp_f2",
        "colab_type": "text"
      },
      "source": [
        "# Time series analysis in Python 1\n",
        "\n",
        "In this practical, we will analyse some time series of data (not spatial, only temporal). This can be applied to average values of remotely sensed variables within areas of interest (poygons) or pixel by pixel if the script is extended.\n",
        "\n",
        "But first, we need to connect to our data drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RvpK17wXp_f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount your Google Drive as a drive in the virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRhYL94o8sse",
        "colab_type": "text"
      },
      "source": [
        "Set some plotting options and open a data file from Google Drive.\n",
        "\n",
        "Then, read the data file with MERIS-derived chlorophyll-a values over Lake Balaton into Python. It is a tab-delimited text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT_LY5hj6Auu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A22Jm258tIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set plotting option for notebook\n",
        "%matplotlib inline \n",
        "\n",
        "# open the data file in plain text format on your Google Drive\n",
        "f = open(\"/content/drive/My Drive/practicals/p06/tsdata.txt\", \"r\")\n",
        "print(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiDyk0Mrp_gB",
        "colab_type": "text"
      },
      "source": [
        "The strange output above gives us the type of Python object, but not the file contents. How can we print the file content?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0G0gX4Kp_gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kefZobdp_gK",
        "colab_type": "text"
      },
      "source": [
        "Now we need to read the data table into a data object, instead of a text object. We can most easily do this using PANDAS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6cfJd71p_gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "chla = pd.read_csv(\"/content/drive/My Drive/practicals/p06/tsdata.txt\", sep=\"\\t\")\n",
        "print(chla)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpKn1DsIp_gQ",
        "colab_type": "text"
      },
      "source": [
        "Notice the difference? If we read in the file into a dataframe, the missing values are set to NaN. Above, when we printed the text file directly, missing values were simply blank.\n",
        "We can examine the data that the dataframe contains as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2LUawT9p_gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(chla.dtypes) # print data types of all columns\n",
        "print(chla.columns) # print index of all column names\n",
        "[col for col in chla] # a more pythonic way of printing all column names as a vector\n",
        "ncol = len([col for col in chla]) # get dimensions of the dataframe as variables for further use\n",
        "print(\"Number of time series columns:\", ncol)\n",
        "nrow = len(chla.index)\n",
        "print(\"Length of the time series:\", nrow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eSWNcJ2p_gZ",
        "colab_type": "text"
      },
      "source": [
        "The dataframe has a 64-bit integer column with the Date and four floating point columns.\n",
        "We can access any particular column as a variable with the following notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fDTB3lMp_gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(chla[\"Basin_1_MERIS_Chl_a\"].head(10)) # head allows us to only print the first n rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUZKy4mNp_gh",
        "colab_type": "text"
      },
      "source": [
        "# Gap filling\n",
        "Let's prepare the data for our trend analysis by filling in missing values.\n",
        "To do this, we make the simple assumption that each missing value can be replaced by linear interpolation between its two neighbouring data points.\n",
        "We will draw on the functionality of some libraries for this task.\n",
        "N.B. This algorithm only works for equidistant data points, i.e. it assumes that all time steps are the same and does not use the time/date column as x when filling gaps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZH5JU7c_p_gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import statsmodels.formula.api as sm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rc('figure', figsize=(15, 7)) # change the default figure size\n",
        "\n",
        "x = 'Date' # time step variable column name\n",
        "\n",
        "chla_filled = chla.copy() # make an indentical copy of the dataframe\n",
        "colnames = list(chla_filled) # get list of column names\n",
        "print(\"Number of NaNs in each column:\")\n",
        "print(chla_filled.isnull().sum(axis = 0)) # get sum of all NaN values in each column\n",
        "for c in colnames[1:len(colnames)]: # iterate over all columns in the dataframe, but omit index 0, which is the Date column\n",
        "    print(\"Gap-filling data column: \", c)\n",
        "    gaps = chla[c].index[chla[c].isna()] # gives a list of all the data gaps (NaN or None) as indices of the dataframe\n",
        "    good = chla[c].index[chla[c].notnull()] # gives a list of all the good data points (not NaN or None) as indices of the dataframe\n",
        "    plt.plot(chla[x].index[good], chla[c].iloc[good], '+k--') # the letter k indicates black line colour \n",
        "    #plt.xticks(chla[x].index, chla[x])\n",
        "    #plt.show()\n",
        "    #print(chla[c].iloc[gaps[0]])\n",
        "    for t in gaps: # iterate over each gap\n",
        "        tlow = max(chla[c].iloc[0:t].index[chla[c].iloc[0:t].notnull()]) \n",
        "            # time index of nearest earlier data point that is not NaN\n",
        "        thi = min(chla[c].iloc[(t+1):nrow].index[chla[c].iloc[(t+1):nrow].notnull()]) \n",
        "            # time index of nearest later data point that is not NaN\n",
        "        # you can uncomment the following line to see detailed output of the search algorithm:\n",
        "        # print(\"Index: \", t, \".  Nearest good data pair: \", tlow, \" and \", thi)\n",
        "        ylow = chla[c].iloc[tlow] # value of nearest earlier data point\n",
        "        yhi = chla[c].iloc[thi] # value of nearest later data point\n",
        "        df = pd.DataFrame({\"X\": [tlow, thi], \"Y\": [ylow, yhi]}) # define a Pandas Dataframe\n",
        "        model = sm.ols(formula=\"Y ~ X\", data=df).fit() # fit a linear regression to the two data points\n",
        "        yhat = model.predict(exog=dict(X=t)) # predict missing values for the gap at time t. We need to hand over the x values as a Python dictionary like so.\n",
        "        chla_filled.at[t,c] = yhat # fill in the interpolated values into the gaps\n",
        "        plt.plot(df[\"X\"], df[\"Y\"], 'xb-') # add the data points used for interpolation to the plot in blue\n",
        "        plt.plot(t, yhat, 'or') # add interpolated data to the plot in red\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8JBgoUNp_gs",
        "colab_type": "text"
      },
      "source": [
        "# Trend analysis\n",
        "\n",
        "We will use an ordinary least-squares linear regression to check for a linear trend. There are at least 8 different packages offering this functionality in Python. Here, we use STATSMODELS, which offers the best output functionality.\n",
        "The code below is written for time series data that have equal time intervals between data points and uses the time index as x values rather than the Date column.\n",
        "Let's use Basin 1 as an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LZCS6WC1p_gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# define the dataframe columns for the trend analysis\n",
        "xcol = 'Date' # independent variable\n",
        "ycol = 'Basin_1_MERIS_Chl_a'# dependent variable\n",
        "\n",
        "# pull out x and y data series\n",
        "x = chla_filled[xcol].index.values\n",
        "x = sm.add_constant(x) # this adds a column to x and makes the model include a constant (or intercept)\n",
        "y = chla_filled[ycol].values\n",
        "\n",
        "# fit the model\n",
        "model = sm.OLS(y, x).fit()\n",
        "print(model.summary())\n",
        "\n",
        "# predict y from the data\n",
        "chla_filled['B1pred'] = model.predict()\n",
        "\n",
        "# plot the results\n",
        "plt.plot(x[:,1], chla_filled[ycol], 'xk-') # plot the time series data\n",
        "plt.plot(x[:,1], chla_filled['B1pred'], 'r-') # plot the linear model\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONKqp6J-p_gy",
        "colab_type": "text"
      },
      "source": [
        "The p value shows that at the 95% confidence level the slope is not statistically significant (p=0.154)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mfzQru4p_g1",
        "colab_type": "text"
      },
      "source": [
        "# Your portfolio task\n",
        "\n",
        "For your practical portfolio, redo the analysis for a basin (1-4) of your choice. Add relevant graphs and figures and describe what you can infer from the trend analysis about the algal blooms in Lake Balaton.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "915izJVRp_g4",
        "colab_type": "text"
      },
      "source": [
        "# Seasonality analysis\n",
        "\n",
        "We can decompose the time series into a trend, seasonal component and a residual error term. STATSMODEL offers a naive decomposition:\n",
        "The additive model is Y[t] = T[t] + S[t] + e[t]\n",
        "The multiplicative model is Y[t] = T[t] * S[t] * e[t]\n",
        "\n",
        "The seasonal component is first removed by applying a convolution filter to the data. The average of this smoothed series for each period is the returned seasonal component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkUGkFMzp_g7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# define a frequency that we think causes the seasonality, here 36 dekads (10-day periods) per year\n",
        "f = 36\n",
        "\n",
        "# fit the model\n",
        "result = seasonal_decompose(y, model='additive', freq=f)\n",
        "plt.figure(figsize=(15,2))\n",
        "plt.plot(result.observed)\n",
        "plt.title(\"Observed\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(15,2))\n",
        "plt.plot(result.trend)\n",
        "plt.title(\"Trend\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(15,2))\n",
        "plt.plot(result.seasonal)\n",
        "plt.title(\"Seasonality\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(15,2))\n",
        "plt.plot(result.resid)\n",
        "plt.title(\"Residuals\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJlC63dZp_hA",
        "colab_type": "text"
      },
      "source": [
        "# Your portfolio task\n",
        "\n",
        "Add relevant graphs and reflect on the decompositon of the time series of phytoplankton concentration in Lake Balaton into a trend, a seasonal component and a residual term. What does this analysis allow you to infer about the data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZjSMv1sp_hB",
        "colab_type": "text"
      },
      "source": [
        "# Autocorrelation analysis\n",
        "\n",
        "This analysis tests whether the values in the time series are correlated with their predecessors.\n",
        "The autocorrelation function calculates the correlation coefficient of the time series with itself shifted by lag x.\n",
        "The partial autocorrelation function does the same, but eliminates the correlations with all lower lags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5KR0-Pp_hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.stattools import acf\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# calculate the autocorrelation function, in case you want to use the results for further analysis\n",
        "acfresult =  acf(y, unbiased=True, nlags=36, qstat=True, alpha=0.05, fft=True)\n",
        "\n",
        "# plot the autocorrelation function graphically\n",
        "plot_acf(y, unbiased=True, lags=36, alpha=0.05, title='Autocorrelation Basin 1')\n",
        "\n",
        "# plot the partial autocorrelation function, which removes the correlation of all lower lags for each lag term\n",
        "plot_pacf(y, lags=36)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT2KHyFsp_hJ",
        "colab_type": "text"
      },
      "source": [
        "# Your portfolio task\n",
        "\n",
        "Change the 'lags' parameter and find the best value. Add relevant graphs and discuss the autocorrelation properties in the time series of phytoplankton concentration in Lake Balaton. Which environmental processes could influence the autocorrelation?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klf2sOE4p_hL",
        "colab_type": "text"
      },
      "source": [
        "# ARIMA\n",
        "\n",
        "Remember, ARIMA model has 3 parameters:\n",
        "    p — Number of autoregressive terms of the model\n",
        "    d — Number of nonseasonal differencing terms of the model\n",
        "    q — Number of moving average terms of the model\n",
        "\n",
        "Be aware that ARIMA requires stationary time series data. This means, we have to remove any trend over time and any seasonality if we want to apply this technique to this dataset. There is a better suited method for non-stationary seasonal data:\n",
        "\n",
        "# Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
        "\n",
        "For our data, the Seasonal Autoregressive Integrated Moving Average (SARIMA) method is appropriate. It combines the ARIMA model with the ability to perform the same autoregression, differencing, and moving average modelling at the seasonal level.\n",
        "\n",
        "The notation for the model involves specifying the order for the AR(p), I(d), and MA(q) models as parameters to an ARIMA function and AR(P), I(D), MA(Q) and m parameters at the seasonal level, e.g. SARIMA(p, d, q)(P, D, Q)m where “m” is the number of time steps in each season (the seasonal period). A SARIMA model can be used to develop AR, MA, ARMA and ARIMA models.\n",
        "\n",
        "The method is suitable for univariate time series with trend and/or seasonal components.\n",
        "The autocorrelation function gives important clues for the parameterisation of the ARIMA or SARIMA model.\n",
        "\n",
        "Details of other methods: https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/\n",
        "\n",
        "Here are some useful rules how to choose good parameters for ARIMA models: http://people.duke.edu/~rnau/arimrule.htm\n",
        "And here for choosing SARIMA model parameters: https://www.datasciencecentral.com/profiles/blogs/tutorial-forecasting-with-seasonal-arima\n",
        "\n",
        "The autocorrelation plot above shows us that the data have a positive autocorrelation for lag 1 that gradually declines for lags 2 to 4. This is a clue that our data has an AR term.\n",
        "The partial autocorrelation plot shows positive partial autocorrelation for lags 1 and 2. This suggests that 2 AR terms are a good starting point for our model.\n",
        "We can explore an expanded model that also includes an MA term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVUgZVfZp_hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from random import random, sample\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDqm6M3hp_hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# measure execution time of this model fitting\n",
        "start = time.time()\n",
        "\n",
        "# define the SARIMA parameters of model 1\n",
        "p = 2 # 2 AR terms, based on partial autocorrelation function being positive for lags 1 and 2\n",
        "d = 1 # to remove any longer-term trend by differencing\n",
        "q = 0 # no MA term\n",
        "ps = 2 # 2 seasonality AR terms based on partial autocorrelation function\n",
        "ds = 1\n",
        "qs = 0\n",
        "ms = 36 # 36 dekads (10-day periods) are the length of one 'season' (year)\n",
        "\n",
        "# fit model 1 \n",
        "model1 = SARIMAX(y, order=(p,d,q), seasonal_order=(ps,ds,qs,ms))\n",
        "SARIMA1result = model1.fit(disp=0)\n",
        "\n",
        "# print the model result\n",
        "print(SARIMA1result.summary())\n",
        "\n",
        "# print execution time\n",
        "end = time.time()\n",
        "print(\"Execution time for model 1 (s) = % 10.4f\" % (end - start))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50gJUPMRp_hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this bit can take some time to compute (5-15 minutes when I tried it out)\n",
        "# measure execution time of this model fitting\n",
        "start = time.time()\n",
        "\n",
        "# define the SARIMA parameters of model 2\n",
        "p = 2 # 2 AR terms, based on partial autocorrelation function being positive for lags 1 and 2\n",
        "d = 1 # to remove any longer-term trend by differencing\n",
        "q = 1 # add an MA term\n",
        "ps = 2 # 2 seasonality AR terms based on partial autocorrelation function\n",
        "ds = 1 # seasonal differencing\n",
        "qs = 0 # no seasonal MA term\n",
        "ms = 36 # 36 dekads (10-day periods) are the length of one 'season' (year)\n",
        "\n",
        "# fit model 2 \n",
        "model2 = SARIMAX(y, order=(p,d,q), seasonal_order=(ps,ds,qs,ms))\n",
        "SARIMA2result = model2.fit(disp=0)\n",
        "\n",
        "# print the model result\n",
        "print(SARIMA2result.summary())\n",
        "\n",
        "# print execution time\n",
        "end = time.time()\n",
        "print(\"Execution time for model 2 (s) = % 10.4f\" % (end - start))\n",
        "\n",
        "# measure execution time of the model predictions\n",
        "start = time.time()\n",
        "\n",
        "# In-sample one-step-ahead predictions\n",
        "# One-step-ahead prediction uses the true values of the endogenous values at each step to predict the next in-sample value. \n",
        "yhat1 = SARIMA1result.predict(start=0, end=len(y)-1)\n",
        "yhat2 = SARIMA2result.predict(start=0, end=len(y)-1)\n",
        "\n",
        "# print execution time\n",
        "end = time.time()\n",
        "print(\"Execution time for model predictions (s) = % 10.4f\" % (end - start))\n",
        "\n",
        "# plot the results\n",
        "plt.plot(x[:,1], y, 'xk-') # plot the time series data\n",
        "plt.plot(x[:,1], yhat1, 'r-') # plot the linear model\n",
        "plt.plot(x[:,1], yhat2, 'b-') # plot the linear model\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT1VTgu4p_hc",
        "colab_type": "text"
      },
      "source": [
        "If you have done this with the Basin 1 data, then we can see from the results above that both models (with and without the MA term) provide a good fit to the data. Based on the summary tables of the model results, model 1 is a better fit. This is evident from three things: (i) the Akaike Information Criterion (AIC) is higher; (ii) model 2 fails to converge to a solution (see warning message), (iii) all MA terms are not statistically significant (P-values >0.05).\n",
        "If you have used a different Basin, interpret your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91GvlJwIp_hd",
        "colab_type": "text"
      },
      "source": [
        "# Your portfolio task\n",
        "\n",
        "Change some of the SARIMA parameters and find a model that you think is good. \n",
        "Add relevant graphs and tables to your portfolio and discuss why you chose that particular model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrBhnSRlp_hf",
        "colab_type": "text"
      },
      "source": [
        "# Harmonic analysis\n",
        "\n",
        "Harmonic analysis (sometimes called HANTS) is based on a Fourier transform of the time-series.\n",
        "\n",
        "Here is an implementation of HANTS for Python 2.7 that allows direct analysis of stacks of images: https://github.com/gespinoza/hants\n",
        "\n",
        "If we write code in a Python environment, we can call an executable R script from within Python: \n",
        "To create an R executable file, we just need to add this in the first line of the R script:\n",
        "    #! /usr/bin/Rscript\n",
        "and give it execution rights under file permissions.\n",
        "Then we can call the R script from Python with subprocess:\n",
        "    subprocess.call (\"/pathto/MyrScript.r\")\n",
        "https://www.linkedin.com/pulse/interfacing-r-from-python-3-jupyter-notebook-jared-stufft/\n",
        "\n",
        "What we are going to do in this Notebook is running R code directly in a Jupyter Notebook cell.\n",
        "In our environment, we have installed some packages to make this work: rpy2 and tzlocal.\n",
        "rpy2 needs to be of a specific version, to be compatible with the Windows Operating System."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU9X_m9Hp_hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the package that allows execution of R code within Python scripts\n",
        "# note that version 2.9.4 is compatible with Windows also\n",
        "import rpy2.rinterface\n",
        "%load_ext rpy2.ipython\n",
        "\n",
        "# define working directory to be handed to R script\n",
        "# this must point to the location of the data file \"Balaton_Chla_HB-harmonics.txt\"\n",
        "wd = r'/content/drive/My Drive/practicals/p06'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCSKIwzcp_hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%R -i wd\n",
        "# switch from Python to R programming language for this Notebook cell\n",
        "# and hand over a variable with the working directory\n",
        "\n",
        "#######################################\n",
        "# Code for harmonic analysis in R\n",
        "#######################################\n",
        "#\n",
        "# written by Heiko Balzter, copyright University of Leicester (2014)\n",
        "# includes modified functions from source: http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.html\n",
        "#\n",
        "#######################################\n",
        "\n",
        "# t is a time index from 0 to n-1 where n is the number of measurements in the time-series\n",
        "# xt is the vector of time points at which the measurements were taken\n",
        "# The amplitude of a wave is defined as half the height from the maximum to the minimum point.\n",
        "# The phase of the wave is defined as the angle by which the sine wave is delayed to its first peak.\n",
        "# A harmonic term is defined by how many complete waves it has within the defined time series, from start to end,\n",
        "#    i.e. harmonic term 2 has two full waves (two maxima and two minima) within the time series.\n",
        "# The fundamental period is the period between the first sample and the last.\n",
        "# The acquisition frequency is the number of measurements between two successive units of time. \n",
        "# The fundamental frequency f_0 is 1/N where N is the number of time steps. \n",
        "# The frequencies of the wave components must be integer multiples of the fundamental frequency.\n",
        "# f_0 is called the first harmonic, the second harmonic is 2*f_0, the third is 3*f_0, etc.\n",
        "\n",
        "get.trajectory <- function(X.k,xt,acq.freq) {\n",
        "# Inverse Fourier Transform: \n",
        "# Returns the x.n time series for a given time sequence (xt) and a vector with the amount of frequencies k in the signal (X.k)\n",
        "  n   <- length(xt)\n",
        "  i   <- complex(real = 0, imaginary = 1)\n",
        "  x.n <- rep(0,n)\n",
        "  ks  <- 0:(length(X.k)-1)\n",
        "  for(j in 0:(n-1)) { # compute each time point x_n based on freqs X.k\n",
        "    x.n[j+1] <- sum(X.k * exp(i*2*pi*ks*j/n)) / n\n",
        "  }\n",
        "  x.n * n\n",
        "}\n",
        "\n",
        "plot.fourier <- function(fourier.series, f.0, xt, ...) {\n",
        "# plot a Fourier series\n",
        "# ***** This function has been verified. *****\n",
        "  w <- 2*pi*f.0\n",
        "  trajectory <- sapply(xt, function(t) fourier.series(t,w))\n",
        "  plot(xt/length(xt), trajectory, type=\"l\", xlab=\"time\", ylab=\"f(t)\"); abline(h=0,lty=3)\n",
        "}\n",
        "\n",
        "convert.fft <- function(x.k, acq.freq=1) {\n",
        "# convert a FFT to amplitude and phase\n",
        "# x.k is the vector of complex points to convert\n",
        "  n <- length(Re(x.k)) # number of points\n",
        "  x.k <- x.k / n # normalize\n",
        "  distance.center <- function(c)signif( Mod(c),        4)\n",
        "  angle           <- function(c)signif( 180*Arg(c)/pi, 3)\n",
        "  df <- data.frame(cycle    = 0:(n-1),\n",
        "                   freq     = 0:(n-1) / acq.freq,\n",
        "                   t        = n / 0:(n-1) / acq.freq, # in time units, not sequential units\n",
        "                   ampl     = sapply(x.k, distance.center) * 2 * n,\n",
        "                   phase    = sapply(x.k, angle))\n",
        "  df\n",
        "}\n",
        "\n",
        "plot.frequency.spectrum <- function(X.k, acq.freq=acq.freq, col = 1, lwd = 2, pch = \"+\", ...) {\n",
        "# plot a frequency spectrum of a given X_k\n",
        "  xax <- (0:(length(X.k)-1)) / length(X.k) * acq.freq\n",
        "  xlimits <- c(0, max(xax)/2)\n",
        "  plot.data  <- cbind( xax, 2 * Mod(X.k))\n",
        "  plot(plot.data, t=\"h\", main=\"Periodogram\", xlab=\"Frequency\", ylab=\"Power spectral density\",\n",
        "       col = col, lwd = lwd, pch = pch, xlim=xlimits, ylim=c(0,max(Mod(plot.data[,2]))))\n",
        "}\n",
        "\n",
        "plot.harmonic <- function(xk, i, xt, acq.freq, mar=c(1,1,1,1),\n",
        "  col = 3, lwd = 2, pch = \"+\", cex.lab = 1, cex.axis = 1, cex.main = 1, cex.sub = 1, ...) {\n",
        "# plot.harmonic() plots the i-th harmonic on the current plot\n",
        "# xk: the frequencies computed by the FFt\n",
        "#  i: which harmonic\n",
        "# xt: the sampling time points\n",
        "# acq.freq: the acquisition rate\n",
        "  xk.h <- rep(0,length(xk))\n",
        "  xk.h[i+1] <- xk[i+1] # i-th harmonic\n",
        "  harmonic.trajectory <- 2 * get.trajectory(xk.h, xt, acq.freq=acq.freq)\n",
        "  points(xt, Re(harmonic.trajectory), type=\"l\", mar=mar,\n",
        "       col = col, lwd = lwd, pch = pch, cex.lab = cex.lab, cex.axis = cex.axis,\n",
        "       cex.main = cex.main, cex.sub = cex.sub)\n",
        "}\n",
        "\n",
        "get.harmonic <- function(xk, i, xt, acq.freq) {\n",
        "# Get the values that define the i-th harmonic term.\n",
        "# xk: the frequencies computed by the FFt\n",
        "#  i: which harmonic term(s)\n",
        "# xt: the sampling time points\n",
        "# acq.freq: the acquisition rate\n",
        "  xk.h <- rep(0,length(xk))\n",
        "  xk.h[i+1] <- xk[i+1] # i-th harmonic\n",
        "  harmonic.trajectory <- 2 * get.trajectory(xk.h, xt, acq.freq=acq.freq)\n",
        "  Re(harmonic.trajectory)\n",
        "}\n",
        "\n",
        "harmonic <- function(xt, x, acq.freq, N, alpha, detrend, which, test, ...) {\n",
        "# core harmonic analysis function\n",
        "# xt = a vector of time steps in units of s,min, hr or other time units, does not have to be integers\n",
        "# x = a vector of time-series observations with the same length as xt\n",
        "# N = number of the harmonic terms to be included, starting with largest amplitude\n",
        "# acq.freq = number of measurements between two successive units of time. \n",
        "# alpha = type I error probability for statistical significance testing (default 0.05 or 5%)\n",
        "# detrend = TRUE or FALSE, if TRUE (default) then linear detrending is applied to the data\n",
        "# which = \"strongest\": the N strongest harmonic terms are included in the model (default)\n",
        "#       = \"first\": the first N terms are included, or\n",
        "#       = \"all\": all harmonics are included.\n",
        "# test = \"bonferroni\" adjusts type I error by the number of tests N; \"holm\" adjusts the type I error by N+1-k where k=1:N \n",
        "#\n",
        "# How to understand the harmonic terms: \n",
        "# Cycle means the number of waves in the time series, i.e. cycle = 9 the wave fits 9 times into the length of the data\n",
        "#      which is the annual cycle for a 9-year series. \n",
        "# Freq is the position in the frequency domain (periodogram) from 0 to 0.5. \n",
        "# t is the position in the time domain from 0 to n-1 where n is the number of measurements. \n",
        "# Amplitude is the strength of the wave and \n",
        "# phase is the delay of the wave in degrees (0-360).\n",
        "\n",
        "  dig.aov <- 4 # number of significant digits for ANOVA table\n",
        "  if (missing(acq.freq)) acq.freq <- 1\n",
        "  if (missing(N)) N <- 20\n",
        "  if (missing(alpha)) alpha <- 0.05\n",
        "  if (missing(detrend)) detrend <- TRUE\n",
        "  if (missing(which)) which <- \"strongest\"\n",
        "  if (!(which %in% c(\"strongest\", \"first\", \"all\"))) which <- \"strongest\"\n",
        "  if (missing(test)) test <- \"bonferroni\"\n",
        "  if (!(test %in% c(\"bonferroni\", \"holm\"))) test <- \"bonferroni\"\n",
        "  t <- 0:(n-1)\n",
        "  # detrending\n",
        "  n <- length(x)\n",
        "  if (detrend) {\n",
        "    trend <- lm(x~xt) # linear model\n",
        "    cat(\"Linear detrending result:\\n\")\n",
        "    print(summary(trend))\n",
        "    detrended.x <- trend$residuals\n",
        "    } else {\n",
        "    cat(\"No detrending.\\n\\n\")\n",
        "    detrended.x <- x\n",
        "    trend <- \"No detrending\"\n",
        "    }\n",
        "\n",
        "  detrended.x.k <- fft(detrended.x) / n\n",
        "  plot.frequency.spectrum(detrended.x.k, acq.freq=acq.freq, main=\"frequency spectrum\")\n",
        "\n",
        "  # Calculate the amplitude and phase angle for the N harmonic terms\n",
        "  # Cycle 9 means that the harmonic wave repeats 9 times over the time series\n",
        "  # Note that cycle is indexed from 0 and ndx from 1\n",
        "  x.fft <- convert.fft(detrended.x.k, acq.freq)\n",
        "  nx <- length(x.fft$cycle)\n",
        "  # you can get the components of the table from:\n",
        "  #   x.fft$cycle[ndx]\n",
        "  #   x.fft$freq[ndx]\n",
        "  #   x.fft$t[ndx]\n",
        "  #   x.fft$ampl[ndx]\n",
        "  #   x.fft$phase[ndx]\n",
        "\n",
        "  # find the N strongest harmonics\n",
        "  if (which==\"strongest\") {\n",
        "    ndx <- order(x.fft$ampl[1:(nx/2)], decreasing = T)[1:N]\n",
        "    cat(paste(N, \"strongest harmonic terms:\\n\", sep=\" \"))\n",
        "    write.table(round(x.fft[ndx,],4), quote = F, sep = \"\\t\", row.names=F)\n",
        "    cat(\"\\n\")\n",
        "    }\n",
        "  if (which==\"first\") {\n",
        "    ndx <- 2:(N+1)\n",
        "    cat(paste(N, \"first harmonic terms:\\n\", sep=\" \"))\n",
        "    write.table(round(x.fft[ndx,],4), quote = F, sep = \"\\t\", row.names=F)\n",
        "    cat(\"\\n\")\n",
        "    }\n",
        "  if (which==\"all\") {\n",
        "    ndx <- 1:n\n",
        "    cat(paste(\"All harmonic terms:\\n\", sep=\" \"))\n",
        "    write.table(round(x.fft[ndx,],4), quote = F, sep = \"\\t\", row.names=F)\n",
        "    cat(\"\\n\")\n",
        "    }\n",
        "\n",
        "  # test for significance of the individual harmonic terms using the F test\n",
        "  # Reference: Hartley, H. O. (1949): Tests of Significance in Harmonic Analysis. Biometrika, 36, 194-201.\n",
        "  # time dimension ts is in arbitrary units, with acq.freq measurements between two successive units 1 and 2, say\n",
        "  # without loss of generality, for the purpose of the significance testing we treat the \n",
        "  #     time dimension as steps of 1, 2, ..., n\n",
        "  # we do this by adjusting the time index tn <- xt * acq.freq\n",
        "\n",
        "  if (which==\"strongest\" || which==\"first\") {\n",
        "  bonferroni <- alpha / N   # adjusted alpha probability by the number of comparisons, Bonferroni correction\n",
        "  holm <- alpha / seq(N, 1, -1)   # adjusted alpha probability, Bonferroni/Holm correction\n",
        "  gamma = 2*pi/n # in Hartley's paper, but only for time steps of 1\n",
        "  a0 <- mean(detrended.x)\n",
        "  # work out the mean squares (MSQ) of each harmonic term for ANOVA table\n",
        "  ssq <- rep(0, N) # SSQ components \n",
        "  df <- rep(2, N) # degrees of freedom\n",
        "  msq <- rep(0, N) # MSQ components = SSQ / df\n",
        "  a <- rep(0, N) # ai\n",
        "  b <- rep(0, N) # bi\n",
        "  f <- rep(0, N) # F values for each harmonic term\n",
        "  p <- rep(0, N) # p values for each harmonic term\n",
        "  sig <- rep(\"n.s.\", N) # significance\n",
        "  for (i in 1:N) {\n",
        "    a[i] <- 2/n * sum(detrended.x * cos(x.fft$cycle[ndx[i]] * t * gamma)) # note that we use t here and not xt, see above\n",
        "    b[i] <- 2/n * sum(detrended.x * sin(x.fft$cycle[ndx[i]] * t * gamma))\n",
        "  }\n",
        "  # calculate SSQ terms\n",
        "  ssq <- n/2*(a^2+b^2)\n",
        "  # calculate MSQ terms\n",
        "  msq <- ssq/df\n",
        "  # Calculate the residual MSQ variance component:\n",
        "  rssq <- sum((detrended.x-a0)^2) - n/2 * sum(a^2+b^2)\n",
        "  # The total df is n-1. The residual df is the total n  2N - 1.\n",
        "  rdf <- n-2*N-1 # residual df\n",
        "  rmsq <- rssq/rdf\n",
        "  # Work out the F values:\n",
        "  f <- msq / rmsq\n",
        "  # Each harmonic term has 2 degrees of freedom since it is characterised by 2 parameters ai and bi. \n",
        "  # The F ratio is calculated by dividing the MSQ of each harmonic term by the residual MSQ. It should be compared to the F distribution for 2,11 degrees of freedom for the 5%/m point, assuming type I error is controlled at 5%.\n",
        "  p <- pf(f, df1=2, df2=n-N*2-1, lower=FALSE)\n",
        "  # rounding for pretty printing:\n",
        "  ssq <- round(ssq, 2)\n",
        "  msq <- round(msq, 2)\n",
        "  f <- round(f, 1)\n",
        "  p <- round(p, 5)\n",
        "  bonferroni <- round(bonferroni, 5)\n",
        "  holm <- round(holm, dig.aov)\n",
        "  rssq <- round(rssq, dig.aov)\n",
        "  rmsq <- round(rmsq, dig.aov)\n",
        "  # now merge all into a data.frame\n",
        "  if (test==\"bonferroni\") {\n",
        "    sig[p<bonferroni] <- \"*\"\n",
        "    x.aov <- as.data.frame(cbind(x.fft$cycle[ndx], ssq, df, msq, f, p, bonferroni, sig))\n",
        "    names(x.aov) <- c(\"cycle\", \"SSQ\", \"df\", \"MSQ\", \"F\", \"p\", \"pBonf\", \"Sig\")\n",
        "    }\n",
        "  if (test==\"holm\") {\n",
        "    sig[p<holm] <- \"*\"\n",
        "    for (i in 1:(N-1)) if (p[i]>=holm[i]) sig[(i+1):N] <- rep(\"n.s.\", N-i)\n",
        "    x.aov <- as.data.frame(cbind(x.fft$cycle[ndx], ssq, df, msq, f, p, holm, sig))\n",
        "    names(x.aov) <- c(\"cycle\", \"SSQ\", \"df\", \"MSQ\", \"F\", \"p\", \"pHolm\", \"Sig\")\n",
        "    }\n",
        "  # print it\n",
        "  cat(\"ANOVA table for the selected harmonic terms:\\n\")\n",
        "  write.table(format(x.aov, trim = FALSE, justify = \"right\"), quote = F, sep = \"\\t\", row.names=F)\n",
        "  write.table(format(cbind(\"Res.\", rssq, rdf, rmsq), trim = FALSE, justify = \"right\"), quote = F, sep = \"\\t\", row.names=F, col.names=F)\n",
        "  cat(\"\\n\")\n",
        "  x.aov <- merge.data.frame(x.aov, data.frame(c(NA, round(rssq,dig.aov), rdf, round(rmsq,dig.aov), NA,NA,NA,NA), row.names = names(x.aov)))\n",
        "\n",
        "  if (which==\"strongest\") { # select only the significant harmonic terms\n",
        "    ndxs <- ndx[sig==\"*\"]\n",
        "    N <- length(ndxs)\n",
        "    cat(\"\\nRetaining \", N, \"significant harmonic terms.\\n\")\n",
        "    }\n",
        "  if (which==\"first\") { # select the first N harmonic terms\n",
        "    ndxs <- ndx\n",
        "    cat(\"\\nRetaining the first\", N, \" harmonic terms.\\n\")\n",
        "    }\n",
        "\n",
        "  # plot detrended time series and overlay the individual N significant harmonics with the largest amplitudes:\n",
        "  # only plot up to 40 harmonics\n",
        "  x.n <- get.trajectory(detrended.x.k, xt, acq.freq)  # create time wave from detrended data (if detrending is switched on)\n",
        "  par(mfrow = c(1,1))\n",
        "  plot(xt, Re(x.n), type=\"l\", lwd=1, main=\"detrended time series\")\n",
        "  abline(h=0,lty=2)\n",
        "  for (i in 1:min(40, N)) plot.harmonic(detrended.x.k, ndxs[i], xt, acq.freq, col=i+1)\n",
        "\n",
        "  # Now plot detrended time series and the composite of the first significant N harmonics:\n",
        "  plot(xt, Re(x.n), type=\"l\", lwd=1, main=\"composite harmonic model\")\n",
        "  abline(h=0, lty=2)\n",
        "  wave <- get.harmonic(detrended.x.k, ndxs, xt, acq.freq)\n",
        "  lines(xt, wave, col=\"red\")\n",
        "\n",
        "  # And now plot add the trend back on to the composite of the first N harmonics:\n",
        "  if (detrend) {\n",
        "    plot(xt, x, type=\"l\",lwd=1, main=\"composite model with trend\")\n",
        "    abline(trend)\n",
        "    wave <- wave + trend$coef[1] + trend$coef[2] * xt\n",
        "    lines(xt, wave, col=\"red\")\n",
        "    }\n",
        "\n",
        "  # plot residuals\n",
        "    plot(xt, x - wave, type=\"p\", pch=\"+\", main=\"residuals\")\n",
        "  }\n",
        "  else\n",
        "  { # if which == \"all\"\n",
        "  ndxs <- 2:n # in case all harmonics will be included except term 0\n",
        "  N <- n\n",
        "  x.aov <- \"No ANOVA available. All harmonics are included.\"\n",
        "  wave <- x\n",
        "  }\n",
        "\n",
        "  # return these components:\n",
        "  harm <- list(xt)\n",
        "  names(harm) <- \"xt\"\n",
        "  harm$lm <- trend\n",
        "  harm$detrended <- detrended.x\n",
        "  harm$Nsig <- N\n",
        "  harm$ndx <- ndx\n",
        "  harm$frequency.spectrum <- detrended.x.k\n",
        "  harm$Amp <- x.fft$ampl[ndx]\n",
        "  harm$Ph <- x.fft$phase[ndx]\n",
        "  harm$aov <- x.aov\n",
        "  harm$fitted.values <- wave\n",
        "  harm$residuals <- x - wave\n",
        "  harm$call <- match.call()\n",
        "  class(harm) <- \"harmonic\"\n",
        "\n",
        "  # return results  \n",
        "  harm\n",
        "}\n",
        "##################\n",
        "# end of functions\n",
        "##################\n",
        "\n",
        "###############\n",
        "# MAIN code\n",
        "###############\n",
        "\n",
        "# set working directory\n",
        "setwd(wd)\n",
        "\n",
        "# set the handling of warning messages. \n",
        "# If warn is negative all warnings are ignored. \n",
        "# If warn is zero (the default) warnings are stored until the toplevel function returns. \n",
        "#     If 10 or fewer warnings were signalled they will be printed otherwise a message saying how many were signalled. An object called last.warning is created and can be printed through the function warnings. If warn is one, warnings are printed as they occur. \n",
        "# If warn is two or larger all warnings are turned into errors.\n",
        "options(warn=0)\n",
        "\n",
        "####################################\n",
        "# Process Chlorophyll-a data from Lake Balaton by Palmer et al. (2014):\n",
        "####################################\n",
        "\n",
        "file <- \"Log_Balaton_Chla_harmonics.txt\"\n",
        "sink(file)\n",
        "cat(\"Balaton chlorophyll-a analysis from MERIS\\n\")\n",
        "cat(\"Log file: \", file, \"\\n\")\n",
        "file <- \"Balaton_Chla_HB-harmonics.txt\"\n",
        "cat(\"Data file: \", file, \"\\n\")\n",
        "chla <- read.table(file, sep=\"\\t\", header=TRUE)\n",
        "time <- length(chla[,1])/36              # measuring time interval (in years), the original data is in dekads\n",
        "acq.freq <- 36                           # data acquisition frequency (Hz), how many measurements per year\n",
        "ts <- seq(0,time-1/acq.freq, 1/acq.freq) # vector of sampling time-points in months\n",
        "n <- length(chla[,1])\n",
        "f.0 <- 1/time\n",
        "chlafilled <- chla\n",
        "for (site in 1:length(chla[1,])) {\n",
        "  cat(\"\\nSite: \", names(chla)[site], \"\\n\")\n",
        "  cat(\"\\nGap-filling the data:\\n\")\n",
        "  cat(\"Number of NAs: \", length(which(is.na(chla[, site]))), \"\\n\")\n",
        "  plot(chla[,site], type=\"l\", xlab=\"t\", ylab=\"chl-a\", lwd=1, main=names(chla)[site]) \n",
        "  for (t in 1:length(chla[,1])) { # for each time step\n",
        "    if (is.na(chla[t,site])) { # is the value missing?\n",
        "      tlow <- max(which(!is.na(chla[1:(t-1), site]))) # time index of nearest earlier data point\n",
        "      thi  <- t+min(which(!is.na(chla[(t+1):n, site]))) # time index of nearest later data point\n",
        "      xlow <- chla[tlow, site] # value of nearest earlier data point\n",
        "      xhi  <- chla[thi,  site] # value of nearest later data point\n",
        "      tt <- c(tlow, thi)\n",
        "      xx <- c(xlow, xhi)\n",
        "      xlm <- lm(xx~tt)\n",
        "      chlafilled[t,site] <- xlm$coef[1]+xlm$coef[2]*t # fill in\n",
        "      lines(c(t-1, t), c(chlafilled[t-1,site],chlafilled[t,site]), type=\"l\",col=2, lwd=2)\n",
        "      if (!is.na(chlafilled[min(t+1,n),site])) lines(c(t, min(t+1,n)), c(chlafilled[t,site],chlafilled[min(t+1,n),site]), type=\"l\",col=2, lwd=2)\n",
        "      } # endif\n",
        "    } # end of for t\n",
        "  cat(\"\\nCalculating harmonic terms for site .\", names(chla)[site],\"\\n\")\n",
        "  chla.harm <- harmonic(ts, chlafilled[,site], N=40, alpha=0.05, detrend=TRUE, which=\"strongest\", test=\"holm\")\n",
        "  if (site==1) b1.harm <- chla.harm # store the results\n",
        "  if (site==2) b2.harm <- chla.harm # note that the site numbers are column numbers\n",
        "  if (site==3) b3.harm <- chla.harm # they do not necessarily match the data column names\n",
        "  if (site==4) b4.harm <- chla.harm\n",
        "} # end for site\n",
        "\n",
        "#############################\n",
        "# Now, make publication quality graphs, equal axes etc.\n",
        "# resolution of jpeg in dpi\n",
        "res <- 72\n",
        "# width\n",
        "width <- 24*res\n",
        "# height\n",
        "height <- 14*res\n",
        "# point size for text\n",
        "ps <- 30\n",
        "# colour of second data plots\n",
        "col <- 2\n",
        "#############################\n",
        "for (site in 1:length(chla[1,])) {\n",
        "  if (site==1) chla.harm <- b1.harm\n",
        "  if (site==2) chla.harm <- b2.harm\n",
        "  if (site==3) chla.harm <- b3.harm\n",
        "  if (site==4) chla.harm <- b4.harm\n",
        "\n",
        "  cat(\"Plotting the gap filled data.\\n\")\n",
        "  jpeg(filename = paste(names(chla)[site], \"_data.jpg\", sep=\"\"), width = width, height = height, \n",
        "      units = \"px\", pointsize = ps, quality = 100, bg = \"white\", res = res)\n",
        "  plot(chla.harm$xt, chlafilled[,site], type=\"l\", col=col, lwd=3, lty=2, main=names(chla)[site],\n",
        "    ylim=c(0,50))\n",
        "  lines(chla.harm$xt, chla[,site], type=\"l\", xlab=\"t\", ylab=\"chl-a\", lwd=3, col=1)\n",
        "  dev.off()\n",
        "\n",
        "  cat(\"Plotting the periodogram.\\n\")\n",
        "  jpeg(filename = paste(names(chla)[site], \"_periodogram.jpg\", sep=\"\"), width = height, height = height, \n",
        "      units = \"px\", pointsize = ps, quality = 100, bg = \"white\", res = res)\n",
        "  plot.frequency.spectrum(chla.harm$frequency.spectrum, acq.freq=acq.freq)\n",
        "  dev.off()\n",
        "\n",
        "  cat(\"Plotting the individual harmonic terms.\\n\")\n",
        "  jpeg(filename = paste(names(chla)[site], \"_harmonics.jpg\", sep=\"\"), width = width, height = height, \n",
        "      units = \"px\", pointsize = ps, quality = 100, bg = \"white\", res = res)\n",
        "  plot(chla.harm$xt, chlafilled[,site]-chla.harm$lm[[1]][1]-chla.harm$lm[[1]][2]*chla.harm$xt, \n",
        "      type=\"l\", lwd=2, ylim=c(-20,40))\n",
        "  abline(h=0,lty=2)\n",
        "  N <- chla.harm$Nsig\n",
        "  for (i in 1:N) {\n",
        "    plot.harmonic(chla.harm$frequency.spectrum, chla.harm$ndx[i], chla.harm$xt, acq.freq, col=i+1)\n",
        "    }\n",
        "  dev.off()\n",
        "\n",
        "  cat(\"Plotting the composite harmonic model with trend added back on against the original data.\\n\")\n",
        "  jpeg(filename = paste(names(chla)[site], \"_predicted.jpg\", sep=\"\"), width = width, height = height, \n",
        "      units = \"px\", pointsize = ps, quality = 100, bg = \"white\", res = res)\n",
        "  plot(chla.harm$xt, chlafilled[,site], type=\"l\", col=1, lwd=2, ylim=c(0,50))\n",
        "  lines(chla.harm$xt, Re(chla.harm$fitted.values), type=\"l\", xlab=\"t\", ylab=\"chl-a\", lwd=2, col=col)\n",
        "  abline(chla.harm$lm, lty = 2)\n",
        "  dev.off()\n",
        "\n",
        "  cat(\"Plotting the residuals.\\n\")\n",
        "  jpeg(filename = paste(names(chla)[site], \"_residuals.jpg\", sep=\"\"), width = width, height = height, \n",
        "      units = \"px\", pointsize = ps, quality = 100, bg = \"white\", res = res)\n",
        "  plot(chla.harm$xt, chla.harm$residuals, col=1, lwd=2, pch=\"+\", ylim=c(-20,20))\n",
        "  dev.off()\n",
        "}\n",
        "\n",
        "sink()\n",
        "\n",
        "cat(\"Read the log file: \", wd, \"/\", file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JtcPTXOp_hu",
        "colab_type": "text"
      },
      "source": [
        "# Don't forget:\n",
        "\n",
        "The R code creates graphics files in the data directory and writes a log file. Check them out.\n",
        "You can see a list of all files in the folder by running a command line like this:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xiJ_L0YneH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -l '/content/drive/My Drive/practicals/p06'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDGFHrZip_hw",
        "colab_type": "text"
      },
      "source": [
        "# Your portfolio task\n",
        "\n",
        "Choose a basin and add the harmonic analysis results (graphs and/or tables) to your practical portfolio. Explain whether in your opinion, this analysis adds new information to the previous methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoWZOuWGp_hy",
        "colab_type": "text"
      },
      "source": [
        "# Wavelet analysis\n",
        "\n",
        "Now on to the wavelets.\n",
        "\n",
        "\n",
        "The wavelet plot above has the frequency on the y axis, time on the x axis and the colours show the magnitude of the correlation of the time-series data with the wavelet we chose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0RHUCbPp_hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the SciPy package\n",
        "from scipy import signal\n",
        "\n",
        "# remember that column 1 (the second column after column zero) of x and y (only has one column) \n",
        "#     contain our phenology data from Lake Balaton used earlier:\n",
        "plt.figure(figsize=(15,2))\n",
        "plt.plot(x[:,1],y)\n",
        "plt.xlabel('time step [dekads]')\n",
        "plt.ylabel('Chlorophyll A content')\n",
        "plt.title('Our phenology data of Lake Balaton')\n",
        "plt.show()\n",
        "\n",
        "# print and plot the Ricker wavelet function\n",
        "a = 5.0 # width parameter\n",
        "vec5 = signal.ricker(len(x[:,1]), a)\n",
        "a = 10.0 # now change the width of the wavelet\n",
        "vec10 = signal.ricker(len(x[:,1]), a)\n",
        "plt.figure(figsize=(15,2))\n",
        "plt.plot(vec5)\n",
        "plt.title(\"Ricker wavelet, width=5\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(15,2))\n",
        "plt.plot(vec10)\n",
        "plt.title(\"Ricker wavelet, width=10\")\n",
        "plt.show()\n",
        "\n",
        "# now fit the wavelet function to the time series data\n",
        "maxw = 36 # maximum width of the wavelet\n",
        "widths = np.arange(1, maxw) # widths of the wavelet used for the wavelet transform\n",
        "cwt_result = signal.cwt(y, signal.ricker, widths)\n",
        "vmax=abs(cwt_result).max() # set colour range\n",
        "vmin=-abs(cwt_result).max()\n",
        "plt.imshow(cwt_result, cmap='hot', aspect='auto', vmax=vmax, vmin=vmin)\n",
        "plt.xlabel('time steps [dekads]')\n",
        "plt.ylabel('period [dekads]')\n",
        "plt.title('Wavelet plot')\n",
        "plt.colorbar(label='magnitude')\n",
        "plt.clim(vmin,vmax)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqWVki-0p_h3",
        "colab_type": "text"
      },
      "source": [
        "Above, you can see that the 'hot' colours correspond to time points (x axis) where the wavelet function stretched by a scale factor (y axis) correlates positively with the local time series data.\n",
        "The 'cool' colours show negative correlation, i.e. where the wavelet has its peak, the data have a local minimum at that time scale.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY9tJUE6p_h4",
        "colab_type": "text"
      },
      "source": [
        "# Your portfolio task\n",
        "\n",
        "Change the type of wavelet you fit to the time series in the comand 'signal.cwt'. Choose the best one you can find and add the wavelet plot to your portfolio. Justify briefly why you think this is the best wavelet and describe the main features of the plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C21gGNbp_h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy, paste and edit your code here.\n",
        "# add it to the portfolio in your word document, too.\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}