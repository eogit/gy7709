{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "practical_sync_week7_SentinelSat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUexW0CmhlF3"
      },
      "source": [
        "# Week 7: Accessing the Copernicus Open Access Hub with the SentinelSat library\n",
        "\n",
        "Individual learning outcomes: At the end of this week, all students should be able to access the Copernicus Open Access Hub via the API, set up and submit a data query and automatically download individual Sentinel-2 images to Google Drive and Colab for further analysis. Students should understand the Sentinel-2 file structure and to pre-process the image automatically, including unzipping, reprojecting, and clipping.\n",
        "\n",
        "The Copernicus Open Access Hub allows you to do a search for single images. However, there are limitations as to how many images you can request from the Long-Term Archive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1skim1-_4qGc"
      },
      "source": [
        "# Get a user account on the Sentinel Open Access Data Hub\n",
        "\n",
        "Before we begin, make sure to register for an account in the Copernicus Open Access Hub.\n",
        "\n",
        "For registration follow the link to the Open Access Hub and register: https://scihub.copernicus.eu/dhus/#/home\n",
        "\n",
        "In previous weeks, we had manually uploaded a Sentinel-2 image to our Google Drive directory. We had also used Google Earth Engine to process multi-temporal Sentinel-2 images into image composites for us.\n",
        "\n",
        "Today, we want to access the Sentinel Data Hub and search for available images over an area of interest of our choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___N_2vyp_f2"
      },
      "source": [
        "Connect to our Google Drive from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RvpK17wXp_f7"
      },
      "source": [
        "# Load the Drive helper and mount your Google Drive as a drive in the virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fajjFxKt5nwT"
      },
      "source": [
        "## Sentinelsat API\n",
        "\n",
        "We'll be using an API designed by Wille and Clauss (2016) called sentinelsat. This API was designed to query and download Copernicus product imagery from the Copernicus Open Access Hub API.\n",
        "\n",
        "Follow the link to the API, and see if you can understand how it works: https://sentinelsat.readthedocs.io/en/stable/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnzg_pcTSEP"
      },
      "source": [
        "#import required libraries, including the sentinelsat library this time\n",
        "!pip install rasterio\n",
        "!pip install sentinelsat\n",
        "!pip install geopandas\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio import plot\n",
        "from rasterio.plot import show_hist\n",
        "from rasterio.windows import Window\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt, geojson\n",
        "from collections import OrderedDict\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "from osgeo import gdal, ogr\n",
        "from pyproj import Proj\n",
        "from pprint import pprint\n",
        "import shutil\n",
        "import sys\n",
        "import zipfile\n",
        "from math import floor, ceil\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37tiOSa7wq8"
      },
      "source": [
        "We need a help function at a later stage, so let's define that now. It converts between lat/lon coordinates and pixel locations in a raster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gceBeSJI7pKF"
      },
      "source": [
        "# define a helper function that converts latitude, longitude coordinates into pixel locations\n",
        "def longlat2window(lon, lat, dataset):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        lon (tuple): Tuple of min and max lon\n",
        "        lat (tuple): Tuple of min and max lat\n",
        "        dataset: Rasterio dataset\n",
        "\n",
        "    Returns:\n",
        "        rasterio.windows.Window\n",
        "    \"\"\"\n",
        "    p = Proj(dataset.crs)\n",
        "    t = dataset.transform\n",
        "    xmin, ymin = p(lon[0], lat[0])\n",
        "    xmax, ymax = p(lon[1], lat[1])\n",
        "    col_min, row_min = ~t * (xmin, ymin)\n",
        "    col_max, row_max = ~t * (xmax, ymax)\n",
        "    return Window.from_slices(rows=(floor(row_max), ceil(row_min)),\n",
        "                              cols=(floor(col_min), ceil(col_max)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcro81WxTf4_"
      },
      "source": [
        "# Accessing Sentinel-2 images\n",
        "\n",
        "The workflow for this practical is similar to the previous one:\n",
        "* Define an area of interest based on an ESRI shapefile\n",
        "* Define a time window for our data search\n",
        "* Set a maximum acceptable cloud cover for our search\n",
        "* Search the ESA Copernicus Open Access Hub for all available images\n",
        "* Select the individual images with the least cloud cover and download them to Google Drive\n",
        "* Reproject (warp) the TCI images and crop them to our area of interest\n",
        "* Make a movie for our area of interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MWM3wcwTf5P"
      },
      "source": [
        "Before proceeding, we have to go to Google Drive and create a text file called \"sencredentials.txt\" with our login details for the ESA Copernicus Sentinel Hub. \n",
        "\n",
        "The file has two lines of text.\n",
        "\n",
        "Line 1: Your username\n",
        "\n",
        "Line 2: Your password"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmx17s7RvTG9"
      },
      "source": [
        "Set some directory paths on Google Drive.\n",
        "BEFORE YOU RUN THIS CELL, EDIT THE VARIABLE wd BELOW TO POINT TO YOUR DIRECTORY ON GOOGLE DRIVE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzmlF4GQD1ji"
      },
      "source": [
        "# set up your directories for the satellite data\n",
        "# Note that we do all the downloading and data analysis on the temporary drive\n",
        "#    on Colab. We will copy the output directory to our Google Drive at the end.\n",
        "#    Colab has more disk space (about 40 GB free space) than Google Drive (15 GB).\n",
        "#    However, the data on the Colab disk space are NOT kept when you log out.\n",
        "\n",
        "# path to your Google Drive\n",
        "# EDIT THIS LINE (/content/drive/My Drive is the top directory on Google Drive):\n",
        "wd = \"/content/drive/My Drive/practicals20-21\"\n",
        "print(\"Connected to data directory: \" + wd)\n",
        "\n",
        "# path to your temporary drive on the Colab Virtual Machine\n",
        "cd = \"/content/work\"\n",
        "\n",
        "# directory for downloading the Sentinel-2 granules\n",
        "# Note that we are using the 'join' function imported from the os library here for the first time\n",
        "# It is an easy way of merging strings into a directory structure.\n",
        "# It is clever and chooses the / or \\ depending on whether you are on Windows or Linux.\n",
        "downloaddir = join(cd, 'download') # where we save the downloaded images\n",
        "quickdir = join(cd, 'quicklooks')  # where we save the quicklooks\n",
        "outdir = join(cd, 'out')           # where we save any other outputs\n",
        "\n",
        "# CAREFUL: This code removes the named directories and everything inside them to free up space\n",
        "# Note: shutil provides a lot of useful functions for file and directory management\n",
        "try:\n",
        "  shutil.rmtree(downloaddir)\n",
        "except:\n",
        "  print(downloaddir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(quickdir)\n",
        "except:\n",
        "  print(quickdir + \" not found.\")\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(outdir)\n",
        "except:\n",
        "  print(outdir + \" not found.\")\n",
        "\n",
        "# create the new directories, unless they already exist\n",
        "os.makedirs(cd, exist_ok=True)\n",
        "os.makedirs(downloaddir, exist_ok=True)\n",
        "os.makedirs(quickdir, exist_ok=True)\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "print(\"Connected to Colab temporary data directory: \" + cd)\n",
        "\n",
        "print(\"\\nList of contents of \" + wd)\n",
        "for f in sorted(os.listdir(wd)):\n",
        "  print(f)\n",
        "\n",
        "# check whether the file with the login details exists\n",
        "if \"sencredentials.txt\" not in os.listdir(wd):\n",
        "  print(\"\\nERROR: File sencredentials.txt not found. Cannot log in to Data Hub.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0-BMbjnTf5K"
      },
      "source": [
        "Set up some directory names. \n",
        "\n",
        "Modify these string variables to match your data directory structure.\n",
        "\n",
        "IMPORTANT: You must upload a shapefile of your area of interest to your Google Drive before running the next cell. Set the variable 'shapefile' below to point to this file. You can draw a polygon and save it as a shapefile on http://www.geojson.io."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy8PKeYGTf5Q"
      },
      "source": [
        "# BEFORE YOU RUN THIS BLOCK, YOU NEED A USER ACCOUNT ON THE ESA SENTINEL HUB\n",
        "# In a browser, go to https://scihub.copernicus.eu/dhus/#/home\n",
        "# Click on the user symbol in the top right and then on 'sign up'\n",
        "# Follow the instructions.\n",
        "# When you have your account, create a .txt file in Word that contains two lines:\n",
        "#   line 1 - your username\n",
        "#   line 2 - your password\n",
        "# save it under the name \"sencredentials.txt\"\n",
        "# upload it to the same directory as the Jupyter Notebook on your Google Drive.\n",
        "\n",
        "# This will allow the notebook to connect to your account on the ESA Data Archive.\n",
        "credentials = join(wd, 'sencredentials.txt')  # contains two lines of text with username and password\n",
        "\n",
        "# Download options and Data Hub search parameters\n",
        "\n",
        "# EDIT THE OPTIONS BELOW\n",
        "ndown = 3 # number of scenes to be downloaded (in order of least cloud cover), should be less than 11 on Colab due to space\n",
        "\n",
        "# YOU CAN PLACE A DIFFERENT SHAPEFILE ONTO YOUR GOOGLE DRIVE BUT MAKE SURE THAT\n",
        "#    THE VARIABLE shapefile POINTS TO THE CORRECT FILE:\n",
        "shapefile = join(wd, 'oakham', 'Polygons_small.shp') # ESRI Shapefile of the study area\n",
        "\n",
        "# Define a date range for our search\n",
        "datefrom = '20190301' # start date for imagery search\n",
        "dateto   = '20191231' # end date for imagery search\n",
        "\n",
        "# Define which cloud cover we accept in the images\n",
        "clouds = '[0 TO 10]' # range of acceptable cloud cover % for imagery search\n",
        "# Note that later versions of the Sentinelsat package require this in the format: clouds = (0, 10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss25DF_R3CXo"
      },
      "source": [
        "# Search for available image files on the ESA data server\n",
        "\n",
        "We begin by reading in the user name and password we have saved in our text file 'sencredentials.txt'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV1byFoZKbtq"
      },
      "source": [
        "# go to working directory\n",
        "os.chdir(wd)\n",
        "\n",
        "# load user credentials for Sentinel Data Hub at ESA, i.e. read two lines of text with username and password\n",
        "with open(join(wd, credentials)) as f:\n",
        "    lines = f.readlines()\n",
        "username = lines[0].strip()\n",
        "password = lines[1].strip()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkQDS_6hChjm"
      },
      "source": [
        "Now let's create the Sentinel API object in Python.\n",
        "\n",
        "API stands for 'application programming interface'. An API defines interactions between multiple software intermediaries, in this case between our Jupyter Notebook and the ESA Copernicus Data Hub. It defines the kinds of calls or requests that can be made, how to make them, the data formats that should be used, the conventions to follow etc. (text modified after Wikipedia)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBWrTe2PCmGb"
      },
      "source": [
        "# Define the API\n",
        "api = SentinelAPI(username, password, 'https://scihub.copernicus.eu/dhus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV--_jrZHuXb"
      },
      "source": [
        "Before going further, let's just check that the shapefile extent and projection are in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDXQnbChE0b3"
      },
      "source": [
        "# Get the shapefile layer's extent\n",
        "driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
        "ds = driver.Open(shapefile, 0)\n",
        "lyr = ds.GetLayer()\n",
        "extent = lyr.GetExtent()\n",
        "print(\"Extent of the area of interest (shapefile):\\n\", extent)\n",
        "\n",
        "# get projection information from the shapefile to reproject the images to\n",
        "outSpatialRef = lyr.GetSpatialRef().ExportToWkt()\n",
        "ds = None # close file\n",
        "print(\"\\nSpatial referencing information of the shapefile:\\n\", outSpatialRef)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsrHe5vcH4Lb"
      },
      "source": [
        "Because the Copernicus Open Access Hub requires the region of interest coordinates in a specific format, we define the following helper function.\n",
        "\n",
        "It takes the corner coordinates of the extent of the shapefile and turns them into a closed polygon object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tOoBk83HgsG"
      },
      "source": [
        "# We need to define a helper function that creates a simply bounding box polygon from the \n",
        "#   extent of our shapefile in the right format for the Data Hub API.\n",
        "def bbox(coord_list):\n",
        "  # Create a Polygon from the extent tuple\n",
        "  box = ogr.Geometry(ogr.wkbLinearRing)\n",
        "  box.AddPoint(extent[0],extent[2])\n",
        "  box.AddPoint(extent[1], extent[2])\n",
        "  box.AddPoint(extent[1], extent[3])\n",
        "  box.AddPoint(extent[0], extent[3])\n",
        "  box.AddPoint(extent[0],extent[2])\n",
        "  poly = ogr.Geometry(ogr.wkbPolygon)\n",
        "  poly.AddGeometry(box)\n",
        "  return poly\n",
        "\n",
        "# Let's see what it does\n",
        "print(extent)\n",
        "print(bbox(extent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmFu2fxDKbts"
      },
      "source": [
        "Now we have the extent of our area of interest in the right format for the API, we can query the ESA Sentinel data hub by submitting our options as arguments.\n",
        "\n",
        "The **kwargs argument list is a flexible way of handing over multiple arguments to a Python function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ashlcpOdKbts"
      },
      "source": [
        "# set query parameters and search the Sentinel data hub\n",
        "kwargs = {\n",
        "        'area': bbox(extent),\n",
        "        'date': (datefrom, dateto),\n",
        "        'platformname': 'Sentinel-2',\n",
        "        'processinglevel': 'Level-2A',\n",
        "        'cloudcoverpercentage': clouds\n",
        "        }\n",
        "\n",
        "# search the Sentinel data hub API\n",
        "products = api.query(**kwargs)\n",
        "# the search returns an ordered dictionary object of the image products\n",
        "print(type(products))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5IAPzszK674"
      },
      "source": [
        "Let's look at the query results, save them in a format that can be read into Excel and select the images we want to download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sghwq092Kbtt"
      },
      "source": [
        "# ordered dictionaries are hard to work with, so we convert the list of image \n",
        "#    products to a a Pandas DataFrame\n",
        "# you can read up on this data structure here (https://pymotw.com/2/collections/ordereddict.html)\n",
        "products_df = api.to_dataframe(products)\n",
        "print('Search resulted in '+str(products_df.shape[0])+' satellite images with '+\n",
        "      str(products_df.shape[1])+' attributes.')\n",
        "\n",
        "os.chdir(outdir) # set working direcory for output files\n",
        "\n",
        "# sort the search results\n",
        "products_df_sorted = products_df.sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True])\n",
        "print(products_df_sorted)\n",
        "\n",
        "# save the full search results to a .csv file that you can read into Excel\n",
        "outfile = 'searchresults_full.csv'\n",
        "products_df_sorted.to_csv(outfile)\n",
        "print(\"Search results saved: \" + outfile)\n",
        "\n",
        "# limit the download to the first 'ndown' images \n",
        "#   sorted by lowest cloud cover and earliest acquisition date\n",
        "products_df_n = products_df_sorted.head(ndown)\n",
        "print(products_df_n)\n",
        "\n",
        "# save the list of data to be downloaded to a .csv file that you can read into Excel\n",
        "outfile = 'searchresults4download.csv'\n",
        "products_df_n.to_csv(outfile)\n",
        "print(\"Download list saved: \" + outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2DYvzmz_uIR"
      },
      "source": [
        "# Download the individual Sentinel-2 granules to Google Drive\n",
        "This takes a long time if many images are selected. Each is 100x100 km in size and has bands of 10 m and coarser resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSGlMtjLKbtt"
      },
      "source": [
        "# Download all selected images into a data directory\n",
        "os.chdir(downloaddir) # set working direcory to download directory\n",
        "\n",
        "# print the unique image IDs that we will submit to the API\n",
        "for i in products_df_n['uuid']:\n",
        "  pprint(i)\n",
        "\n",
        "# download sorted and reduced products in order\n",
        "api.download_all(products_df_n['uuid'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbeTnWOEK-da"
      },
      "source": [
        "We can save the footprints of the images returned in the query. They can be visualised with QGIS or any other GIS software. \n",
        "\n",
        "Try it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7DmNDkOKbtt"
      },
      "source": [
        "# get the footprints of the selected scenes for use in Excel\n",
        "s2footprints = products_df_n.footprint\n",
        "outfile = join(outdir, 'footprints.csv')\n",
        "s2footprints.to_csv(outfile, header = False)\n",
        "print(\"Granule footprints saved as csv: \" + outfile)\n",
        "\n",
        "# save the footprints of the scenes marked for download together with their metadata in a Geojson file\n",
        "\n",
        "# first, we run a new query to get the metadata for the selected images\n",
        "# we do this by creating an Ordered Dictionary with all our found images \n",
        "products_n = OrderedDict()\n",
        "\n",
        "# iterate over all unique image IDs in our list and get their metadata into our ordered dictionary\n",
        "for uuid in products_df_n['uuid']:\n",
        "    kw = kwargs.copy()\n",
        "    kw['uuid'] = uuid\n",
        "    pp = api.query(**kw)\n",
        "    products_n.update(pp)\n",
        "\n",
        "# write the footprints and metadata to a geojson file\n",
        "outfile = join(outdir, 'footprints.geojson')\n",
        "with open(outfile, 'w') as f:\n",
        "    json.dump(api.to_geojson(products_n), f)\n",
        "print(\"Granule footprints saved as GeoJson: \" + outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlB0QLm__yXX"
      },
      "source": [
        "# Explore the data directory structure of our downloaded files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ0b3ZURLOdm"
      },
      "source": [
        "# where we stored the text files and csv files\n",
        "os.chdir(outdir)\n",
        "print(\"contents of \", outdir, \":\")\n",
        "!ls -l\n",
        "\n",
        "# where we stored the downloaded Sentinel-2 images\n",
        "os.chdir(downloaddir)\n",
        "print(\"contents of \", downloaddir, \":\")\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrSJL471MrcD"
      },
      "source": [
        "Remember that we have saved the downloaded images to a temporary directory that will be deleted when we close the virtual machine. If you want to save your images to your local directory, this is how it goes.\n",
        "\n",
        "Go to your Google Colab  folder in the panel on the left hand side.\n",
        "\n",
        "Find the download directory and click on a Sentinel-2 image folder.\n",
        "\n",
        "Right-click on it and select 'download' to save it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJp7NfpL_1bf"
      },
      "source": [
        "# Iterate over all images and show the TCI file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXb24b6hLDui"
      },
      "source": [
        "The downloaded Sentinel-2 granules (or single images) are zipped. We need to unzip them first. At this stage, we remove the zipped file to free up disk space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCwuRpEJLS4E"
      },
      "source": [
        "# set working direcory to download directory\n",
        "os.chdir(downloaddir)\n",
        "\n",
        "# get list of all zip files in the data directory\n",
        "allfiles = [f for f in listdir(downloaddir) if isfile(join(downloaddir, f))]\n",
        "\n",
        "# unzip all downloaded Sentinel-2 files\n",
        "for x in range(len(allfiles)):\n",
        "\n",
        "  # we can split the file name and check whether it ends with '.zip'\n",
        "  if allfiles[x].split(\".\")[1] == \"zip\":\n",
        "    print(\"Unzipping file \", x+1, \": \", allfiles[x])\n",
        "\n",
        "    with zipfile.ZipFile(allfiles[x], \"r\") as zipf:\n",
        "      # first extract the files\n",
        "      zipf.extractall(downloaddir)\n",
        "\n",
        "      # then remove the zip file to save disk space\n",
        "      os.remove(join(downloaddir, allfiles[x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuDbMvzGUR74"
      },
      "source": [
        "Before we can create a quick visualisation of the TCI files of all downloaded images, we need to find all the files. They are located in the 20m subdirectories of our downloaded Sentinel-2 directories for each image.\n",
        "How can we do that? We can iterate over all directories and search for the right files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fm_Av5gPOCh"
      },
      "source": [
        "# make a list of all TCI files across all downloaded image directories\n",
        "tciIDs = [] # empty list of all Sentinel-2 granule IDs we have downloaded\n",
        "tcidirs = [] # empty list of all directory paths pointing to the TCI files\n",
        "tcifiles = [] # empty list of all TCI file names\n",
        "\n",
        "# get the list of all directories in the download directory\n",
        "# there is one directory for each Sentinel-2 image (granule)\n",
        "dirlist = [d for d in listdir(downloaddir) if isdir(join(downloaddir, d))]\n",
        "pprint(dirlist)\n",
        "\n",
        "# make a list of only the directories that start with \"S2A\" or \"S2B\"\n",
        "s2dirlist = []\n",
        "for y in range(len(dirlist)):\n",
        "  # remove any subdirectories that are not containing Sentinel-2 data\n",
        "  # for example, .iPythonNotebook checkpoints that are created automatically\n",
        "  if dirlist[y].split(\"_\")[0] == \"S2A\" or dirlist[y].split(\"_\")[0] == \"S2B\":\n",
        "    # append this TCI file name to a list of all TCI files found in any directory\n",
        "    s2dirlist.append(dirlist[y])\n",
        "pprint(s2dirlist)\n",
        "\n",
        "# iterate over all Sentinel-2 image directories and show the TCI file to check the image quality on screen\n",
        "for d in range(len(s2dirlist)):\n",
        "  # the directory names have the following structure, for example:\n",
        "  # S2A_MSIL2A_20190919T110721_N0213_R137_T30UXD_20190919T140654.SAFE\n",
        "  # the first part of the directory name is the granule ID\n",
        "  # so we split off the \".SAFE\" as follows:\n",
        "  sceneID = s2dirlist[d].split(\".\")[0] \n",
        "  tciIDs.append(sceneID) #append the uniqu identifier to the list\n",
        "\n",
        "  # find the GRANULE, then L2A_*, then IMG_DATA, then R20m directory\n",
        "  thisdir = join(downloaddir, s2dirlist[d], \"GRANULE\")\n",
        "  print(thisdir)\n",
        "\n",
        "  # find the full name of the L2A_* subdirectory (contains the scene ID)\n",
        "  subdirlist = [s for s in listdir(thisdir) if isdir(join(thisdir, s))]\n",
        "  print(subdirlist)\n",
        "  for y in range(len(subdirlist)):\n",
        "    if subdirlist[y].split(\"_\")[0] == \"L2A\":\n",
        "      thisdir = join(thisdir, subdirlist[y])\n",
        "\n",
        "  # add IMG_DATA/R20m to subdirectory, this is where the TCI image is found\n",
        "  tcidir = join(thisdir, \"IMG_DATA\", \"R20m\")\n",
        "  tcidirs.append(tcidir) # add it to our list\n",
        "\n",
        "  # find the TCI image file name\n",
        "  files_20m = [f for f in listdir(tcidir) if isfile(join(tcidir, f))]\n",
        "\n",
        "  # We split the filename into components based on the underscore _\n",
        "  # e.g. \"T30UXD_20190919T110721_TCI_20m.jp2\"\n",
        "  # becomes [\"T30UXD\", \"20190919T110721\", \"TCI\", \"20m.jp2\"]\n",
        "  # so the component indexed 2 should be \"TCI\" if we have found the file\n",
        "  for y in range(len(files_20m)):\n",
        "    if files_20m[y].split(\"_\")[2] == \"TCI\":\n",
        "      # append this TCI file name to a list of all TCI files found in any directory\n",
        "      tcifiles.append(files_20m[y])\n",
        "\n",
        "# the output looks neater if we print each element of the list of strings in a new line\n",
        "print(\"List of all Granule IDs:\")\n",
        "for i in tciIDs:\n",
        "  print(i)\n",
        "print(\"List of all TCI directories:\")\n",
        "for i in tcidirs:\n",
        "  print(i)\n",
        "print(\"List of all TCI files:\")\n",
        "for i in tcifiles:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxhvZZe2UmCu"
      },
      "source": [
        "Now we know which image files we want to show on screen, the rest is easy. Just like last week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaCimAhcUlFz"
      },
      "source": [
        "# how many files are in the file list?\n",
        "nfiles = len(tcifiles)\n",
        "\n",
        "# arrange our subplots, assuming a 16:9 screen ratio\n",
        "cols = min(nfiles, 4) # maximum of 4 plots in one row\n",
        "rows = math.ceil(nfiles / cols) # round up to nearest integer\n",
        "\n",
        "# create a figure with subplots\n",
        "fig, ax = plt.subplots(rows, cols, figsize=(21,7))\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "# iterate over all Sentinel-2 image directories and show the TCI file to check the image quality on screen\n",
        "for x in range(nfiles):\n",
        "  # join the directory path with the file name\n",
        "  tcifile = join(tcidirs[x], tcifiles[x])\n",
        "\n",
        "  #open bands as separate single-band raster from the image directory pointing to the 20 m resolution bands\n",
        "  bandTCI = rasterio.open(tcifile, driver='JP2OpenJPEG') #True Colour Image in uint8 data format\n",
        "\n",
        "  #plot band using RasterIO as we did last week\n",
        "  # note that this time we define the axes as an indexable list, so we can iterate over the subplots\n",
        "  plot.show(bandTCI, ax=ax[x])\n",
        "\n",
        "  # set a title for the subplot\n",
        "  mytitle = tciIDs[x]\n",
        "  ax[x].set_title(mytitle, fontsize=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIpqcW9a8Fnu"
      },
      "source": [
        "To zoom to our shapefile area, let's warp all TCI images to the same projection as the shapefile. \n",
        "\n",
        "Remember we did this last week:\n",
        "\n",
        "```\n",
        "ds = gdal.Warp('Sentinel-2_stack_100m_BNG.tiff',\n",
        "               'Sentinel-2_stack_100m.tiff', dstSRS='EPSG:27700')\n",
        "ds = None #remember to close and save the output file\n",
        "```\n",
        "\n",
        "So let's put GDAL to work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SziStT4I8L0p"
      },
      "source": [
        "# get the spatial referencing system of our shapefile into which we want to reproject the TCI images\n",
        "# remember, we did this when we opened the shapefile earlier and saved it in outSpatialRef\n",
        "print(\"Reprojecting all TCI images to the following projection:\")\n",
        "print(outSpatialRef)\n",
        "\n",
        "warpfiles = [] # make an empty list where we can remember all the warped output file names\n",
        "\n",
        "# iterate over all Sentinel-2 image directories and warp the image\n",
        "for x in range(nfiles):\n",
        "  # join the directory path with the file name\n",
        "  tcifile = join(tcidirs[x], tcifiles[x])\n",
        "\n",
        "  # make a directory path and file name for the warped output file\n",
        "  warpfile = join(quickdir, tciIDs[x] + \"_warped.jp2\")\n",
        "  warpfiles.append(warpfile) # add it to our list\n",
        "\n",
        "  # call the GDAL Warp command\n",
        "  ds = gdal.Warp(warpfile, tcifile, dstSRS=outSpatialRef)\n",
        "  ds = None #remember to close and save the output file\n",
        "\n",
        "pprint(warpfiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm_DVizqkATe"
      },
      "source": [
        "# open the shapefile for plotting\n",
        "driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
        "ds = driver.Open(shapefile, 0)\n",
        "\n",
        "# plot the warped images\n",
        "# Improve the function to convert uint16 to unit8 and rescale to 0-255\n",
        "# to include the visualisation commands\n",
        "\n",
        "# create a figure with subplots\n",
        "fig, ax = plt.subplots(rows, cols, figsize=(21,7))\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "# We will use the Geopandas library for plotting the shapefile on top of the raster image.\n",
        "# This is the easiest available option for plotting.\n",
        "\n",
        "# iterate over all Sentinel-2 image directories and show the TCI file to check the image quality on screen\n",
        "for f in range(nfiles):\n",
        "  # join the directory path with the file name\n",
        "  tcifile = warpfiles[f]\n",
        "\n",
        "  #open bands as separate single-band raster from the image directory pointing to the 20 m resolution bands\n",
        "  bandTCI = rasterio.open(tcifile, 'r') #True Colour Image in uint8 data format\n",
        "\n",
        "  #plot band using RasterIO as we did last week\n",
        "  # note that this time we define the axes as an indexable list, so we can iterate over the subplots\n",
        "  plot.show(bandTCI, ax=ax[f])\n",
        "  \n",
        "  # plot the shapefile using Geopandas\n",
        "  shp = gpd.read_file(shapefile)\n",
        "  shp.boundary.plot(ax=ax[f], edgecolor=\"yellow\")\n",
        "  \n",
        "  # set a title for the subplot\n",
        "  mytitle = tciIDs[f]\n",
        "  ax[f].set_title(mytitle, fontsize=8)\n",
        "  \n",
        "  #close the file\n",
        "  bandTCI.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pLK_X_Cjnu7"
      },
      "source": [
        "#Clip the raster\n",
        "\n",
        "Let's do the same visualisation, but zoom into our shapefile area. \n",
        "\n",
        "We will convert the output file to Geotiff, as this causes fewer problems than JPEG2000. \n",
        "\n",
        "You can look up all available options for the Python GDAL library gdal.Warp function here: https://gdal.org/python/osgeo.gdal-pysrc.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Or538qfZoLb"
      },
      "source": [
        "# clip the files in GDAL\n",
        "# make an empty list where we will remember all zoom file names\n",
        "zoomfiles = [] \n",
        "\n",
        "# create a figure with subplots\n",
        "fig, ax = plt.subplots(rows, cols, figsize=(21,7))\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "# iterate over all warped Sentinel-2 TCI files to check the image quality on screen\n",
        "for x in range(nfiles):\n",
        "  warpfile = warpfiles[x]\n",
        "  print(warpfile)\n",
        "\n",
        "  # make the filename of the new zoom image file\n",
        "  zoomfile = warpfile.split(\".\")[0] + \"_zoom.tif\"\n",
        "  zoomfiles.append(zoomfile) # remember the zoom file name in our list\n",
        "  print(zoomfile)\n",
        "\n",
        "  # clip it with rasterio to the shapefile extent\n",
        "  # rasterio offers an option called 'window' to load a subset of a raster file\n",
        "\n",
        "  # open the source file\n",
        "  with rasterio.open(warpfile, 'r') as src:\n",
        "    \n",
        "    # convert the shapefile extent to a rasterio window object\n",
        "    window = longlat2window((extent[0], extent[1]), (extent[2], extent[3]), src)\n",
        "    print(window)\n",
        "    \n",
        "    # read all bands but only for the window extent\n",
        "    arr = src.read(window=window, out_shape=(src.count, window.height, window.width))\n",
        "    print(arr.shape)\n",
        "\n",
        "    # get the data type\n",
        "    dt = arr.dtype\n",
        "\n",
        "    # open the destination file\n",
        "    with rasterio.open(zoomfile, 'w', driver='Gtiff', width=window.width, \n",
        "                       height=window.height, count=src.count, crs=src.crs, \n",
        "                       transform=src.transform, dtype=dt) as dst:\n",
        "      \n",
        "      # iterate over all bands in the source file and write them to the destination file\n",
        "      for b in range(arr.shape[0]):\n",
        "        dst.write(arr[b,:,:], b+1)\n",
        "\n",
        "      # close the destination file\n",
        "      dst.close()\n",
        "\n",
        "    # close the sourcefile\n",
        "    src.close()\n",
        "  \n",
        "  # make maps of the destination files\n",
        "  with rasterio.open(zoomfile, \"r\") as img:\n",
        "    plot.show(img, ax=ax[x])\n",
        "    # set a title for the subplot\n",
        "    mytitle = tciIDs[x]\n",
        "    ax[x].set_title(mytitle, fontsize=8)\n",
        "    # ax[x].set_xlim(extent[0], extent[1])\n",
        "    # ax[x].set_ylim(extent[2], extent[3])\n",
        "    \n",
        "os.chdir(quickdir)\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCo-c2l6_75f"
      },
      "source": [
        "# make a movie from all quicklooks of our area of interest\n",
        "\n",
        "We will use the imageio library to do this efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-U60XS6LWfo"
      },
      "source": [
        "import imageio\n",
        "# create an empty Numpy array where we will merge all raster images\n",
        "images = []\n",
        "# iterate over all zoom files\n",
        "for f in zoomfiles:\n",
        "    images.append(imageio.imread(f)) # read the next image and append it\n",
        "imageio.mimsave(join(outdir, \"movie_fast.gif\"), images)\n",
        "\n",
        "# We can improve it by slowing down the movie.\n",
        "# Let's set the frame rate to 3 seconds.\n",
        "framerate = { 'duration': 3 }\n",
        "imageio.mimsave(join(outdir, \"movie_slow.gif\"), images, **framerate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgp6Ogx1SvD"
      },
      "source": [
        "Now download the file movie.gif from Colab using the folder icon on the left hand side. Locate the files in the 'out' directory, right-click and select 'download'. Save them both to your local hard drive and open them to view them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNHJ320VpfeK"
      },
      "source": [
        "Before we end this session, we want to copy our downloaded Sentinel-2 data from Colab to Google Drive where the data will not be lost when this virtual machine terminates.\n",
        "\n",
        "You could do it via the Linux command line like this:\n",
        "\n",
        "```\n",
        "!cp -r '/content/work/download' '/content/drive/MyDrive/practicals20-21'\n",
        "```\n",
        "\n",
        "Or in a more pythonic way using the shutil library. The copytree function copies a directory and all its contents to a destination folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kngN9aIeoerP"
      },
      "source": [
        "# Copy the content of the download directory we created in Colab on a temporary drive\n",
        "#    to the Google Drive partition, where it is not deleted when this session ends\n",
        "\n",
        "# name of our new directory on Google Drive\n",
        "# NOTE THAT THIS WILL BE DELETED AND OVERWRITTEN!\n",
        "target_dir = \"/content/drive/MyDrive/practicals20-21/download\"\n",
        "\n",
        "def copy_and_overwrite(from_path, to_path, delete_target_dir=True):\n",
        "  '''\n",
        "  Modified from: https://stackoverflow.com/questions/12683834/how-to-copy-directory-recursively-in-python-and-overwrite-all\n",
        "  '''\n",
        "  if os.path.exists(to_path):\n",
        "    if delete_target_dir:\n",
        "      shutil.rmtree(to_path)\n",
        "      shutil.copytree(from_path, to_path)\n",
        "    else:\n",
        "      print(\"Error: Target directory exists and delete_target_dir is set to False.\")\n",
        "  return()\n",
        "\n",
        "temp = copy_and_overwrite(downloaddir, target_dir, delete_target_dir=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATEBsLYvpa14"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}